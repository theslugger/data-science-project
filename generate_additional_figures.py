#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¢å¤–å›¾è¡¨ç”Ÿæˆå™¨ - é«˜è´¨é‡å¯è§†åŒ–
ç”Ÿæˆé¢å¤–çš„é«˜è´¨é‡å›¾è¡¨å’Œåˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®é«˜è´¨é‡å›¾è¡¨å‚æ•°
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 10
plt.rcParams['font.sans-serif'] = ['Times New Roman', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['mathtext.fontset'] = 'stix'

class AdditionalFigureGenerator:
    """é¢å¤–å›¾è¡¨ç”Ÿæˆå™¨"""
    
    def __init__(self, results_file='optimized_90f_ensemble_results.csv'):
        self.results_file = results_file
        self.results_df = None
        self.figures_dir = 'paper_figures'
        
        import os
        os.makedirs(self.figures_dir, exist_ok=True)
    
    def load_results(self):
        """åŠ è½½å®éªŒç»“æœ"""
        try:
            self.results_df = pd.read_csv(self.results_file)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.results_df)} ä¸ªæ¨¡å‹çš„ç»“æœ")
            return True
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶: {self.results_file}")
            return False
    
    def generate_detailed_performance_table_figure(self):
        """ç”Ÿæˆè¯¦ç»†æ€§èƒ½è¡¨æ ¼å›¾"""
        if self.results_df is None:
            return
        
        # åˆ›å»ºè¡¨æ ¼æ ·å¼çš„å›¾
        fig, ax = plt.subplots(figsize=(16, 12))
        ax.axis('tight')
        ax.axis('off')
        
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        table_data = self.results_df.copy()
        table_data = table_data.sort_values('Test_RMSE')
        
        # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
        display_cols = ['Model', 'Type', 'Test_RMSE', 'Test_RÂ²', 'Test_MAE', 'Test_MAPE', 'Training_Time']
        table_subset = table_data[display_cols].head(15)  # æ˜¾ç¤ºå‰15ä¸ªæ¨¡å‹
        
        # æ ¼å¼åŒ–æ•°å€¼
        table_subset = table_subset.copy()
        table_subset['Test_RMSE'] = table_subset['Test_RMSE'].round(2)
        table_subset['Test_RÂ²'] = table_subset['Test_RÂ²'].round(4)
        table_subset['Test_MAE'] = table_subset['Test_MAE'].round(2)
        table_subset['Test_MAPE'] = table_subset['Test_MAPE'].round(2)
        table_subset['Training_Time'] = table_subset['Training_Time'].round(2)
        
        # åˆ›å»ºè¡¨æ ¼
        table = ax.table(cellText=table_subset.values,
                        colLabels=display_cols,
                        cellLoc='center',
                        loc='center',
                        bbox=[0, 0, 1, 1])
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨å¤´æ ·å¼
        for i in range(len(display_cols)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        # è®¾ç½®è¡Œé¢œè‰²äº¤æ›¿
        for i in range(1, len(table_subset) + 1):
            for j in range(len(display_cols)):
                if i % 2 == 0:
                    table[(i, j)].set_facecolor('#f0f0f0')
                else:
                    table[(i, j)].set_facecolor('white')
                
                # é«˜äº®æœ€ä½³å€¼
                if i == 1:  # ç¬¬ä¸€è¡Œï¼ˆæœ€ä½³æ¨¡å‹ï¼‰
                    table[(i, j)].set_facecolor('#FFE082')
                    table[(i, j)].set_text_props(weight='bold')
        
        plt.title('Top 15 Models - Detailed Performance Comparison\n(Sorted by Test RMSE)', 
                 fontsize=16, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(f'{self.figures_dir}/detailed_performance_table.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(f'{self.figures_dir}/detailed_performance_table.pdf', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        print("âœ… è¯¦ç»†æ€§èƒ½è¡¨æ ¼å›¾å·²ç”Ÿæˆ")
    
    def generate_method_comparison_boxplot(self):
        """ç”Ÿæˆæ–¹æ³•ç±»å‹å¯¹æ¯”ç®±çº¿å›¾"""
        if self.results_df is None:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Statistical Distribution Analysis by Method Type', 
                     fontsize=16, fontweight='bold')
        
        metrics = ['Test_RMSE', 'Test_RÂ²', 'Test_MAE', 'Test_MAPE']
        titles = ['Test RMSE Distribution', 'Test RÂ² Distribution', 
                 'Test MAE Distribution', 'Test MAPE Distribution']
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
            ax = axes[idx // 2, idx % 2]
            
            # å‡†å¤‡æ•°æ®
            data_by_type = []
            labels = []
            for method_type in self.results_df['Type'].unique():
                type_data = self.results_df[self.results_df['Type'] == method_type][metric]
                data_by_type.append(type_data.values)
                labels.append(f'{method_type}\n(n={len(type_data)})')
            
            # åˆ›å»ºç®±çº¿å›¾
            bp = ax.boxplot(data_by_type, labels=labels, patch_artist=True)
            
            # è®¾ç½®é¢œè‰²
            for patch in bp['boxes']:
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            ax.set_title(title, fontweight='bold')
            ax.set_ylabel(metric.replace('Test_', '').replace('_', ' '))
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ å‡å€¼ç‚¹
            means = [np.mean(data) for data in data_by_type]
            ax.scatter(range(1, len(means) + 1), means, 
                      color='red', marker='D', s=50, zorder=10, label='Mean')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, mean_val in enumerate(means):
                ax.text(i + 1, mean_val, f'{mean_val:.2f}', 
                       ha='center', va='bottom', fontweight='bold', 
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f'{self.figures_dir}/method_comparison_boxplot.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(f'{self.figures_dir}/method_comparison_boxplot.pdf', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        print("âœ… æ–¹æ³•ç±»å‹å¯¹æ¯”ç®±çº¿å›¾å·²ç”Ÿæˆ")
    
    def generate_correlation_heatmap(self):
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§çƒ­å›¾"""
        if self.results_df is None:
            return
        
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # é€‰æ‹©æ€§èƒ½æŒ‡æ ‡
        metrics = ['Test_RMSE', 'Test_MAE', 'Test_RÂ²', 'Test_MAPE', 'Test_SMAPE', 'Training_Time']
        corr_matrix = self.results_df[metrics].corr()
        
        # åˆ›å»ºçƒ­å›¾
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # åªæ˜¾ç¤ºä¸‹ä¸‰è§’
        
        heatmap = sns.heatmap(corr_matrix, 
                             mask=mask,
                             annot=True, 
                             cmap='RdBu_r', 
                             center=0,
                             square=True,
                             fmt='.3f',
                             cbar_kws={"shrink": .8},
                             ax=ax)
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_title('Performance Metrics Correlation Matrix', 
                    fontsize=16, fontweight='bold', pad=20)
        
        # æ—‹è½¬æ ‡ç­¾
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        plt.savefig(f'{self.figures_dir}/correlation_heatmap.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(f'{self.figures_dir}/correlation_heatmap.pdf', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        print("âœ… ç›¸å…³æ€§çƒ­å›¾å·²ç”Ÿæˆ")
    
    def generate_ensemble_effectiveness_analysis(self):
        """ç”Ÿæˆé›†æˆå­¦ä¹ æœ‰æ•ˆæ€§åˆ†æå›¾"""
        if self.results_df is None:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Ensemble Learning Effectiveness Analysis', 
                     fontsize=16, fontweight='bold')
        
        # 1. åŸºç¡€æ¨¡å‹ vs é›†æˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”
        ax1 = axes[0, 0]
        base_models = self.results_df[self.results_df['Type'] == 'Base Model']
        ensemble_models = self.results_df[self.results_df['Type'] != 'Base Model']
        
        base_rmse = base_models['Test_RMSE'].values
        ensemble_rmse = ensemble_models['Test_RMSE'].values
        
        ax1.hist(base_rmse, alpha=0.7, label=f'Base Models (n={len(base_rmse)})', 
                bins=10, color='#FF6B6B', edgecolor='black')
        ax1.hist(ensemble_rmse, alpha=0.7, label=f'Ensemble Models (n={len(ensemble_rmse)})', 
                bins=10, color='#4ECDC4', edgecolor='black')
        
        ax1.axvline(np.mean(base_rmse), color='red', linestyle='--', 
                   label=f'Base Mean: {np.mean(base_rmse):.2f}')
        ax1.axvline(np.mean(ensemble_rmse), color='teal', linestyle='--', 
                   label=f'Ensemble Mean: {np.mean(ensemble_rmse):.2f}')
        
        ax1.set_xlabel('Test RMSE')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Base vs Ensemble Models RMSE Distribution', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. é›†æˆæ–¹æ³•ç±»å‹å¯¹æ¯”
        ax2 = axes[0, 1]
        ensemble_types = ['Bagging', 'Voting', 'Stacking']
        ensemble_means = []
        ensemble_stds = []
        
        for ens_type in ensemble_types:
            type_data = self.results_df[self.results_df['Type'] == ens_type]['Test_RMSE']
            if len(type_data) > 0:
                ensemble_means.append(type_data.mean())
                ensemble_stds.append(type_data.std())
            else:
                ensemble_means.append(0)
                ensemble_stds.append(0)
        
        bars = ax2.bar(ensemble_types, ensemble_means, yerr=ensemble_stds, 
                      capsize=5, color=['#FF9999', '#66B2FF', '#99FF99'], 
                      edgecolor='black', alpha=0.8)
        
        # æ·»åŠ åŸºç¡€æ¨¡å‹å¹³å‡çº¿
        ax2.axhline(np.mean(base_rmse), color='red', linestyle='--', 
                   label=f'Base Model Average: {np.mean(base_rmse):.2f}')
        
        ax2.set_ylabel('Average Test RMSE')
        ax2.set_title('Ensemble Method Comparison', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean_val in zip(bars, ensemble_means):
            if mean_val > 0:
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,
                        f'{mean_val:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. æ€§èƒ½æ”¹è¿›åˆ†æ
        ax3 = axes[1, 0]
        
        # è®¡ç®—æ¯ç§é›†æˆæ–¹æ³•ç›¸å¯¹äºåŸºç¡€æ¨¡å‹çš„æ”¹è¿›
        base_best = base_models['Test_RMSE'].min()
        improvements = []
        method_names = []
        
        for ens_type in ensemble_types:
            type_data = self.results_df[self.results_df['Type'] == ens_type]
            if len(type_data) > 0:
                ens_best = type_data['Test_RMSE'].min()
                improvement = ((base_best - ens_best) / base_best) * 100
                improvements.append(improvement)
                method_names.append(f'{ens_type}\n({improvement:.1f}%)')
            else:
                improvements.append(0)
                method_names.append(f'{ens_type}\n(0.0%)')
        
        colors = ['green' if imp > 0 else 'red' for imp in improvements]
        bars = ax3.bar(range(len(method_names)), improvements, 
                      color=colors, alpha=0.7, edgecolor='black')
        
        ax3.set_xticks(range(len(method_names)))
        ax3.set_xticklabels(method_names)
        ax3.set_ylabel('RMSE Improvement (%)')
        ax3.set_title('Performance Improvement over Best Base Model', fontweight='bold')
        ax3.axhline(0, color='black', linestyle='-', alpha=0.5)
        ax3.grid(True, alpha=0.3)
        
        # 4. è®­ç»ƒæ—¶é—´ vs æ€§èƒ½æ”¹è¿›
        ax4 = axes[1, 1]
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹ç›¸å¯¹äºæœ€å·®åŸºç¡€æ¨¡å‹çš„æ”¹è¿›
        base_worst = base_models['Test_RMSE'].max()
        
        for method_type in self.results_df['Type'].unique():
            if method_type != 'Base Model':
                type_data = self.results_df[self.results_df['Type'] == method_type]
                improvements = ((base_worst - type_data['Test_RMSE']) / base_worst) * 100
                training_times = type_data['Training_Time']
                
                ax4.scatter(training_times, improvements, 
                           label=method_type, s=60, alpha=0.7)
        
        ax4.set_xlabel('Training Time (seconds)')
        ax4.set_ylabel('RMSE Improvement over Worst Base Model (%)')
        ax4.set_title('Training Time vs Performance Improvement', fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.figures_dir}/ensemble_effectiveness_analysis.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(f'{self.figures_dir}/ensemble_effectiveness_analysis.pdf', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        print("âœ… é›†æˆå­¦ä¹ æœ‰æ•ˆæ€§åˆ†æå›¾å·²ç”Ÿæˆ")
    
    def generate_model_ranking_figure(self):
        """ç”Ÿæˆæ¨¡å‹æ’åå›¾"""
        if self.results_df is None:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(18, 8))
        fig.suptitle('Model Performance Ranking Analysis', 
                     fontsize=16, fontweight='bold')
        
        # 1. å‰10åæ¨¡å‹æ’å
        ax1 = axes[0]
        top10 = self.results_df.nsmallest(10, 'Test_RMSE')
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„
        type_colors = {'Base Model': '#FF6B6B', 'Bagging': '#4ECDC4', 
                      'Voting': '#45B7D1', 'Stacking': '#96CEB4'}
        colors = [type_colors.get(t, 'gray') for t in top10['Type']]
        
        bars = ax1.barh(range(len(top10)), top10['Test_RMSE'], color=colors, alpha=0.8)
        
        # è®¾ç½®æ ‡ç­¾
        ax1.set_yticks(range(len(top10)))
        ax1.set_yticklabels([f"{i+1}. {name[:30]}{'...' if len(name) > 30 else ''}" 
                            for i, name in enumerate(top10['Model'])])
        ax1.set_xlabel('Test RMSE')
        ax1.set_title('Top 10 Models by RMSE Performance', fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='x')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, rmse, r2) in enumerate(zip(bars, top10['Test_RMSE'], top10['Test_RÂ²'])):
            ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                    f'RMSE: {rmse:.2f}\nRÂ²: {r2:.3f}', 
                    va='center', fontsize=8,
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, alpha=0.8, label=method) 
                          for method, color in type_colors.items()]
        ax1.legend(handles=legend_elements, loc='lower right')
        
        # 2. æ€§èƒ½åˆ†å¸ƒé›·è¾¾å›¾ï¼ˆå‰5åï¼‰
        ax2 = axes[1]
        top5 = self.results_df.nsmallest(5, 'Test_RMSE')
        
        # æ ‡å‡†åŒ–æŒ‡æ ‡ï¼ˆ0-1èŒƒå›´ï¼‰
        metrics = ['Test_RMSE', 'Test_MAE', 'Test_MAPE']
        normalized_data = top5[metrics].copy()
        
        # RMSEå’ŒMAEéœ€è¦åè½¬ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        for metric in ['Test_RMSE', 'Test_MAE', 'Test_MAPE']:
            max_val = self.results_df[metric].max()
            min_val = self.results_df[metric].min()
            normalized_data[metric] = 1 - (top5[metric] - min_val) / (max_val - min_val)
        
        # RÂ²ç›´æ¥æ ‡å‡†åŒ–ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        r2_max = self.results_df['Test_RÂ²'].max()
        r2_min = self.results_df['Test_RÂ²'].min()
        normalized_data['Test_RÂ²'] = (top5['Test_RÂ²'] - r2_min) / (r2_max - r2_min)
        
        # åˆ›å»ºæ•£ç‚¹å›¾æ˜¾ç¤ºå¤šç»´æ€§èƒ½
        x_vals = normalized_data['Test_RMSE']
        y_vals = normalized_data['Test_RÂ²']
        sizes = (1 - normalized_data['Test_MAE']) * 300 + 50  # åŸºäºMAEçš„ç‚¹å¤§å°
        colors_scatter = [type_colors.get(t, 'gray') for t in top5['Type']]
        
        scatter = ax2.scatter(x_vals, y_vals, s=sizes, c=colors_scatter, alpha=0.7, edgecolors='black')
        
        ax2.set_xlabel('Normalized RMSE Performance (higher is better)')
        ax2.set_ylabel('Normalized RÂ² Performance (higher is better)')
        ax2.set_title('Top 5 Models - Multi-dimensional Performance\n(Bubble size: MAE performance)', 
                     fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ¨¡å‹æ ‡ç­¾
        for i, (x, y, name) in enumerate(zip(x_vals, y_vals, top5['Model'])):
            ax2.annotate(f'{i+1}. {name[:20]}...', 
                        xy=(x, y), xytext=(5, 5), textcoords='offset points',
                        fontsize=8, ha='left',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f'{self.figures_dir}/model_ranking_analysis.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.savefig(f'{self.figures_dir}/model_ranking_analysis.pdf', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        print("âœ… æ¨¡å‹æ’ååˆ†æå›¾å·²ç”Ÿæˆ")
    
    def generate_all_additional_figures(self):
        """ç”Ÿæˆæ‰€æœ‰é¢å¤–å›¾è¡¨"""
        if not self.load_results():
            return
        
        print("\nğŸ¨ å¼€å§‹ç”Ÿæˆé¢å¤–çš„å›¾è¡¨...")
        print("="*60)
        
        print("1. ç”Ÿæˆè¯¦ç»†æ€§èƒ½è¡¨æ ¼å›¾...")
        self.generate_detailed_performance_table_figure()
        
        print("\n2. ç”Ÿæˆæ–¹æ³•ç±»å‹å¯¹æ¯”ç®±çº¿å›¾...")
        self.generate_method_comparison_boxplot()
        
        print("\n3. ç”Ÿæˆç›¸å…³æ€§çƒ­å›¾...")
        self.generate_correlation_heatmap()
        
        print("\n4. ç”Ÿæˆé›†æˆå­¦ä¹ æœ‰æ•ˆæ€§åˆ†æå›¾...")
        self.generate_ensemble_effectiveness_analysis()
        
        print("\n5. ç”Ÿæˆæ¨¡å‹æ’ååˆ†æå›¾...")
        self.generate_model_ranking_figure()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰é¢å¤–å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“ æ‰€æœ‰å›¾è¡¨ä¿å­˜åœ¨: {self.figures_dir}/")
        print("\næ–°ç”Ÿæˆçš„å›¾è¡¨:")
        print("  ğŸ“‹ detailed_performance_table.png/pdf")
        print("  ğŸ“Š method_comparison_boxplot.png/pdf")
        print("  ğŸ”¥ correlation_heatmap.png/pdf")
        print("  ğŸ“ˆ ensemble_effectiveness_analysis.png/pdf")
        print("  ğŸ† model_ranking_analysis.png/pdf")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ é¢å¤–å›¾è¡¨ç”Ÿæˆå™¨")
    print("="*50)
    
    generator = AdditionalFigureGenerator()
    generator.generate_all_additional_figures()

if __name__ == "__main__":
    main() 