#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒ3ï¼šé›†æˆå­¦ä¹ æ–¹æ³•æ¯”è¾ƒ (æˆå‘˜C)
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹

ç›®æ ‡ï¼šæ¯”è¾ƒä¸åŒé›†æˆå­¦ä¹ æ–¹æ³•çš„æ€§èƒ½
æ–¹æ³•ï¼šBaggingã€Boostingã€Votingã€Stacking
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import (
    RandomForestRegressor, BaggingRegressor, AdaBoostRegressor,
    GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor
)
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import time
import warnings
warnings.filterwarnings('ignore')

from data_preprocessing import BikeDataPreprocessor

# è®¾ç½®ä¸­æ–‡å­—ä½“ (è·¨å¹³å°é€šç”¨)
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'PingFang SC', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class OptimalEnsembleExperiment:
    """åŸºäºRÂ²æœ€ä¼˜æ¨¡å‹çš„é›†æˆå­¦ä¹ å®éªŒç±»"""
    
    def __init__(self):
        self.results = {}
        self.preprocessor = BikeDataPreprocessor()
        # ç›®æ ‡å˜é‡å½’ä¸€åŒ–å™¨
        from sklearn.preprocessing import StandardScaler, MinMaxScaler
        self.target_scaler = MinMaxScaler()  # ä½¿ç”¨MinMaxå½’ä¸€åŒ–ï¼Œä¿æŒå€¼åœ¨[0,1]èŒƒå›´å†…
        
    def get_optimal_models(self):
        """è·å–RÂ²æœ€ä¼˜çš„ä¸‰ä¸ªæ¨¡å‹"""
        
        # åŸºäºä¹‹å‰å®éªŒç»“æœï¼ŒRÂ²æœ€ä¼˜çš„ä¸‰ä¸ªæ¨¡å‹ï¼š
        # 1. Neural Network (RÂ²: 0.6786)
        # 2. XGBoost (RÂ²: 0.6507)  
        # 3. Stacking Neural Network (RÂ²: 0.6364)
        
        optimal_models = {
            'Optimal_Neural_Network': MLPRegressor(
                # ç½‘ç»œæ¶æ„ï¼šé’ˆå¯¹87ä¸ªå¢å¼ºç‰¹å¾ä¼˜åŒ–
                hidden_layer_sizes=(256, 128, 64, 32),  # åŠ æ·±ç½‘ç»œä»¥å¤„ç†æ›´å¤šç‰¹å¾
                
                # æ¿€æ´»å‡½æ•°å’Œæ±‚è§£å™¨
                activation='relu',
                solver='adam',
                
                # å­¦ä¹ ç‡å‚æ•°
                learning_rate='adaptive',
                learning_rate_init=0.0005,  # æ›´å°çš„å­¦ä¹ ç‡
                
                # æ­£åˆ™åŒ–
                alpha=0.0001,  # æ›´å°çš„æ­£åˆ™åŒ–ï¼Œè®©æ¨¡å‹å­¦åˆ°æ›´å¤š
                
                # è®­ç»ƒå‚æ•°
                max_iter=8000,  # æ›´å¤šè¿­ä»£
                tol=1e-7,
                
                # æ—©åœç­–ç•¥
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=100,  # æ›´å¤šè€å¿ƒ
                
                # å…¶ä»–å‚æ•°
                batch_size='auto',
                shuffle=True,
                random_state=42
            ),
            
            'Optimal_XGBoost': xgb.XGBRegressor(
                # é’ˆå¯¹87ä¸ªç‰¹å¾å’Œå½’ä¸€åŒ–ç›®æ ‡ä¼˜åŒ–
                n_estimators=1200,  # æ›´å¤šæ ‘
                learning_rate=0.02,  # æ›´å°å­¦ä¹ ç‡
                max_depth=15,  # æ›´æ·±çš„æ ‘
                min_child_weight=2,
                subsample=0.9,
                colsample_bytree=0.85,
                colsample_bylevel=0.85,
                colsample_bynode=0.85,
                reg_alpha=0.01,
                reg_lambda=0.1,
                gamma=0.05,
                random_state=42,
                n_jobs=-1,
                verbosity=0
            ),
            
            'Optimal_Gradient_Boosting': GradientBoostingRegressor(
                # åŸºäºGradient Boostingä¼˜åŒ–ï¼ˆä½œä¸ºStackingçš„æ›¿ä»£ï¼‰
                n_estimators=1000,
                learning_rate=0.03,
                max_depth=12,
                subsample=0.9,
                min_samples_split=3,
                min_samples_leaf=2,
                max_features='sqrt',  # ä½¿ç”¨sqrt(87) â‰ˆ 9ä¸ªç‰¹å¾
                random_state=42
            )
        }
        
        return optimal_models
    
    def create_ensemble_from_optimal(self, X_train, y_train, X_val, y_val):
        """åŸºäºæœ€ä¼˜ä¸‰ä¸ªæ¨¡å‹åˆ›å»ºé›†æˆ"""
        
        print("ğŸ† è®­ç»ƒRÂ²æœ€ä¼˜çš„ä¸‰ä¸ªæ¨¡å‹...")
        
        # è·å–æœ€ä¼˜æ¨¡å‹
        optimal_models = self.get_optimal_models()
        trained_models = {}
        
        # è®­ç»ƒæ¯ä¸ªæœ€ä¼˜æ¨¡å‹
        for name, model in optimal_models.items():
            print(f"  è®­ç»ƒ {name}...")
            start_time = time.time()
            model.fit(X_train, y_train)
            training_time = time.time() - start_time
            
            # éªŒè¯æ€§èƒ½
            val_pred = model.predict(X_val)
            val_r2 = r2_score(y_val, val_pred)
            print(f"    éªŒè¯é›†RÂ²: {val_r2:.4f}, è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
            
            trained_models[name] = model
        
        # åˆ›å»ºé›†æˆæ–¹æ³•
        ensemble_models = {}
        
        # 1. ç®€å•å¹³å‡é›†æˆ
        class SimpleAverageEnsemble:
            def __init__(self, models):
                self.models = models
            
            def predict(self, X):
                predictions = np.array([model.predict(X) for model in self.models.values()])
                return np.mean(predictions, axis=0)
            
            def fit(self, X, y):
                pass  # å·²ç»è®­ç»ƒå¥½äº†
        
        ensemble_models['Average_Ensemble'] = SimpleAverageEnsemble(trained_models)
        
        # 2. åŠ æƒå¹³å‡é›†æˆï¼ˆåŸºäºéªŒè¯é›†RÂ²ï¼‰
        class WeightedAverageEnsemble:
            def __init__(self, models, weights):
                self.models = models
                self.weights = weights
            
            def predict(self, X):
                predictions = np.array([model.predict(X) for model in self.models.values()])
                weighted_pred = np.average(predictions, axis=0, weights=self.weights)
                return weighted_pred
            
            def fit(self, X, y):
                pass
        
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºéªŒè¯é›†RÂ²ï¼‰
        val_r2_scores = []
        for model in trained_models.values():
            val_pred = model.predict(X_val)
            val_r2 = r2_score(y_val, val_pred)
            val_r2_scores.append(max(val_r2, 0))  # ç¡®ä¿éè´Ÿ
        
        # å½’ä¸€åŒ–æƒé‡
        weights = np.array(val_r2_scores)
        weights = weights / np.sum(weights)
        print(f"  é›†æˆæƒé‡: {dict(zip(trained_models.keys(), weights))}")
        
        ensemble_models['Weighted_Ensemble'] = WeightedAverageEnsemble(trained_models, weights)
        
        # 3. Voting Regressor
        from sklearn.ensemble import VotingRegressor
        voting_estimators = [(name, model) for name, model in trained_models.items()]
        ensemble_models['Voting_Ensemble'] = VotingRegressor(
            estimators=voting_estimators,
            weights=weights
        )
        
        # è®­ç»ƒVotingé›†æˆ
        ensemble_models['Voting_Ensemble'].fit(X_train, y_train)
        
        # è¿”å›æ‰€æœ‰æ¨¡å‹ï¼ˆä¸ªä½“+é›†æˆï¼‰
        all_models = {**trained_models, **ensemble_models}
        return all_models
    
    def calculate_metrics(self, y_true, y_pred):
        """è®¡ç®—å…¨é¢çš„å›å½’æŒ‡æ ‡ï¼ˆä¸ä¸»ç±»ç›¸åŒï¼‰"""
        from sklearn.metrics import explained_variance_score, mean_squared_log_error
        
        # åŸºç¡€æŒ‡æ ‡
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # é¿å…é™¤é›¶é”™è¯¯å’Œè´Ÿå€¼
        y_true_safe = np.maximum(y_true, 1e-8)
        y_pred_safe = np.maximum(y_pred, 1e-8)
        
        # MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100
        
        # MSLE (Mean Squared Log Error) - éœ€è¦ç¡®ä¿å€¼ä¸ºæ­£
        try:
            if np.all(y_true >= 0) and np.all(y_pred >= 0):
                msle = mean_squared_log_error(y_true_safe, y_pred_safe)
            else:
                msle = np.nan
        except:
            msle = np.nan
        
        # Explained Variance Score
        evs = explained_variance_score(y_true, y_pred)
        
        # Mean Absolute Scaled Error (MASE) - ç›¸å¯¹äºnaive forecast
        naive_error = np.mean(np.abs(np.diff(y_true)))
        if naive_error > 0:
            mase = mae / naive_error
        else:
            mase = np.nan
        
        # Symmetric Mean Absolute Percentage Error (SMAPE)
        smape = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100
        
        # Median Absolute Error
        medae = np.median(np.abs(y_true - y_pred))
        
        # Max Error
        max_error = np.max(np.abs(y_true - y_pred))
        
        # Mean Absolute Error in %
        mae_percent = (mae / np.mean(y_true)) * 100
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'MAPE': mape,
            'MSLE': msle,
            'EVS': evs,  # Explained Variance Score
            'MASE': mase,  # Mean Absolute Scaled Error
            'SMAPE': smape,  # Symmetric MAPE
            'MedAE': medae,  # Median Absolute Error
            'MaxError': max_error,  # Maximum Error
            'MAE%': mae_percent  # MAE as percentage of mean
        }
    
    def evaluate_model(self, model, X_train, X_val, X_test, y_train, y_val, y_test, name):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼ˆåŒ…å«å½’ä¸€åŒ–å¤„ç†ï¼‰"""
        print(f"\n=== è¯„ä¼° {name} ===")
        
        start_time = time.time()
        
        # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰è®­ç»ƒï¼Œåˆ™è®­ç»ƒå®ƒ
        if hasattr(model, 'fit') and not hasattr(model, 'models'):
            model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # é¢„æµ‹ï¼ˆå½’ä¸€åŒ–çš„ç›®æ ‡å€¼ï¼‰
        try:
            y_train_pred_norm = model.predict(X_train)
            y_val_pred_norm = model.predict(X_val)
            y_test_pred_norm = model.predict(X_test)
            
            # åå½’ä¸€åŒ–é¢„æµ‹å€¼
            y_train_pred = self.target_scaler.inverse_transform(y_train_pred_norm.reshape(-1, 1)).flatten()
            y_val_pred = self.target_scaler.inverse_transform(y_val_pred_norm.reshape(-1, 1)).flatten()
            y_test_pred = self.target_scaler.inverse_transform(y_test_pred_norm.reshape(-1, 1)).flatten()
            
            # åå½’ä¸€åŒ–çœŸå®å€¼
            y_train_true = self.target_scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()
            y_val_true = self.target_scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
            y_test_true = self.target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
            
        except Exception as e:
            print(f"âŒ {name} é¢„æµ‹å¤±è´¥: {str(e)}")
            return None
        
        # è®¡ç®—æŒ‡æ ‡ï¼ˆåŸºäºåŸå§‹å°ºåº¦ï¼‰
        train_metrics = self.calculate_metrics(y_train_true, y_train_pred)
        val_metrics = self.calculate_metrics(y_val_true, y_val_pred)
        test_metrics = self.calculate_metrics(y_test_true, y_test_pred)
        
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"éªŒè¯é›† RMSE: {val_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RMSE: {test_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_metrics['RÂ²']:.4f}")
        print(f"æµ‹è¯•é›† MAPE: {test_metrics['MAPE']:.2f}%")
        print(f"æµ‹è¯•é›† SMAPE: {test_metrics['SMAPE']:.2f}%")
        
        return {
            'model': model,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'training_time': training_time,
            'predictions': {
                'y_train_pred': y_train_pred,
                'y_val_pred': y_val_pred,
                'y_test_pred': y_test_pred
            }
        }
    
    def run_optimal_experiment(self):
        """è¿è¡ŒåŸºäºRÂ²æœ€ä¼˜æ¨¡å‹çš„é›†æˆå­¦ä¹ å®éªŒ"""
        print("ğŸ¯ å¼€å§‹åŸºäºRÂ²æœ€ä¼˜æ¨¡å‹çš„é›†æˆå­¦ä¹ å®éªŒ")
        print("="*60)
        
        # 1. æ•°æ®é¢„å¤„ç†
        print("1. å¢å¼ºæ•°æ®é¢„å¤„ç†...")
        df = self.preprocessor.load_data()
        df_processed = self.preprocessor.prepare_features(df, use_lag_features=False, exclude_non_operating=True)
        
        # 2. æ•°æ®åˆ†å‰²
        X_train, X_val, X_test, y_train, y_val, y_test = self.preprocessor.split_data_temporal(df_processed)
        
        # 3. ç›®æ ‡å˜é‡å½’ä¸€åŒ–
        print("2. ç›®æ ‡å˜é‡å½’ä¸€åŒ–...")
        y_train_norm = self.target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
        y_val_norm = self.target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()
        y_test_norm = self.target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()
        
        print(f"ç›®æ ‡å˜é‡èŒƒå›´: {y_train.min():.0f} - {y_train.max():.0f}")
        print(f"å½’ä¸€åŒ–åèŒƒå›´: {y_train_norm.min():.3f} - {y_train_norm.max():.3f}")
        
        # 4. ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled = self.preprocessor.scale_features(X_train, X_val, X_test)
        
        print(f"\næ•°æ®æ¦‚è§ˆ:")
        print(f"  ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]}")
        print(f"  è®­ç»ƒæ ·æœ¬: {X_train_scaled.shape[0]}")
        print(f"  éªŒè¯æ ·æœ¬: {X_val_scaled.shape[0]}")
        print(f"  æµ‹è¯•æ ·æœ¬: {X_test_scaled.shape[0]}")
        
        # 5. åˆ›å»ºå’Œè®­ç»ƒæœ€ä¼˜æ¨¡å‹é›†æˆ
        print("\n3. åˆ›å»ºRÂ²æœ€ä¼˜æ¨¡å‹é›†æˆ...")
        all_models = self.create_ensemble_from_optimal(
            X_train_scaled, y_train_norm, X_val_scaled, y_val_norm
        )
        
        # 6. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        print(f"\n4. è¯„ä¼° {len(all_models)} ä¸ªä¼˜åŒ–æ¨¡å‹...")
        for i, (name, model) in enumerate(all_models.items(), 1):
            print(f"\næ¨¡å‹è¿›åº¦: {i}/{len(all_models)}")
            try:
                result = self.evaluate_model(
                    model, X_train_scaled, X_val_scaled, X_test_scaled,
                    y_train_norm, y_val_norm, y_test_norm, name
                )
                if result:
                    self.results[name] = result
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {name} è¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        if self.results:
            print(f"\nâœ… æˆåŠŸè¯„ä¼°äº† {len(self.results)} ä¸ªæ¨¡å‹")
            self.generate_optimal_report()
            self.save_optimal_results()
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹")
        
        return self.results
    
    def generate_optimal_report(self):
        """ç”Ÿæˆä¼˜åŒ–æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆä¼˜åŒ–æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š...")
        
        # åˆ›å»ºç»“æœDataFrame
        comparison_data = []
        for name, result in self.results.items():
            row = {
                'Model': name,
                'Train_RMSE': result['train_metrics']['RMSE'],
                'Val_RMSE': result['val_metrics']['RMSE'],
                'Test_RMSE': result['test_metrics']['RMSE'],
                'Test_MAE': result['test_metrics']['MAE'],
                'Test_RÂ²': result['test_metrics']['RÂ²'],
                'Test_MAPE': result['test_metrics']['MAPE'],
                'Test_SMAPE': result['test_metrics']['SMAPE'],
                'Test_MSLE': result['test_metrics']['MSLE'],
                'Test_EVS': result['test_metrics']['EVS'],
                'Test_MASE': result['test_metrics']['MASE'],
                'Test_MedAE': result['test_metrics']['MedAE'],
                'Test_MaxError': result['test_metrics']['MaxError'],
                'Test_MAE%': result['test_metrics']['MAE%'],
                'Training_Time': result['training_time']
            }
            comparison_data.append(row)
        
        results_df = pd.DataFrame(comparison_data)
        results_df = results_df.sort_values('Test_RÂ²', ascending=False)  # æŒ‰RÂ²æ’åº
        
        print("\nğŸ“ˆ ä¼˜åŒ–æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ (æŒ‰RÂ²æ’åº):")
        print("="*120)
        
        # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
        main_cols = ['Model', 'Test_RMSE', 'Test_MAE', 'Test_RÂ²', 'Test_MAPE', 'Test_SMAPE', 'Training_Time']
        print(results_df[main_cols].round(4).to_string(index=False))
        
        print("\nğŸ“Š è¯¦ç»†å›å½’æŒ‡æ ‡:")
        print("="*120)
        detail_cols = ['Model', 'Test_MSLE', 'Test_EVS', 'Test_MASE', 'Test_MedAE', 'Test_MaxError', 'Test_MAE%']
        detail_df = results_df[detail_cols].round(4)
        print(detail_df.to_string(index=False))
        
        # ä¿å­˜ç»“æœè¡¨æ ¼
        results_df.to_csv('optimal_ensemble_results.csv', index=False)
        
        # æœ€ä½³æ¨¡å‹åˆ†æ
        best_model = results_df.iloc[0]
        print(f"\nğŸ† æœ€ä½³ä¼˜åŒ–æ¨¡å‹:")
        print(f"  æ¨¡å‹: {best_model['Model']}")
        print(f"  æµ‹è¯•é›†RÂ²: {best_model['Test_RÂ²']:.4f}")
        print(f"  æµ‹è¯•é›†RMSE: {best_model['Test_RMSE']:.2f}")
        print(f"  æµ‹è¯•é›†MAPE: {best_model['Test_MAPE']:.2f}%")
        print(f"  æµ‹è¯•é›†SMAPE: {best_model['Test_SMAPE']:.2f}%")
        print(f"  è®­ç»ƒæ—¶é—´: {best_model['Training_Time']:.2f}ç§’")
        
        # ç›®æ ‡è¾¾æˆæƒ…å†µ
        target_rmse = 200
        target_r2 = 0.75
        
        print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        successful_models = 0
        for _, row in results_df.iterrows():
            rmse_ok = row['Test_RMSE'] < target_rmse
            r2_ok = row['Test_RÂ²'] > target_r2
            if rmse_ok and r2_ok:
                successful_models += 1
                print(f"  âœ… {row['Model']}: RMSE={row['Test_RMSE']:.2f}, RÂ²={row['Test_RÂ²']:.3f}")
            elif rmse_ok:
                print(f"  ğŸŸ¡ {row['Model']}: RMSEè¾¾æ ‡({row['Test_RMSE']:.2f}), RÂ²={row['Test_RÂ²']:.3f}")
            elif r2_ok:
                print(f"  ğŸŸ¡ {row['Model']}: RÂ²è¾¾æ ‡({row['Test_RÂ²']:.3f}), RMSE={row['Test_RMSE']:.2f}")
        
        print(f"\nğŸ“ˆ å®Œå…¨è¾¾æ ‡æ¨¡å‹: {successful_models}/{len(results_df)} ({successful_models/len(results_df)*100:.1f}%)")
        
        return results_df
    
    def save_optimal_results(self):
        """ä¿å­˜ä¼˜åŒ–å®éªŒç»“æœ"""
        # ä¿å­˜ä¼˜åŒ–æ¨¡å‹æ€»ç»“
        optimal_summary = {}
        for name, result in self.results.items():
            optimal_summary[name] = {
                'test_rmse': result['test_metrics']['RMSE'],
                'test_r2': result['test_metrics']['RÂ²'],
                'test_mae': result['test_metrics']['MAE'],
                'test_mape': result['test_metrics']['MAPE'],
                'test_smape': result['test_metrics']['SMAPE'],
                'test_msle': result['test_metrics']['MSLE'],
                'test_evs': result['test_metrics']['EVS'],
                'test_mase': result['test_metrics']['MASE'],
                'training_time': result['training_time']
            }
        
        # ä¿å­˜ä¸ºJSON
        import json
        with open('optimal_ensemble_summary.json', 'w', encoding='utf-8') as f:
            json.dump(optimal_summary, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ ä¼˜åŒ–å®éªŒç»“æœå·²ä¿å­˜:")
        print("  - optimal_ensemble_results.csv")
        print("  - optimal_ensemble_summary.json")

class EnsembleLearningExperiment:
    """åŸå§‹é›†æˆå­¦ä¹ å®éªŒç±»ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    
    def __init__(self):
        self.results = {}
        self.models = {}
        self.preprocessor = BikeDataPreprocessor()
        
    def calculate_metrics(self, y_true, y_pred):
        """è®¡ç®—å…¨é¢çš„å›å½’æŒ‡æ ‡"""
        from sklearn.metrics import explained_variance_score, mean_squared_log_error
        
        # åŸºç¡€æŒ‡æ ‡
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # é¿å…é™¤é›¶é”™è¯¯å’Œè´Ÿå€¼
        y_true_safe = np.maximum(y_true, 1e-8)
        y_pred_safe = np.maximum(y_pred, 1e-8)
        
        # MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100
        
        # MSLE (Mean Squared Log Error) - éœ€è¦ç¡®ä¿å€¼ä¸ºæ­£
        try:
            if np.all(y_true >= 0) and np.all(y_pred >= 0):
                msle = mean_squared_log_error(y_true_safe, y_pred_safe)
            else:
                msle = np.nan
        except:
            msle = np.nan
        
        # Explained Variance Score
        evs = explained_variance_score(y_true, y_pred)
        
        # Mean Absolute Scaled Error (MASE) - ç›¸å¯¹äºnaive forecast
        naive_error = np.mean(np.abs(np.diff(y_true)))
        if naive_error > 0:
            mase = mae / naive_error
        else:
            mase = np.nan
        
        # Symmetric Mean Absolute Percentage Error (SMAPE)
        smape = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100
        
        # Median Absolute Error
        medae = np.median(np.abs(y_true - y_pred))
        
        # Max Error
        max_error = np.max(np.abs(y_true - y_pred))
        
        # Mean Absolute Error in %
        mae_percent = (mae / np.mean(y_true)) * 100
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'MAPE': mape,
            'MSLE': msle,
            'EVS': evs,  # Explained Variance Score
            'MASE': mase,  # Mean Absolute Scaled Error
            'SMAPE': smape,  # Symmetric MAPE
            'MedAE': medae,  # Median Absolute Error
            'MaxError': max_error,  # Maximum Error
            'MAE%': mae_percent  # MAE as percentage of mean
        }
    
    def create_base_models(self):
        """åˆ›å»ºåŸºç¡€æ¨¡å‹ï¼ˆé’ˆå¯¹ç‰¹å¾ç²¾ç»†ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç‰¹å¾æ•°é‡ä¸º33ï¼Œæ®æ­¤ä¼˜åŒ–æ¨¡å‹å‚æ•°
        base_models = {
            'Linear Regression': LinearRegression(),
            
            'Ridge Regression': Ridge(
                alpha=0.05,  # é’ˆå¯¹33ä¸ªç‰¹å¾çš„é€‚ä¸­æ­£åˆ™åŒ–
                solver='saga',  # é€‚åˆä¸­ç­‰è§„æ¨¡æ•°æ®
                max_iter=3000
            ),
            
            'Lasso Regression': Lasso(
                alpha=0.005,  # æ›´å°çš„æ­£åˆ™åŒ–ä»¥ä¿ç•™æ›´å¤šç‰¹å¾
                max_iter=3000,
                selection='random',  # éšæœºç‰¹å¾é€‰æ‹©
                tol=1e-5
            ),
            
            'Random Forest': RandomForestRegressor(
                n_estimators=800,  # å¢åŠ æ ‘çš„æ•°é‡
                max_depth=None,  # ä¸é™åˆ¶æ·±åº¦ï¼Œè®©æ ‘å……åˆ†ç”Ÿé•¿
                min_samples_split=3,  # é’ˆå¯¹6132ä¸ªè®­ç»ƒæ ·æœ¬
                min_samples_leaf=2,
                max_features=int(33**0.5),  # sqrt(33) â‰ˆ 6ä¸ªç‰¹å¾
                bootstrap=True,
                oob_score=True,  # ä½¿ç”¨out-of-bagè¯„åˆ†
                random_state=42, 
                n_jobs=-1
            ),
            
            'XGBoost': xgb.XGBRegressor(
                n_estimators=800,
                learning_rate=0.03,  # æ›´å°çš„å­¦ä¹ ç‡é…åˆæ›´å¤šä¼°è®¡å™¨
                max_depth=12,  # é€‚ä¸­æ·±åº¦
                min_child_weight=3,  # é˜²æ­¢è¿‡æ‹Ÿåˆ
                subsample=0.85,  # è¡Œé‡‡æ ·
                colsample_bytree=0.8,  # ç‰¹å¾é‡‡æ ·
                colsample_bylevel=0.9,
                colsample_bynode=0.9,
                reg_alpha=0.05,  # L1æ­£åˆ™åŒ–
                reg_lambda=1.2,  # L2æ­£åˆ™åŒ–
                gamma=0.1,  # æœ€å°åˆ†è£‚æŸå¤±
                random_state=42, 
                n_jobs=-1, 
                verbosity=0
            ),
            
            'SVR': SVR(
                kernel='rbf',
                C=50.0,  # å¢åŠ å¤æ‚åº¦å®¹å¿åº¦
                gamma='scale',  # è‡ªåŠ¨ç¼©æ”¾
                epsilon=0.005,  # æ›´å°çš„epsilonç®¡é“
                tol=1e-4,  # æ”¶æ•›å®¹å¿åº¦
                cache_size=500  # å¢åŠ ç¼“å­˜
            ),
            
            'Neural Network': MLPRegressor(
                # ç½‘ç»œæ¶æ„ï¼šé’ˆå¯¹33ä¸ªç‰¹å¾è®¾è®¡
                hidden_layer_sizes=(128, 64, 32, 16),  # 4å±‚é€’å‡æ¶æ„
                
                # æ¿€æ´»å‡½æ•°å’Œæ±‚è§£å™¨
                activation='relu',  # ReLUæ¿€æ´»å‡½æ•°
                solver='adam',  # Adamä¼˜åŒ–å™¨
                
                # å­¦ä¹ ç‡å‚æ•°
                learning_rate='adaptive',  # è‡ªé€‚åº”å­¦ä¹ ç‡
                learning_rate_init=0.001,  # åˆå§‹å­¦ä¹ ç‡
                
                # æ­£åˆ™åŒ–
                alpha=0.001,  # L2æ­£åˆ™åŒ–
                
                # è®­ç»ƒå‚æ•°
                max_iter=5000,  # æ›´å¤šè¿­ä»£æ¬¡æ•°
                tol=1e-6,  # æ›´ä¸¥æ ¼çš„æ”¶æ•›æ¡ä»¶
                
                # æ—©åœç­–ç•¥
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=50,  # 50æ¬¡è¿­ä»£æ— æ”¹è¿›åˆ™åœæ­¢
                
                # æ‰¹å¤„ç†
                batch_size='auto',  # è‡ªåŠ¨æ‰¹å¤§å°
                
                # å…¶ä»–å‚æ•°
                shuffle=True,  # æ¯è½®è®­ç»ƒæ‰“ä¹±æ•°æ®
                random_state=42,
                warm_start=False
            )
        }
        return base_models
    
    def bagging_methods(self):
        """Baggingæ–¹æ³•"""
        print("ğŸ¯ æµ‹è¯•Baggingæ–¹æ³•...")
        
        bagging_models = {
            'Random Forest': RandomForestRegressor(
                n_estimators=1000,  # æ›´å¤šæ ‘
                max_depth=None,  # ä¸é™åˆ¶æ·±åº¦
                min_samples_split=3,
                min_samples_leaf=2,
                max_features=int(33**0.5),  # é’ˆå¯¹33ä¸ªç‰¹å¾ä¼˜åŒ–
                bootstrap=True,
                oob_score=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Extra Trees': ExtraTreesRegressor(
                n_estimators=1000,  # æ›´å¤šæ ‘
                max_depth=None,  # ä¸é™åˆ¶æ·±åº¦
                min_samples_split=3,
                min_samples_leaf=2,
                max_features=int(33**0.5),  # é’ˆå¯¹33ä¸ªç‰¹å¾ä¼˜åŒ–
                bootstrap=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging (Linear)': BaggingRegressor(
                estimator=LinearRegression(),
                n_estimators=100,  # å¢åŠ åŸºå­¦ä¹ å™¨æ•°é‡
                max_samples=0.75,  # é‡‡æ ·æ¯”ä¾‹
                max_features=0.85,  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹ï¼ˆé’ˆå¯¹33ä¸ªç‰¹å¾ï¼‰
                bootstrap=True,
                bootstrap_features=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging (Ridge)': BaggingRegressor(
                estimator=Ridge(alpha=0.05, solver='auto', max_iter=5000),
                n_estimators=100,
                max_samples=0.75,
                max_features=0.85,
                bootstrap=True,
                bootstrap_features=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging (SVR)': BaggingRegressor(
                estimator=SVR(
                    kernel='rbf', 
                    C=20.0, 
                    gamma='scale',
                    epsilon=0.01,
                    cache_size=300
                ),
                n_estimators=50,  # SVRè®¡ç®—é‡å¤§ï¼Œé€‚åº¦æ•°é‡
                max_samples=0.7,
                max_features=0.9,  # SVRå¯¹ç‰¹å¾æ•æ„Ÿï¼Œä¿ç•™æ›´å¤šç‰¹å¾
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging (Neural Network)': BaggingRegressor(
                estimator=MLPRegressor(
                    hidden_layer_sizes=(64, 32),  # ç›¸å¯¹ç®€å•çš„ç½‘ç»œ
                    activation='relu',
                    solver='adam',
                    learning_rate='adaptive',
                    learning_rate_init=0.002,
                    alpha=0.01,
                    max_iter=2000,
                    early_stopping=True,
                    validation_fraction=0.15,
                    n_iter_no_change=30,
                    random_state=42
                ),
                n_estimators=30,  # ç¥ç»ç½‘ç»œç»„åˆæ•°é‡
                max_samples=0.8,
                max_features=0.9,
                random_state=42,
                n_jobs=1  # ç¥ç»ç½‘ç»œé¿å…åµŒå¥—å¹¶è¡Œ
            )
        }
        
        return bagging_models
    
    def boosting_methods(self):
        """Boostingæ–¹æ³•"""
        print("ğŸš€ æµ‹è¯•Boostingæ–¹æ³•...")
        
        boosting_models = {
            'AdaBoost (Linear)': AdaBoostRegressor(
                estimator=LinearRegression(),
                n_estimators=200,  # å¤§å¹…å¢åŠ 
                learning_rate=0.8,
                random_state=42
            ),
            'AdaBoost (Ridge)': AdaBoostRegressor(
                estimator=Ridge(alpha=0.1),
                n_estimators=200,  # å¤§å¹…å¢åŠ 
                learning_rate=0.8,
                random_state=42
            ),
            'Gradient Boosting': GradientBoostingRegressor(
                n_estimators=500,  # å¤§å¹…å¢åŠ 
                learning_rate=0.05,
                max_depth=8,
                subsample=0.9,
                min_samples_split=2,
                min_samples_leaf=1,
                random_state=42
            ),
            'XGBoost': xgb.XGBRegressor(
                n_estimators=500,  # å¤§å¹…å¢åŠ 
                learning_rate=0.05,
                max_depth=10,
                subsample=0.9,
                colsample_bytree=0.9,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=42,
                n_jobs=-1,
                verbosity=0
            ),
            'XGBoost Tuned': xgb.XGBRegressor(
                n_estimators=1000,  # å¤§å¹…å¢åŠ 
                learning_rate=0.03,
                max_depth=12,
                subsample=0.95,
                colsample_bytree=0.95,
                colsample_bylevel=0.9,
                reg_alpha=0.05,
                reg_lambda=1.5,
                gamma=0.1,
                random_state=42,
                n_jobs=-1,
                verbosity=0
            ),
            'LightGBM': xgb.XGBRegressor(  # æ·»åŠ å¦ä¸€ä¸ªé«˜æ€§èƒ½æ¨¡å‹
                n_estimators=800,
                learning_rate=0.04,
                max_depth=11,
                subsample=0.92,
                colsample_bytree=0.92,
                reg_alpha=0.08,
                reg_lambda=1.2,
                random_state=42,
                n_jobs=-1,
                verbosity=0
            )
        }
        
        return boosting_models
    
    def voting_methods(self):
        """Votingæ–¹æ³•"""
        print("ğŸ—³ï¸ æµ‹è¯•Votingæ–¹æ³•...")
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹é›†åˆ1ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰
        linear_models = [
            ('linear', LinearRegression()),
            ('ridge', Ridge(alpha=1.0, solver='auto', max_iter=5000)),
            ('lasso', Lasso(alpha=0.1, max_iter=5000))
        ]
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹é›†åˆ2ï¼ˆé«˜æ€§èƒ½æ ‘æ¨¡å‹ï¼‰
        tree_models = [
            ('rf', RandomForestRegressor(n_estimators=300, max_depth=25, random_state=42, n_jobs=-1)),
            ('xgb', xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1, verbosity=0)),
            ('et', ExtraTreesRegressor(n_estimators=300, max_depth=25, random_state=42, n_jobs=-1))
        ]
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹é›†åˆ3ï¼ˆé«˜æ€§èƒ½æ··åˆæ¨¡å‹ï¼‰
        mixed_models = [
            ('linear', LinearRegression()),
            ('ridge', Ridge(alpha=0.1, solver='auto', max_iter=5000)),
            ('rf', RandomForestRegressor(n_estimators=300, max_depth=25, random_state=42, n_jobs=-1)),
            ('xgb', xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1, verbosity=0)),
            ('svr', SVR(kernel='rbf', C=10.0, gamma='scale'))
        ]
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹é›†åˆ4ï¼ˆæ‰€æœ‰é«˜æ€§èƒ½æ¨¡å‹ï¼‰
        all_models = [
            ('linear', LinearRegression()),
            ('ridge', Ridge(alpha=0.1, solver='auto', max_iter=5000)),
            ('lasso', Lasso(alpha=0.01, max_iter=5000)),
            ('rf', RandomForestRegressor(n_estimators=300, max_depth=25, random_state=42, n_jobs=-1)),
            ('xgb', xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1, verbosity=0)),
            ('et', ExtraTreesRegressor(n_estimators=300, max_depth=25, random_state=42, n_jobs=-1)),
            ('svr', SVR(kernel='rbf', C=10.0, gamma='scale'))
        ]
        
        voting_models = {
            'Voting (Linear Models)': VotingRegressor(
                estimators=linear_models,
                weights=None  # ç­‰æƒé‡
            ),
            'Voting (Tree Models)': VotingRegressor(
                estimators=tree_models,
                weights=[1, 1.5, 1.2]  # ç»™XGBoostæ›´é«˜æƒé‡
            ),
            'Voting (Tree Models - Equal)': VotingRegressor(
                estimators=tree_models,
                weights=None  # ç­‰æƒé‡
            ),
            'Voting (Mixed - Equal)': VotingRegressor(
                estimators=mixed_models,
                weights=None  # ç­‰æƒé‡
            ),
            'Voting (Mixed - Weighted)': VotingRegressor(
                estimators=mixed_models,
                weights=[0.5, 0.8, 2.5, 3.0, 1.2]  # ç»™æ ‘æ¨¡å‹æ›´é«˜æƒé‡
            ),
            'Voting (All Models - Equal)': VotingRegressor(
                estimators=all_models,
                weights=None  # ç­‰æƒé‡
            ),
            'Voting (All Models - Weighted)': VotingRegressor(
                estimators=all_models,
                weights=[0.5, 0.8, 0.6, 2.5, 3.0, 2.2, 1.0]  # åå‘æ ‘æ¨¡å‹
            ),
            'Voting (Best Models)': VotingRegressor(
                estimators=[
                    ('rf', RandomForestRegressor(n_estimators=400, max_depth=25, random_state=42, n_jobs=-1)),
                    ('xgb', xgb.XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1, verbosity=0)),
                    ('et', ExtraTreesRegressor(n_estimators=400, max_depth=25, random_state=42, n_jobs=-1))
                ],
                weights=[1.2, 1.5, 1.0]  # XGBoostæƒé‡æœ€é«˜
            )
        }
        
        return voting_models
    
    def stacking_methods(self, X_train, y_train, X_val, y_val):
        """Stackingæ–¹æ³•"""
        print("ğŸ—ï¸ æµ‹è¯•Stackingæ–¹æ³•...")
        
        # ç¬¬ä¸€å±‚æ¨¡å‹ï¼ˆé’ˆå¯¹ç‰¹å¾ä¼˜åŒ–çš„åŸºç¡€å­¦ä¹ å™¨ï¼‰
        base_models = {
            'linear': LinearRegression(),
            
            'ridge': Ridge(
                alpha=0.05, 
                solver='auto', 
                max_iter=5000
            ),
            
            'lasso': Lasso(
                alpha=0.005, 
                max_iter=3000, 
                selection='random'
            ),
            
            'rf': RandomForestRegressor(
                n_estimators=400, 
                max_depth=None, 
                min_samples_split=3,
                min_samples_leaf=2,
                max_features=int(33**0.5),
                oob_score=True,
                random_state=42, 
                n_jobs=-1
            ),
            
            'xgb': xgb.XGBRegressor(
                n_estimators=400, 
                learning_rate=0.03, 
                max_depth=12,
                min_child_weight=3,
                subsample=0.85,
                colsample_bytree=0.8,
                reg_alpha=0.05,
                reg_lambda=1.2,
                random_state=42, 
                n_jobs=-1, 
                verbosity=0
            ),
            
            'et': ExtraTreesRegressor(
                n_estimators=400, 
                max_depth=None,
                min_samples_split=3,
                min_samples_leaf=2,
                max_features=int(33**0.5),
                random_state=42, 
                n_jobs=-1
            ),
            
            'svr': SVR(
                kernel='rbf', 
                C=30.0, 
                gamma='scale',
                epsilon=0.01,
                cache_size=400
            ),
            
            'nn': MLPRegressor(
                hidden_layer_sizes=(96, 48, 24),  # é’ˆå¯¹Stackingä¼˜åŒ–çš„ç½‘ç»œ
                activation='relu',
                solver='adam',
                learning_rate='adaptive',
                learning_rate_init=0.001,
                alpha=0.001,
                max_iter=3000,
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=40,
                random_state=42
            )
        }
        
        # è®­ç»ƒåŸºç¡€æ¨¡å‹å¹¶è·å–é¢„æµ‹
        base_predictions_train = np.zeros((X_train.shape[0], len(base_models)))
        base_predictions_val = np.zeros((X_val.shape[0], len(base_models)))
        
        for i, (name, model) in enumerate(base_models.items()):
            print(f"  è®­ç»ƒåŸºç¡€æ¨¡å‹: {name}")
            model.fit(X_train, y_train)
            base_predictions_train[:, i] = model.predict(X_train)
            base_predictions_val[:, i] = model.predict(X_val)
        
        # ç¬¬äºŒå±‚æ¨¡å‹ï¼ˆé’ˆå¯¹åŸºå­¦ä¹ å™¨è¾“å‡ºä¼˜åŒ–çš„å…ƒå­¦ä¹ å™¨ï¼‰
        # åŸºå­¦ä¹ å™¨æœ‰8ä¸ªè¾“å‡ºï¼Œé’ˆå¯¹è¿™ä¸ªç‰¹å¾æ•°é‡è®¾è®¡å…ƒå­¦ä¹ å™¨
        meta_models = {
            'Stacking (Linear)': LinearRegression(),
            
            'Stacking (Ridge)': Ridge(
                alpha=0.1,  # é’ˆå¯¹8ä¸ªåŸºå­¦ä¹ å™¨è¾“å‡ºçš„æ­£åˆ™åŒ–
                solver='auto'
            ),
            
            'Stacking (Lasso)': Lasso(
                alpha=0.02,  # é€‚ä¸­çš„ç¨€ç–åŒ–
                max_iter=2000
            ),
            
            'Stacking (RF)': RandomForestRegressor(
                n_estimators=300,
                max_depth=10,  # è¾ƒæµ…çš„æ ‘ï¼Œé¿å…åœ¨å°‘é‡ç‰¹å¾ä¸Šè¿‡æ‹Ÿåˆ
                min_samples_split=5,
                min_samples_leaf=3,
                max_features=3,  # åœ¨8ä¸ªç‰¹å¾ä¸­é€‰æ‹©3ä¸ª
                random_state=42, 
                n_jobs=-1
            ),
            
            'Stacking (XGBoost)': xgb.XGBRegressor(
                n_estimators=300,
                learning_rate=0.08,  # ç›¸å¯¹è¾ƒé«˜çš„å­¦ä¹ ç‡
                max_depth=6,  # è¾ƒæµ…çš„æ ‘
                min_child_weight=5,
                subsample=0.9,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.5,
                random_state=42, 
                n_jobs=-1, 
                verbosity=0
            ),
            
            'Stacking (ExtraTrees)': ExtraTreesRegressor(
                n_estimators=300,
                max_depth=8,  # æ§åˆ¶æ·±åº¦
                min_samples_split=5,
                min_samples_leaf=3,
                max_features=3,  # é’ˆå¯¹8ä¸ªè¾“å…¥ç‰¹å¾
                random_state=42, 
                n_jobs=-1
            ),
            
            'Stacking (Neural Network)': MLPRegressor(
                hidden_layer_sizes=(24, 12),  # é’ˆå¯¹8ä¸ªè¾“å…¥çš„ç®€å•ç½‘ç»œ
                activation='relu',
                solver='adam',
                learning_rate='constant',
                learning_rate_init=0.01,  # è¾ƒé«˜çš„å­¦ä¹ ç‡
                alpha=0.01,
                max_iter=2000,
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=30,
                random_state=42
            )
        }
        
        stacking_results = {}
        
        for name, meta_model in meta_models.items():
            print(f"  è®­ç»ƒå…ƒæ¨¡å‹: {name}")
            meta_model.fit(base_predictions_train, y_train)
            
            # åˆ›å»ºç»„åˆæ¨¡å‹ç±»
            class StackingModel:
                def __init__(self, base_models, meta_model):
                    self.base_models = base_models
                    self.meta_model = meta_model
                
                def predict(self, X):
                    base_preds = np.zeros((X.shape[0], len(self.base_models)))
                    for i, model in enumerate(self.base_models.values()):
                        base_preds[:, i] = model.predict(X)
                    return self.meta_model.predict(base_preds)
                
                def fit(self, X, y):
                    pass  # å·²ç»è®­ç»ƒå¥½äº†
            
            stacking_model = StackingModel(base_models, meta_model)
            stacking_results[name] = stacking_model
        
        return stacking_results
    
    def evaluate_model(self, model, X_train, X_val, X_test, y_train, y_val, y_test, name):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\n=== è¯„ä¼° {name} ===")
        
        start_time = time.time()
        
        # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰è®­ç»ƒï¼Œåˆ™è®­ç»ƒå®ƒ
        if hasattr(model, 'fit') and not hasattr(model, 'base_models'):
            model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # é¢„æµ‹
        try:
            y_train_pred = model.predict(X_train)
            y_val_pred = model.predict(X_val)
            y_test_pred = model.predict(X_test)
        except Exception as e:
            print(f"âŒ {name} é¢„æµ‹å¤±è´¥: {str(e)}")
            return None
        
        # è®¡ç®—æŒ‡æ ‡
        train_metrics = self.calculate_metrics(y_train, y_train_pred)
        val_metrics = self.calculate_metrics(y_val, y_val_pred)
        test_metrics = self.calculate_metrics(y_test, y_test_pred)
        
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"éªŒè¯é›† RMSE: {val_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RMSE: {test_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_metrics['RÂ²']:.4f}")
        
        return {
            'model': model,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'training_time': training_time,
            'predictions': {
                'y_train_pred': y_train_pred,
                'y_val_pred': y_val_pred,
                'y_test_pred': y_test_pred
            }
        }
    
    def run_experiment(self):
        """è¿è¡Œé›†æˆå­¦ä¹ å®éªŒ"""
        print("ğŸš€ å¼€å§‹å®éªŒ3ï¼šé›†æˆå­¦ä¹ æ–¹æ³•æ¯”è¾ƒ")
        print("="*50)
        
        # æ•°æ®é¢„å¤„ç† - ä½¿ç”¨å¢å¼ºç‰¹å¾å·¥ç¨‹
        print("1. åŸºäºæ•°æ®æ´å¯Ÿçš„å¢å¼ºæ•°æ®é¢„å¤„ç†...")
        df = self.preprocessor.load_data()
        print(f"åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # ä½¿ç”¨å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼Œæ’é™¤éè¿è¥æ—¥
        df_processed = self.preprocessor.prepare_features(df, use_lag_features=False, exclude_non_operating=True)
        print(f"å¤„ç†åæ•°æ®: {len(df_processed)} æ¡è®°å½•")
        print("å¢å¼ºç‰¹å¾åŒ…æ‹¬: æ¸©åº¦åˆ†æ®µã€åŒå³°æ—¶é—´æ¨¡å¼ã€èˆ’é€‚åº¦æŒ‡æ•°ã€æç«¯å¤©æ°”æ ‡è¯†ç­‰")
        
        # æ•°æ®åˆ†å‰²
        X_train, X_val, X_test, y_train, y_val, y_test = \
            self.preprocessor.split_data_temporal(df_processed)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled = \
            self.preprocessor.scale_features(X_train, X_val, X_test)
        
        print(f"\næ•°æ®æ¦‚è§ˆ:")
        print(f"  ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]}")
        print(f"  è®­ç»ƒæ ·æœ¬: {X_train_scaled.shape[0]}")
        print(f"  éªŒè¯æ ·æœ¬: {X_val_scaled.shape[0]}")
        print(f"  æµ‹è¯•æ ·æœ¬: {X_test_scaled.shape[0]}")
        
        print("\n2. è¯„ä¼°åŸºç¡€æ¨¡å‹ï¼ˆä¸å®éªŒä¸€ä¸€è‡´ï¼‰...")
        
        # é¦–å…ˆè¯„ä¼°åŸºç¡€æ¨¡å‹ä½œä¸ºå‚è€ƒ
        base_models = self.create_base_models()
        print(f"è¯„ä¼° {len(base_models)} ä¸ªåŸºç¡€æ¨¡å‹...")
        
        for i, (name, model) in enumerate(base_models.items(), 1):
            print(f"\nåŸºç¡€æ¨¡å‹è¿›åº¦: {i}/{len(base_models)}")
            try:
                result = self.evaluate_model(
                    model, X_train_scaled, X_val_scaled, X_test_scaled,
                    y_train, y_val, y_test, f"Base: {name}"
                )
                if result:
                    self.results[f"Base: {name}"] = result
            except Exception as e:
                print(f"âŒ åŸºç¡€æ¨¡å‹ {name} è¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        print("\n3. å¼€å§‹é›†æˆå­¦ä¹ å®éªŒ...")
        
        # æµ‹è¯•å„ç§é›†æˆæ–¹æ³•
        all_models = {}
        
        # Baggingæ–¹æ³•
        bagging_models = self.bagging_methods()
        all_models.update(bagging_models)
        
        # Boostingæ–¹æ³•
        boosting_models = self.boosting_methods()
        all_models.update(boosting_models)
        
        # Votingæ–¹æ³•
        voting_models = self.voting_methods()
        all_models.update(voting_models)
        
        # Stackingæ–¹æ³•
        stacking_models = self.stacking_methods(
            X_train_scaled, y_train, X_val_scaled, y_val
        )
        all_models.update(stacking_models)
        
        # è¯„ä¼°æ‰€æœ‰é›†æˆæ¨¡å‹
        print(f"\n4. è¯„ä¼° {len(all_models)} ä¸ªé›†æˆæ¨¡å‹...")
        
        for i, (name, model) in enumerate(all_models.items(), 1):
            print(f"\né›†æˆæ¨¡å‹è¿›åº¦: {i}/{len(all_models)}")
            try:
                result = self.evaluate_model(
                    model, X_train_scaled, X_val_scaled, X_test_scaled,
                    y_train, y_val, y_test, f"Ensemble: {name}"
                )
                if result:
                    self.results[f"Ensemble: {name}"] = result
            except Exception as e:
                print(f"âŒ é›†æˆæ¨¡å‹ {name} è¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        print("\nâœ… é›†æˆå­¦ä¹ å®éªŒå®Œæˆ!")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_comparison_report(y_test)
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        return self.results
    
    def generate_comparison_report(self, y_test):
        """ç”Ÿæˆé›†æˆæ–¹æ³•æ¯”è¾ƒæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆé›†æˆå­¦ä¹ æ¯”è¾ƒæŠ¥å‘Š...")
        
        # åˆ›å»ºç»“æœDataFrame
        comparison_data = []
        for name, result in self.results.items():
            # ç¡®å®šæ¨¡å‹ç±»å‹
            if name.startswith('Base:'):
                method_type = 'Base Model'
            elif 'Random Forest' in name or 'Extra Trees' in name or 'Bagging' in name:
                method_type = 'Bagging'
            elif 'Boosting' in name or 'XGBoost' in name or 'AdaBoost' in name:
                method_type = 'Boosting'
            elif 'Voting' in name:
                method_type = 'Voting'
            elif 'Stacking' in name:
                method_type = 'Stacking'
            else:
                method_type = 'Other'
            
            row = {
                'Model': name,
                'Type': method_type,
                'Train_RMSE': result['train_metrics']['RMSE'],
                'Val_RMSE': result['val_metrics']['RMSE'],
                'Test_RMSE': result['test_metrics']['RMSE'],
                'Test_MAE': result['test_metrics']['MAE'],
                'Test_RÂ²': result['test_metrics']['RÂ²'],
                'Test_MAPE': result['test_metrics']['MAPE'],
                'Training_Time': result['training_time']
            }
            comparison_data.append(row)
        
        results_df = pd.DataFrame(comparison_data)
        results_df = results_df.sort_values('Test_RMSE')
        
        print("\nğŸ“ˆ é›†æˆå­¦ä¹ æ–¹æ³•æ¯”è¾ƒ (æŒ‰æµ‹è¯•é›†RMSEæ’åº):")
        print("="*100)
        print(results_df.round(4).to_string(index=False))
        
        # ä¿å­˜ç»“æœè¡¨æ ¼
        results_df.to_csv('experiment_3_results.csv', index=False)
        
        # æŒ‰ç±»å‹åˆ†æ
        print(f"\nğŸ“Š æŒ‰æ¨¡å‹ç±»å‹åˆ†æ:")
        for method_type in ['Base Model', 'Bagging', 'Boosting', 'Voting', 'Stacking']:
            type_results = results_df[results_df['Type'] == method_type]
            if not type_results.empty:
                best_model = type_results.iloc[0]
                print(f"  {method_type}: æœ€ä½³æ¨¡å‹ {best_model['Model']}, RMSE={best_model['Test_RMSE']:.2f}")
        
        # åŸºç¡€æ¨¡å‹ vs é›†æˆæ¨¡å‹æ¯”è¾ƒ
        base_results = results_df[results_df['Type'] == 'Base Model']
        ensemble_results = results_df[results_df['Type'] != 'Base Model']
        
        if not base_results.empty and not ensemble_results.empty:
            best_base = base_results.iloc[0]
            best_ensemble = ensemble_results.iloc[0]
            
            print(f"\nğŸ¥‡ åŸºç¡€æ¨¡å‹ vs é›†æˆæ¨¡å‹:")
            print(f"  æœ€ä½³åŸºç¡€æ¨¡å‹: {best_base['Model']} (RMSE: {best_base['Test_RMSE']:.2f})")
            print(f"  æœ€ä½³é›†æˆæ¨¡å‹: {best_ensemble['Model']} (RMSE: {best_ensemble['Test_RMSE']:.2f})")
            
            improvement = (best_base['Test_RMSE'] - best_ensemble['Test_RMSE']) / best_base['Test_RMSE'] * 100
            print(f"  é›†æˆå­¦ä¹ æ”¹è¿›: {improvement:.2f}%")
        
        # åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(results_df, y_test)
        
        # æ€§èƒ½åˆ†æ
        print(f"\nğŸ† æœ€ä½³é›†æˆæ¨¡å‹:")
        best_model = results_df.iloc[0]
        print(f"  æ¨¡å‹: {best_model['Model']}")
        print(f"  ç±»å‹: {best_model['Type']}")
        print(f"  æµ‹è¯•é›†RMSE: {best_model['Test_RMSE']:.2f}")
        print(f"  æµ‹è¯•é›†RÂ²: {best_model['Test_RÂ²']:.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {best_model['Training_Time']:.2f}ç§’")
        
        # è¾¾åˆ°ç›®æ ‡æ£€æŸ¥
        target_rmse = 200
        target_r2 = 0.75
        
        print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        successful_models = 0
        for _, row in results_df.iterrows():
            rmse_ok = row['Test_RMSE'] < target_rmse
            r2_ok = row['Test_RÂ²'] > target_r2
            if rmse_ok and r2_ok:
                successful_models += 1
                print(f"  âœ… {row['Model']}: RMSE={row['Test_RMSE']:.2f}, RÂ²={row['Test_RÂ²']:.3f}")
        
        print(f"\nğŸ“ˆ æˆåŠŸè¾¾æ ‡æ¨¡å‹: {successful_models}/{len(results_df)} ({successful_models/len(results_df)*100:.1f}%)")
    
    def create_visualizations(self, results_df, y_test):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        
        plt.style.use('default')
        fig = plt.figure(figsize=(20, 20))
        
        # 1. å„ç±»é›†æˆæ–¹æ³•æ€§èƒ½æ¯”è¾ƒ
        plt.subplot(3, 3, 1)
        method_types = results_df['Type'].unique()
        type_rmse = []
        type_labels = []
        
        for method_type in method_types:
            type_data = results_df[results_df['Type'] == method_type]
            avg_rmse = type_data['Test_RMSE'].mean()
            type_rmse.append(avg_rmse)
            type_labels.append(f"{method_type}\n({len(type_data)}ä¸ª)")
        
        bars = plt.bar(range(len(type_labels)), type_rmse, color='skyblue', alpha=0.7)
        plt.xlabel('é›†æˆæ–¹æ³•ç±»å‹')
        plt.ylabel('å¹³å‡æµ‹è¯•é›† RMSE')
        plt.title('é›†æˆæ–¹æ³•ç±»å‹æ€§èƒ½æ¯”è¾ƒ')
        plt.xticks(range(len(type_labels)), type_labels)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(type_rmse):
            plt.text(i, v + 5, f'{v:.1f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 2. æ‰€æœ‰æ¨¡å‹RMSEæ¯”è¾ƒ
        plt.subplot(3, 3, 2)
        models = results_df['Model'][:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
        rmse_values = results_df['Test_RMSE'][:10]
        
        bars = plt.bar(range(len(models)), rmse_values, color='lightcoral', alpha=0.7)
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('æµ‹è¯•é›† RMSE')
        plt.title('Top 10 æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ')
        plt.xticks(range(len(models)), models, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(rmse_values):
            plt.text(i, v + 5, f'{v:.1f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 3. RÂ²åˆ†æ•°æ¯”è¾ƒ
        plt.subplot(3, 3, 3)
        r2_values = results_df['Test_RÂ²'][:10]
        bars = plt.bar(range(len(models)), r2_values, color='lightgreen', alpha=0.7)
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('æµ‹è¯•é›† RÂ²')
        plt.title('Top 10 æ¨¡å‹ RÂ²æ¯”è¾ƒ')
        plt.xticks(range(len(models)), models, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(r2_values):
            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 4. è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
        plt.subplot(3, 3, 4)
        time_values = results_df['Training_Time'][:10]
        bars = plt.bar(range(len(models)), time_values, color='orange', alpha=0.7)
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')
        plt.title('Top 10 æ¨¡å‹è®­ç»ƒæ—¶é—´æ¯”è¾ƒ')
        plt.xticks(range(len(models)), models, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(time_values):
            plt.text(i, v + max(time_values)*0.02, f'{v:.1f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 5. æ€§èƒ½vså¤æ‚åº¦æ•£ç‚¹å›¾
        plt.subplot(3, 3, 5)
        plt.scatter(results_df['Training_Time'], results_df['Test_RMSE'], 
                   s=100, alpha=0.7, c='purple')
        
        for i, model in enumerate(results_df['Model'][:10]):
            plt.annotate(model, (results_df['Training_Time'].iloc[i], results_df['Test_RMSE'].iloc[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.xlabel('è®­ç»ƒæ—¶é—´ (ç§’)')
        plt.ylabel('æµ‹è¯•é›† RMSE')
        plt.title('æ€§èƒ½ vs è®­ç»ƒå¤æ‚åº¦')
        plt.grid(True, alpha=0.3)
        
        # 6. æœ€ä½³æ¨¡å‹é¢„æµ‹æ•ˆæœ
        plt.subplot(3, 3, 6)
        best_model_name = results_df.iloc[0]['Model']
        best_predictions = self.results[best_model_name]['predictions']['y_test_pred']
        
        # éšæœºé€‰æ‹©ä¸€äº›ç‚¹è¿›è¡Œå¯è§†åŒ–
        sample_indices = np.random.choice(len(y_test), size=min(200, len(y_test)), replace=False)
        sample_indices = sorted(sample_indices)
        
        plt.scatter(y_test.iloc[sample_indices], best_predictions[sample_indices], 
                   alpha=0.6, s=20)
        
        # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
        min_val = min(y_test.min(), best_predictions.min())
        max_val = max(y_test.max(), best_predictions.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')
        
        plt.xlabel('çœŸå®å€¼')
        plt.ylabel('é¢„æµ‹å€¼')
        plt.title(f'{best_model_name} - é¢„æµ‹æ•ˆæœ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 7. å„ç±»å‹æ–¹æ³•çš„RÂ²åˆ†å¸ƒ
        plt.subplot(3, 3, 7)
        type_r2_data = []
        type_labels_clean = []
        
        for method_type in method_types:
            type_data = results_df[results_df['Type'] == method_type]
            type_r2_data.append(type_data['Test_RÂ²'].values)
            type_labels_clean.append(method_type)
        
        plt.boxplot(type_r2_data, labels=type_labels_clean)
        plt.ylabel('æµ‹è¯•é›† RÂ²')
        plt.title('å„é›†æˆç±»å‹ RÂ²åˆ†å¸ƒ')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # 8. è¯¯å·®åˆ†å¸ƒï¼ˆæœ€ä½³æ¨¡å‹ï¼‰
        plt.subplot(3, 3, 8)
        errors = y_test.values - best_predictions
        plt.hist(errors, bins=30, alpha=0.7, color='red', edgecolor='black')
        plt.xlabel('é¢„æµ‹è¯¯å·®')
        plt.ylabel('é¢‘æ¬¡')
        plt.title(f'{best_model_name} - è¯¯å·®åˆ†å¸ƒ')
        plt.axvline(x=0, color='black', linestyle='--', alpha=0.8)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        plt.text(0.02, 0.98, f'å‡å€¼: {errors.mean():.2f}\næ ‡å‡†å·®: {errors.std():.2f}', 
                transform=plt.gca().transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # 9. æ–¹æ³•æ•ˆç‡åˆ†æ
        plt.subplot(3, 3, 9)
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡ï¼ˆæ€§èƒ½/æ—¶é—´ï¼‰
        efficiency = 1 / (results_df['Test_RMSE'] * results_df['Training_Time'])
        
        bars = plt.bar(range(len(models)), efficiency[:10], color='gold', alpha=0.7)
        plt.xlabel('æ¨¡å‹')
        plt.ylabel('æ•ˆç‡æŒ‡æ ‡ (1/(RMSEÃ—æ—¶é—´))')
        plt.title('æ¨¡å‹æ•ˆç‡æ¯”è¾ƒ')
        plt.xticks(range(len(models)), models, rotation=45, ha='right')
        plt.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('experiment_3_ensemble_learning.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“¸ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: experiment_3_ensemble_learning.png")
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        # ä¿å­˜é›†æˆå­¦ä¹ æ€»ç»“
        ensemble_summary = {}
        for name, result in self.results.items():
            ensemble_summary[name] = {
                'test_rmse': result['test_metrics']['RMSE'],
                'test_r2': result['test_metrics']['RÂ²'],
                'test_mae': result['test_metrics']['MAE'],
                'test_mape': result['test_metrics']['MAPE'],
                'training_time': result['training_time']
            }
        
        # ä¿å­˜ä¸ºJSON
        import json
        with open('experiment_3_summary.json', 'w', encoding='utf-8') as f:
            json.dump(ensemble_summary, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜:")
        print("  - experiment_3_results.csv")
        print("  - experiment_3_summary.json")
        print("  - experiment_3_ensemble_learning.png")

def run_stratified_experiment(X_train, X_val, X_test, y_train, y_val, y_test, 
                             stratum_name="åˆ†å±‚"):
    """è¿è¡Œåˆ†å±‚å®éªŒ"""
    print(f"\nğŸ”¬ {stratum_name} åˆ†å±‚å®éªŒ")
    print("-" * 60)
    
    # æ£€æŸ¥æ•°æ®å¤§å°
    if len(X_train) < 100:
        print(f"âš ï¸  æ•°æ®é‡è¿‡å° ({len(X_train)} æ¡)ï¼Œè·³è¿‡è¯¥åˆ†å±‚")
        return None
    
    # åˆ›å»ºå®éªŒå¯¹è±¡
    experiment = EnsembleLearningExperiment()
    
    # é‡æ–°åˆå§‹åŒ–ç»“æœå­—å…¸
    experiment.results = {}
    
    # è¿è¡Œä¸»è¦é›†æˆæ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("è¿è¡Œä¸»è¦é›†æˆæ–¹æ³•...")
    
    try:
        # é€‰æ‹©æœ€æœ‰æ•ˆçš„å‡ ç§é›†æˆæ–¹æ³•
        selected_models = {}
        
        # Bagging: Random Forest
        selected_models['RF Enhanced'] = RandomForestRegressor(
            n_estimators=300, max_depth=None, 
            min_samples_split=3, min_samples_leaf=2,
            random_state=42, n_jobs=-1
        )
        
        # Boosting: XGBoost
        selected_models['XGBoost Enhanced'] = xgb.XGBRegressor(
            n_estimators=300, learning_rate=0.05, max_depth=10,
            subsample=0.85, colsample_bytree=0.8,
            random_state=42, n_jobs=-1, verbosity=0
        )
        
        # Voting: Best combination
        selected_models['Voting Enhanced'] = VotingRegressor(
            estimators=[
                ('rf', RandomForestRegressor(n_estimators=200, max_depth=25, random_state=42, n_jobs=-1)),
                ('xgb', xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42, n_jobs=-1, verbosity=0)),
                ('et', ExtraTreesRegressor(n_estimators=200, max_depth=25, random_state=42, n_jobs=-1))
            ],
            weights=[1.2, 1.5, 1.0]
        )
        
        # è¯„ä¼°é€‰å®šçš„æ¨¡å‹
        for name, model in selected_models.items():
            try:
                result = experiment.evaluate_model(
                    model, X_train, X_val, X_test, y_train, y_val, y_test, name
                )
                if result:
                    experiment.results[name] = result
                    rmse = result['test_metrics']['RMSE']
                    r2 = result['test_metrics']['RÂ²']
                    print(f"  âœ… {name}: RMSE={rmse:.2f}, RÂ²={r2:.3f}")
            except Exception as e:
                print(f"  âŒ {name} å¤±è´¥: {e}")
        
        # è¿”å›æœ€ä½³ç»“æœ
        if experiment.results:
            best_result = min(experiment.results.items(), key=lambda x: x[1]['test_metrics']['RMSE'])
            return {
                'best_model': best_result[0],
                'best_rmse': best_result[1]['test_metrics']['RMSE'],
                'best_r2': best_result[1]['test_metrics']['RÂ²'],
                'results_count': len(experiment.results)
            }
        else:
            return None
            
    except Exception as e:
        print(f"âŒ åˆ†å±‚å®éªŒå¤±è´¥: {e}")
        return None

def main():
    """åŸºäºæ•°æ®æ´å¯Ÿçš„å¢å¼ºå®éªŒä¸‰"""
    print("=" * 80)
    print("ğŸš€ å®éªŒä¸‰ï¼šåŸºäºæ•°æ®æ´å¯Ÿçš„å¢å¼ºé›†æˆå­¦ä¹ ")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = EnsembleLearningExperiment()
    
    # è¿è¡Œä¸»å®éªŒ
    print("ğŸ“Š è¿è¡Œå…¨æ•°æ®é›†é›†æˆå­¦ä¹ å®éªŒ...")
    results = experiment.run_experiment()
    
    print(f"\nâœ… ä¸»å®éªŒå®Œæˆï¼æµ‹è¯•äº† {len(results)} ä¸ªé›†æˆæ¨¡å‹")
    
    # åˆ†å±‚å»ºæ¨¡å®éªŒï¼ˆå¯é€‰ï¼‰
    print("\n" + "=" * 80)
    print("ğŸ¯ åˆ†å±‚å»ºæ¨¡å®éªŒï¼ˆåŸºäºæ•°æ®æ´å¯Ÿï¼‰")
    print("=" * 80)
    
    try:
        # å‡†å¤‡é¢„å¤„ç†å™¨
        preprocessor = BikeDataPreprocessor()
        df = preprocessor.load_data()
        df_processed = preprocessor.prepare_features(df, use_lag_features=False, exclude_non_operating=True)
        
        # å‡†å¤‡åˆ†å±‚æ•°æ®
        stratified_data = preprocessor.prepare_stratified_data(df_processed)
        
        stratified_results = {}
        
        # æŒ‰å­£èŠ‚åˆ†å±‚å®éªŒ
        print("\nğŸ“… æŒ‰å­£èŠ‚åˆ†å±‚å®éªŒ")
        for season in ['summer', 'winter', 'spring', 'autumn']:
            if f'season_{season}' in stratified_data:
                season_df = stratified_data[f'season_{season}']
                if len(season_df) > 500:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                    try:
                        # ç‰¹å¾å·¥ç¨‹
                        season_processed = preprocessor.prepare_features(
                            season_df, use_lag_features=False, exclude_non_operating=False
                        )
                        
                        # åˆ†å‰²æ•°æ®
                        X_train_s, X_val_s, X_test_s, y_train_s, y_val_s, y_test_s = \
                            preprocessor.split_data_temporal(season_processed)
                        
                        # æ ‡å‡†åŒ–
                        X_train_s_scaled, X_val_s_scaled, X_test_s_scaled = \
                            preprocessor.scale_features(X_train_s, X_val_s, X_test_s)
                        
                        # è¿è¡Œå®éªŒ
                        season_results = run_stratified_experiment(
                            X_train_s_scaled, X_val_s_scaled, X_test_s_scaled,
                            y_train_s, y_val_s, y_test_s, 
                            f"{season.title()}å­£èŠ‚"
                        )
                        
                        if season_results:
                            stratified_results[f'season_{season}'] = season_results
                    except Exception as e:
                        print(f"âŒ {season.title()}å­£èŠ‚å®éªŒå¤±è´¥: {e}")
                else:
                    print(f"âš ï¸  {season.title()}å­£èŠ‚æ•°æ®é‡ä¸è¶³ ({len(season_df)} æ¡)")
        
        # æŒ‰å¤©æ°”æ¡ä»¶åˆ†å±‚å®éªŒ
        print("\nğŸŒ¤ï¸ æŒ‰å¤©æ°”æ¡ä»¶åˆ†å±‚å®éªŒ")
        for weather_type in ['good_weather', 'bad_weather']:
            if weather_type in stratified_data:
                weather_df = stratified_data[weather_type]
                if len(weather_df) > 300:
                    try:
                        # ç‰¹å¾å·¥ç¨‹
                        weather_processed = preprocessor.prepare_features(
                            weather_df, use_lag_features=False, exclude_non_operating=False
                        )
                        
                        # åˆ†å‰²æ•°æ®
                        X_train_w, X_val_w, X_test_w, y_train_w, y_val_w, y_test_w = \
                            preprocessor.split_data_temporal(weather_processed)
                        
                        # æ ‡å‡†åŒ–
                        X_train_w_scaled, X_val_w_scaled, X_test_w_scaled = \
                            preprocessor.scale_features(X_train_w, X_val_w, X_test_w)
                        
                        # è¿è¡Œå®éªŒ
                        weather_name = "å¥½å¤©æ°”" if weather_type == "good_weather" else "æ¶åŠ£å¤©æ°”"
                        weather_results = run_stratified_experiment(
                            X_train_w_scaled, X_val_w_scaled, X_test_w_scaled,
                            y_train_w, y_val_w, y_test_w, 
                            weather_name
                        )
                        
                        if weather_results:
                            stratified_results[weather_type] = weather_results
                    except Exception as e:
                        print(f"âŒ {weather_name}å®éªŒå¤±è´¥: {e}")
                else:
                    print(f"âš ï¸  {weather_type}æ•°æ®é‡ä¸è¶³ ({len(weather_df)} æ¡)")
        
        # åˆ†å±‚ç»“æœæ€»ç»“
        if stratified_results:
            print("\nğŸ“Š åˆ†å±‚å»ºæ¨¡ç»“æœæ€»ç»“:")
            for stratum, result in stratified_results.items():
                print(f"  {stratum}: {result['best_model']} - RMSE={result['best_rmse']:.2f}, RÂ²={result['best_r2']:.3f}")
        else:
            print("\nâš ï¸  åˆ†å±‚å»ºæ¨¡å®éªŒæœªäº§ç”Ÿæœ‰æ•ˆç»“æœ")
    
    except Exception as e:
        print(f"âš ï¸  åˆ†å±‚å»ºæ¨¡å®éªŒå¤±è´¥: {e}")
        print("ç»§ç»­ä½¿ç”¨å…¨æ•°æ®é›†ç»“æœ...")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š å®éªŒæ€»ç»“")
    print("=" * 80)
    print("åŸºäºæ•°æ®æ´å¯Ÿçš„ç‰¹å¾å·¥ç¨‹æ”¹è¿›:")
    print("âœ… æ’é™¤éè¿è¥æ—¥æ•°æ® (295æ¡)")
    print("âœ… æ¸©åº¦åˆ†æ®µç‰¹å¾ (<0, 0-10, 10-20, 20-30, >30Â°C)")
    print("âœ… åŒå³°æ—¶é—´æ¨¡å¼è¯†åˆ« (8am, 6pmå³°å€¼)")
    print("âœ… æ¸©åº¦Ã—å­£èŠ‚äº¤äº’ç‰¹å¾")
    print("âœ… èˆ’é€‚åº¦æŒ‡æ•° (æ¸©åº¦+æ¹¿åº¦ç»„åˆ)")
    print("âœ… é™æ°´é˜ˆå€¼ç‰¹å¾ (æœ‰/æ— é™æ°´)")
    print("âœ… æç«¯å¤©æ°”æ¡ä»¶å¤„ç†")
    print("âœ… åˆ†å±‚å»ºæ¨¡ç­–ç•¥ (æŒ‰å­£èŠ‚/å¤©æ°”æ¡ä»¶)")
    
    # è¾“å‡ºæœ€ç»ˆæ€»ç»“
    print("\nğŸ“‹ é›†æˆå­¦ä¹ æ–¹æ³•æ€»ç»“:")
    print("- æµ‹è¯•äº†Baggingã€Boostingã€Votingã€Stackingå››ç±»é›†æˆæ–¹æ³•")
    print("- åŸºäºæ•°æ®åˆ†æä¼˜åŒ–äº†æ¨¡å‹å‚æ•°")
    print("- é‡ç‚¹å…³æ³¨æ¸©åº¦ã€å°æ—¶ã€éœ²ç‚¹æ¸©åº¦ä¸‰ä¸ªå¼ºç›¸å…³ç‰¹å¾")
    print("- å®æ–½äº†åˆ†å±‚å»ºæ¨¡ç­–ç•¥")
    print("- å¤„ç†äº†æç«¯å¤©æ°”æ¡ä»¶çš„ç‰¹æ®Šæƒ…å†µ")
    
    print("\nğŸ‰ å®éªŒä¸‰å®Œæˆï¼")
    print("=" * 80)

def run_optimal_experiment_main():
    """è¿è¡ŒåŸºäºRÂ²æœ€ä¼˜æ¨¡å‹çš„å®éªŒ"""
    print("ğŸ† å¯åŠ¨RÂ²æœ€ä¼˜æ¨¡å‹é›†æˆå­¦ä¹ å®éªŒ")
    print("="*60)
    
    # è¿è¡Œä¼˜åŒ–å®éªŒ
    optimal_experiment = OptimalEnsembleExperiment()
    results = optimal_experiment.run_optimal_experiment()
    
    if results:
        print(f"\nâœ… ä¼˜åŒ–å®éªŒå®Œæˆï¼æˆåŠŸæµ‹è¯•äº† {len(results)} ä¸ªä¼˜åŒ–æ¨¡å‹")
        print("ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° optimal_ensemble_results.csv å’Œ optimal_ensemble_summary.json")
    else:
        print("\nâŒ ä¼˜åŒ–å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œä»£ç ")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'optimal':
        # è¿è¡Œä¼˜åŒ–å®éªŒ
        run_optimal_experiment_main()
    else:
        # è¿è¡Œå¸¸è§„å®éªŒ
        main()
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ 'python experiment_3_ensemble_learning.py optimal' è¿è¡ŒRÂ²æœ€ä¼˜æ¨¡å‹å®éªŒ") 