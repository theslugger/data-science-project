#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæ•°æ®æ¢ç´¢åˆ†æ
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹

å…¨é¢çš„æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ–åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
import json
import os
from scipy import stats
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# ç§‘å­¦è®ºæ–‡çº§åˆ«çš„å›¾å½¢æ ·å¼é…ç½®
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({
    # å­—ä½“è®¾ç½® - ç§‘å­¦è®ºæ–‡æ ‡å‡†
    'font.family': 'serif',
    'font.serif': ['Times New Roman', 'DejaVu Serif', 'SimSun'],
    'font.sans-serif': ['Arial', 'Helvetica', 'SimHei', 'Microsoft YaHei'],
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.titlesize': 16,
    
    # å›¾å½¢æ ·å¼ - å­¦æœ¯æ ‡å‡†
    'figure.figsize': (12, 8),
    'figure.dpi': 100,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1,
    
    # è½´å’Œç½‘æ ¼æ ·å¼
    'axes.grid': True,
    'axes.grid.axis': 'both',
    'grid.alpha': 0.3,
    'grid.linewidth': 0.5,
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.linewidth': 0.8,
    'axes.edgecolor': 'black',
    
    # è‰²å½©å’Œçº¿æ¡
    'axes.prop_cycle': plt.cycler('color', [
        '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'
    ]),
    'lines.linewidth': 1.5,
    'lines.markersize': 6,
    'patch.linewidth': 0.5,
    'patch.facecolor': '#1f77b4',
    'patch.edgecolor': 'black',
    
    # ä¸­æ–‡æ”¯æŒ
    'axes.unicode_minus': False,
})

# å®šä¹‰ç§‘å­¦è®ºæ–‡é…è‰²æ–¹æ¡ˆ
SCIENTIFIC_COLORS = {
    'primary': '#2E86AB',      # ä¸»è‰²è°ƒï¼šç§‘å­¦è“
    'secondary': '#A23B72',    # æ¬¡è‰²è°ƒï¼šæ·±ç´«çº¢
    'accent': '#F18F01',       # å¼ºè°ƒè‰²ï¼šæš–æ©™
    'success': '#C73E1D',      # æˆåŠŸè‰²ï¼šæ·±çº¢
    'info': '#845EC2',         # ä¿¡æ¯è‰²ï¼šç´«è‰²
    'warning': '#FF8500',      # è­¦å‘Šè‰²ï¼šæ©™è‰²
    'light': '#F8F9FA',        # æµ…è‰²
    'dark': '#212529',         # æ·±è‰²
    'muted': '#6C757D',        # ä¸­æ€§è‰²
}

# ä¸“ä¸šè°ƒè‰²æ¿
SCIENTIFIC_PALETTES = {
    'categorical': ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#845EC2', '#FF8500'],
    'sequential_blue': ['#EDF4F8', '#D1E5F0', '#92C5DE', '#4393C3', '#2166AC', '#053061'],
    'sequential_red': ['#FFF5F0', '#FEE0D2', '#FCBBA1', '#FC9272', '#FB6A4A', '#DE2D26'],
    'diverging': ['#8E0152', '#C51B7D', '#DE77AE', '#F1B6DA', '#FDE0EF', '#E6F5D0', '#B8E186', '#7FBC41', '#4D9221', '#276419'],
    'temperature': ['#313695', '#4575B4', '#74ADD1', '#ABD9E9', '#E0F3F8', '#FEE090', '#FDAE61', '#F46D43', '#D73027', '#A50026']
}

class BikeDataExplorer:
    """é¦–å°”è‡ªè¡Œè½¦æ•°æ®æ¢ç´¢åˆ†æç±»"""
    
    def __init__(self, data_file='SeoulBikeData.csv'):
        self.data_file = data_file
        self.df = None
        self.insights = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = 'outputs'
        self.eda_dir = os.path.join(self.output_dir, 'eda')
        self.figures_dir = os.path.join(self.output_dir, 'figures')
        
        for directory in [self.output_dir, self.eda_dir, self.figures_dir]:
            os.makedirs(directory, exist_ok=True)
    
    def load_and_basic_info(self):
        """åŠ è½½æ•°æ®å¹¶è·å–åŸºæœ¬ä¿¡æ¯"""
        print("ğŸ” æ•°æ®åŠ è½½ä¸åŸºæœ¬ä¿¡æ¯åˆ†æ")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        try:
            self.df = pd.read_csv(self.data_file, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                self.df = pd.read_csv(self.data_file, encoding='latin-1')
            except UnicodeDecodeError:
                self.df = pd.read_csv(self.data_file, encoding='cp1252')
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {self.data_file}")
        print(f"ğŸ“Š æ•°æ®ç»´åº¦: {self.df.shape[0]} è¡Œ Ã— {self.df.shape[1]} åˆ—")
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“‹ æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ•°æ®é›†å¤§å°: {self.df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        print(f"  æ•°æ®ç±»å‹åˆ†å¸ƒ:")
        print(self.df.dtypes.value_counts().to_string())
        
        # åˆ—åä¿¡æ¯
        print(f"\nğŸ“ åˆ—ååˆ—è¡¨:")
        for i, col in enumerate(self.df.columns, 1):
            print(f"  {i:2d}. {col}")
        
        # å‰å‡ è¡Œæ•°æ®
        print(f"\nğŸ” å‰5è¡Œæ•°æ®:")
        print(self.df.head().to_string())
        
        self.insights.append(f"æ•°æ®é›†åŒ…å«{self.df.shape[0]}æ¡è®°å½•å’Œ{self.df.shape[1]}ä¸ªç‰¹å¾")
        
        return self.df.describe()
    
    def data_quality_check(self):
        """æ•°æ®è´¨é‡æ£€æŸ¥"""
        print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
        print("="*60)
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        missing_info = self.df.isnull().sum()
        missing_percent = (missing_info / len(self.df)) * 100
        
        print("ğŸ“Š ç¼ºå¤±å€¼ç»Ÿè®¡:")
        if missing_info.sum() == 0:
            print("  âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
            self.insights.append("æ•°æ®é›†å®Œæ•´ï¼Œæ— ç¼ºå¤±å€¼")
        else:
            missing_df = pd.DataFrame({
                'ç¼ºå¤±æ•°é‡': missing_info,
                'ç¼ºå¤±æ¯”ä¾‹(%)': missing_percent
            })
            missing_df = missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0].sort_values('ç¼ºå¤±æ•°é‡', ascending=False)
            print(missing_df.to_string())
            self.insights.append(f"å‘ç°{missing_df.shape[0]}ä¸ªå­—æ®µå­˜åœ¨ç¼ºå¤±å€¼")
        
        # é‡å¤å€¼æ£€æŸ¥
        duplicates = self.df.duplicated().sum()
        print(f"\nğŸ”„ é‡å¤å€¼ç»Ÿè®¡:")
        if duplicates == 0:
            print("  âœ… æ²¡æœ‰å‘ç°é‡å¤è¡Œ")
            self.insights.append("æ•°æ®é›†æ— é‡å¤è®°å½•")
        else:
            print(f"  âš ï¸  å‘ç° {duplicates} æ¡é‡å¤è®°å½• ({duplicates/len(self.df)*100:.2f}%)")
            self.insights.append(f"å‘ç°{duplicates}æ¡é‡å¤è®°å½•")
        
        # æ•°æ®ç±»å‹æ£€æŸ¥
        print(f"\nğŸ“Š æ•°æ®ç±»å‹è¯¦æƒ…:")
        print(self.df.dtypes.to_string())
        
        # å”¯ä¸€å€¼ç»Ÿè®¡
        print(f"\nğŸ”¢ å”¯ä¸€å€¼ç»Ÿè®¡:")
        for col in self.df.columns:
            unique_count = self.df[col].nunique()
            unique_ratio = unique_count / len(self.df)
            print(f"  {col}: {unique_count} ä¸ªå”¯ä¸€å€¼ ({unique_ratio:.3f})")
        
        return missing_info, duplicates
    
    def target_analysis(self):
        """ç›®æ ‡å˜é‡åˆ†æ"""
        print("\nğŸ¯ ç›®æ ‡å˜é‡åˆ†æ (ç§Ÿå€Ÿè‡ªè¡Œè½¦æ•°é‡)")
        print("="*60)
        
        target_col = 'Rented Bike Count'
        
        # åŸºç¡€ç»Ÿè®¡
        stats_info = self.df[target_col].describe()
        print("ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:")
        print(stats_info.to_string())
        
        # åˆ†å¸ƒç‰¹å¾
        skewness = self.df[target_col].skew()
        kurtosis = self.df[target_col].kurtosis()
        
        print(f"\nğŸ“ˆ åˆ†å¸ƒç‰¹å¾:")
        print(f"  ååº¦ (Skewness): {skewness:.4f}")
        print(f"  å³°åº¦ (Kurtosis): {kurtosis:.4f}")
        
        if skewness > 1:
            skew_desc = "ä¸¥é‡å³å"
        elif skewness > 0.5:
            skew_desc = "è½»å¾®å³å"
        elif skewness < -1:
            skew_desc = "ä¸¥é‡å·¦å"
        elif skewness < -0.5:
            skew_desc = "è½»å¾®å·¦å"
        else:
            skew_desc = "è¿‘ä¼¼æ­£æ€"
        
        print(f"  åˆ†å¸ƒç‰¹å¾: {skew_desc}")
        
        # åˆ›å»ºç›®æ ‡å˜é‡åˆ†å¸ƒå›¾ - ç§‘å­¦è®ºæ–‡æ ·å¼
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Rental Bike Count Distribution Analysis', fontsize=16, fontweight='bold', y=0.95)
        
        # ç›´æ–¹å›¾ - ä¼˜é›…çš„è“è‰²ç³»
        n, bins, patches = axes[0, 0].hist(self.df[target_col], bins=50, alpha=0.8, 
                                          color=SCIENTIFIC_COLORS['primary'], 
                                          edgecolor='white', linewidth=0.5)
        axes[0, 0].set_title('(a) Distribution Histogram', fontweight='bold', pad=15)
        axes[0, 0].set_xlabel('Rental Count', fontweight='bold')
        axes[0, 0].set_ylabel('Frequency', fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç®±çº¿å›¾ - ä¸“ä¸šé…è‰²
        bp = axes[0, 1].boxplot(self.df[target_col], patch_artist=True, 
                               boxprops=dict(facecolor=SCIENTIFIC_COLORS['secondary'], alpha=0.8),
                               medianprops=dict(color='white', linewidth=2),
                               whiskerprops=dict(color=SCIENTIFIC_COLORS['dark'], linewidth=1.5),
                               capprops=dict(color=SCIENTIFIC_COLORS['dark'], linewidth=1.5),
                               flierprops=dict(marker='o', markerfacecolor=SCIENTIFIC_COLORS['accent'], 
                                             markersize=4, alpha=0.6))
        axes[0, 1].set_title('(b) Box Plot', fontweight='bold', pad=15)
        axes[0, 1].set_ylabel('Rental Count', fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Q-Qå›¾ - ç»Ÿè®¡å­¦æ ‡å‡†
        stats.probplot(self.df[target_col], dist="norm", plot=axes[1, 0])
        axes[1, 0].get_lines()[0].set_markerfacecolor(SCIENTIFIC_COLORS['info'])
        axes[1, 0].get_lines()[0].set_markeredgecolor('white')
        axes[1, 0].get_lines()[0].set_markersize(4)
        axes[1, 0].get_lines()[1].set_color(SCIENTIFIC_COLORS['accent'])
        axes[1, 0].get_lines()[1].set_linewidth(2)
        axes[1, 0].set_title('(c) Q-Q Plot (Normality Test)', fontweight='bold', pad=15)
        axes[1, 0].grid(True, alpha=0.3)
        
        # ç´¯ç§¯åˆ†å¸ƒå›¾ - ç§‘å­¦é…è‰²
        sorted_data = np.sort(self.df[target_col])
        cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
        axes[1, 1].plot(sorted_data, cumulative, color=SCIENTIFIC_COLORS['success'], 
                       linewidth=2.5, alpha=0.9)
        axes[1, 1].fill_between(sorted_data, cumulative, alpha=0.2, 
                               color=SCIENTIFIC_COLORS['success'])
        axes[1, 1].set_title('(d) Cumulative Distribution Function', fontweight='bold', pad=15)
        axes[1, 1].set_xlabel('Rental Count', fontweight='bold')
        axes[1, 1].set_ylabel('Cumulative Probability', fontweight='bold')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, f'target_distribution_{self.timestamp}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        Q1 = self.df[target_col].quantile(0.25)
        Q3 = self.df[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = self.df[(self.df[target_col] < lower_bound) | (self.df[target_col] > upper_bound)]
        outlier_count = len(outliers)
        outlier_percent = outlier_count / len(self.df) * 100
        
        print(f"\nğŸš¨ å¼‚å¸¸å€¼æ£€æµ‹ (IQRæ–¹æ³•):")
        print(f"  ä¸‹è¾¹ç•Œ: {lower_bound:.2f}")
        print(f"  ä¸Šè¾¹ç•Œ: {upper_bound:.2f}")
        print(f"  å¼‚å¸¸å€¼æ•°é‡: {outlier_count} ({outlier_percent:.2f}%)")
        
        self.insights.append(f"ç›®æ ‡å˜é‡å‘ˆ{skew_desc}åˆ†å¸ƒï¼Œå¼‚å¸¸å€¼å æ¯”{outlier_percent:.1f}%")
        
        return stats_info, outlier_count
    
    def temporal_analysis(self):
        """æ—¶é—´åºåˆ—åˆ†æ"""
        print("\nâ° æ—¶é—´åºåˆ—åˆ†æ")
        print("="*60)
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        self.df['Date'] = pd.to_datetime(self.df['Date'], format='%d/%m/%Y')
        self.df['DateTime'] = pd.to_datetime(
            self.df['Date'].dt.strftime('%Y-%m-%d') + ' ' + 
            self.df['Hour'].astype(str) + ':00:00'
        )
        
        # æ—¶é—´èŒƒå›´
        date_range = f"{self.df['Date'].min().strftime('%Y-%m-%d')} åˆ° {self.df['Date'].max().strftime('%Y-%m-%d')}"
        total_days = (self.df['Date'].max() - self.df['Date'].min()).days + 1
        
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {date_range} ({total_days} å¤©)")
        
        # æŒ‰æ—¶é—´ç»´åº¦èšåˆåˆ†æ
        hourly_avg = self.df.groupby('Hour')['Rented Bike Count'].agg(['mean', 'std']).round(2)
        daily_avg = self.df.groupby('Date')['Rented Bike Count'].agg(['mean', 'sum']).round(2)
        monthly_avg = self.df.groupby(self.df['Date'].dt.month)['Rented Bike Count'].mean().round(2)
        seasonal_avg = self.df.groupby('Seasons')['Rented Bike Count'].mean().round(2)
        
        print(f"\nğŸ“Š æ—¶é—´æ¨¡å¼åˆ†æ:")
        print(f"  é«˜å³°å°æ—¶: {hourly_avg['mean'].idxmax()}æ—¶ (å¹³å‡{hourly_avg['mean'].max():.0f}è¾†)")
        print(f"  ä½è°·å°æ—¶: {hourly_avg['mean'].idxmin()}æ—¶ (å¹³å‡{hourly_avg['mean'].min():.0f}è¾†)")
        print(f"  æœ€é«˜å•æ—¥æ€»é‡: {daily_avg['sum'].max():.0f}è¾†")
        print(f"  æœ€ä½å•æ—¥æ€»é‡: {daily_avg['sum'].min():.0f}è¾†")
        
        # åˆ›å»ºæ—¶é—´åºåˆ—å¯è§†åŒ– - ç§‘å­¦è®ºæ–‡æ ·å¼
        fig, axes = plt.subplots(2, 2, figsize=(18, 12))
        fig.suptitle('Temporal Patterns in Bike Rental Demand', fontsize=16, fontweight='bold', y=0.95)
        
        # å°æ—¶æ¨¡å¼ - åŒå³°æ›²çº¿
        line = axes[0, 0].plot(hourly_avg.index, hourly_avg['mean'], 
                              marker='o', linewidth=2.5, markersize=6,
                              color=SCIENTIFIC_COLORS['primary'], markerfacecolor='white',
                              markeredgecolor=SCIENTIFIC_COLORS['primary'], markeredgewidth=2)
        axes[0, 0].fill_between(hourly_avg.index, 
                                hourly_avg['mean'] - hourly_avg['std'],
                                hourly_avg['mean'] + hourly_avg['std'], 
                                alpha=0.25, color=SCIENTIFIC_COLORS['primary'])
        axes[0, 0].set_title('(a) Hourly Rental Pattern', fontweight='bold', pad=15)
        axes[0, 0].set_xlabel('Hour of Day', fontweight='bold')
        axes[0, 0].set_ylabel('Average Rental Count', fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_xticks(range(0, 24, 3))
        
        # æ—¥è¶‹åŠ¿ - æ—¶é—´åºåˆ—
        axes[0, 1].plot(daily_avg.index, daily_avg['mean'], 
                       alpha=0.8, linewidth=1.5, color=SCIENTIFIC_COLORS['secondary'])
        axes[0, 1].set_title('(b) Daily Average Trend', fontweight='bold', pad=15)
        axes[0, 1].set_xlabel('Date', fontweight='bold')
        axes[0, 1].set_ylabel('Daily Average Count', fontweight='bold')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # æœˆä»½æ¨¡å¼ - æ¸å˜æŸ±çŠ¶å›¾
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        colors = plt.cm.viridis(np.linspace(0.2, 0.8, 12))
        bars = axes[1, 0].bar(range(1, 13), monthly_avg.values, 
                             color=colors, alpha=0.8, edgecolor='white', linewidth=0.5)
        axes[1, 0].set_title('(c) Monthly Rental Pattern', fontweight='bold', pad=15)
        axes[1, 0].set_xlabel('Month', fontweight='bold')
        axes[1, 0].set_ylabel('Average Rental Count', fontweight='bold')
        axes[1, 0].set_xticks(range(1, 13))
        axes[1, 0].set_xticklabels(month_names)
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for i, bar in enumerate(bars):
            height = bar.get_height()
            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 10,
                           f'{height:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
        
        # å­£èŠ‚æ¨¡å¼ - ä¸“ä¸šé…è‰²
        season_colors = {
            'Spring': SCIENTIFIC_COLORS['info'],     # ç´«è‰²
            'Summer': SCIENTIFIC_COLORS['accent'],   # æ©™è‰²
            'Autumn': SCIENTIFIC_COLORS['warning'],  # æ·±æ©™
            'Winter': SCIENTIFIC_COLORS['primary']   # è“è‰²
        }
        bars = axes[1, 1].bar(seasonal_avg.index, seasonal_avg.values,
                             color=[season_colors.get(season, SCIENTIFIC_COLORS['muted']) 
                                   for season in seasonal_avg.index],
                             alpha=0.85, edgecolor='white', linewidth=1)
        axes[1, 1].set_title('(d) Seasonal Rental Pattern', fontweight='bold', pad=15)
        axes[1, 1].set_xlabel('Season', fontweight='bold')
        axes[1, 1].set_ylabel('Average Rental Count', fontweight='bold')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for i, bar in enumerate(bars):
            height = bar.get_height()
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 15,
                           f'{height:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, f'time_series_{self.timestamp}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # å‘¨å†…æ¨¡å¼
        self.df['Weekday'] = self.df['Date'].dt.dayofweek
        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        weekday_avg = self.df.groupby('Weekday')['Rented Bike Count'].mean()
        
        print(f"\nğŸ“… å‘¨å†…æ¨¡å¼:")
        for i, avg in weekday_avg.items():
            print(f"  {weekday_names[i]}: {avg:.0f}è¾†")
        
        peak_hour = hourly_avg['mean'].idxmax()
        peak_season = seasonal_avg.idxmax()
        
        self.insights.append(f"ä¸€æ—¥åŒå³°æ¨¡å¼ï¼šé«˜å³°åœ¨{peak_hour}æ—¶")
        self.insights.append(f"å­£èŠ‚æ€§å¼ºï¼š{peak_season}å­£éœ€æ±‚æœ€é«˜")
        
        return hourly_avg, seasonal_avg
    
    def weather_analysis(self):
        """å¤©æ°”å› ç´ åˆ†æ"""
        print("\nğŸŒ¤ï¸ å¤©æ°”å› ç´ åˆ†æ")
        print("="*60)
        
        weather_cols = ['Temperature(Â°C)', 'Humidity(%)', 'Wind speed (m/s)', 
                       'Visibility (10m)', 'Rainfall(mm)', 'Snowfall (cm)']
        
        # å¤©æ°”ç»Ÿè®¡
        weather_stats = self.df[weather_cols].describe().round(2)
        print("ğŸ“Š å¤©æ°”å˜é‡ç»Ÿè®¡:")
        print(weather_stats.to_string())
        
        # åˆ›å»ºå¤©æ°”ç›¸å…³æ€§åˆ†æå›¾ - ç§‘å­¦è®ºæ–‡æ ·å¼
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Weather Factors Impact on Bike Rental Demand', fontsize=16, fontweight='bold', y=0.95)
        axes = axes.flatten()
        
        # ç§‘å­¦é…è‰²æ–¹æ¡ˆ - æ¯ä¸ªå¤©æ°”å› å­ä½¿ç”¨ä¸åŒé¢œè‰²
        weather_colors = SCIENTIFIC_PALETTES['categorical']
        
        for i, col in enumerate(weather_cols):
            # æ•£ç‚¹å›¾æ˜¾ç¤ºä¸ç§Ÿå€Ÿé‡çš„å…³ç³» - ä½¿ç”¨å¯†åº¦ç€è‰²
            scatter = axes[i].scatter(self.df[col], self.df['Rented Bike Count'], 
                                    alpha=0.6, s=25, c=weather_colors[i % len(weather_colors)],
                                    edgecolors='white', linewidth=0.2)
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            correlation = self.df[col].corr(self.df['Rented Bike Count'])
            
            # æ·»åŠ è¶‹åŠ¿çº¿ - ä½¿ç”¨å¯¹æ¯”è‰²
            z = np.polyfit(self.df[col], self.df['Rented Bike Count'], 1)
            p = np.poly1d(z)
            axes[i].plot(self.df[col], p(self.df[col]), color=SCIENTIFIC_COLORS['dark'], 
                        linestyle='--', alpha=0.9, linewidth=2.5)
            
            # æ ‡é¢˜åŒ…å«å­å›¾æ ‡å·
            subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)']
            col_short = col.replace('(Â°C)', '').replace('(%)', '').replace(' (m/s)', '').replace(' (10m)', '').replace('(mm)', '').replace(' (cm)', '')
            axes[i].set_title(f'{subplot_labels[i]} {col_short}\n(r = {correlation:.3f})', 
                            fontweight='bold', pad=15)
            axes[i].set_xlabel(col, fontweight='bold')
            axes[i].set_ylabel('Rental Count', fontweight='bold')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, f'weather_correlation_{self.timestamp}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # å¤©æ°”æ¡ä»¶åˆ†ç±»åˆ†æ
        print(f"\nğŸŒ¡ï¸ æ¸©åº¦å½±å“åˆ†æ:")
        temp_bins = [-20, 0, 10, 20, 30, 40]
        temp_labels = ['ä¸¥å¯’(<0Â°C)', 'å¯’å†·(0-10Â°C)', 'å‡‰çˆ½(10-20Â°C)', 'æ¸©æš–(20-30Â°C)', 'ç‚çƒ­(>30Â°C)']
        self.df['Temp_Category'] = pd.cut(self.df['Temperature(Â°C)'], bins=temp_bins, labels=temp_labels)
        temp_analysis = self.df.groupby('Temp_Category')['Rented Bike Count'].agg(['count', 'mean', 'std']).round(2)
        print(temp_analysis.to_string())
        
        print(f"\nğŸ’§ é™æ°´å½±å“åˆ†æ:")
        rain_analysis = self.df.groupby(self.df['Rainfall(mm)'] > 0)['Rented Bike Count'].agg(['count', 'mean']).round(2)
        rain_analysis.index = ['æ— é™é›¨', 'æœ‰é™é›¨']
        print(rain_analysis.to_string())
        
        snow_analysis = self.df.groupby(self.df['Snowfall (cm)'] > 0)['Rented Bike Count'].agg(['count', 'mean']).round(2)
        snow_analysis.index = ['æ— é™é›ª', 'æœ‰é™é›ª']
        print(snow_analysis.to_string())
        
        # æ‰¾å‡ºå¼ºç›¸å…³çš„å¤©æ°”å› å­
        correlations = {}
        for col in weather_cols:
            correlations[col] = abs(self.df[col].corr(self.df['Rented Bike Count']))
        
        strongest_factor = max(correlations, key=correlations.get)
        self.insights.append(f"æœ€å¼ºå¤©æ°”ç›¸å…³å› å­ï¼š{strongest_factor} (r={correlations[strongest_factor]:.3f})")
        
        return weather_stats, correlations
    
    def categorical_analysis(self):
        """åˆ†ç±»å˜é‡åˆ†æ"""
        print("\nğŸ·ï¸ åˆ†ç±»å˜é‡åˆ†æ")
        print("="*60)
        
        categorical_cols = ['Seasons', 'Holiday', 'Functioning Day']
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 8))
        fig.suptitle('Categorical Variables Impact on Bike Rental', fontsize=16, fontweight='bold', y=0.95)
        
        subplot_labels = ['(a)', '(b)', '(c)']
        colors = [SCIENTIFIC_COLORS['primary'], SCIENTIFIC_COLORS['secondary'], SCIENTIFIC_COLORS['accent']]
        
        for i, col in enumerate(categorical_cols):
            category_stats = self.df.groupby(col)['Rented Bike Count'].agg(['count', 'mean', 'std']).round(2)
            print(f"\nğŸ“Š {col} åˆ†æ:")
            print(category_stats.to_string())
            
            # å‡†å¤‡æ•°æ®ç”¨äºç®±çº¿å›¾
            groups = [self.df[self.df[col] == category]['Rented Bike Count'].values 
                     for category in self.df[col].unique()]
            labels = self.df[col].unique()
            
            # åˆ›å»ºä¸“ä¸šç®±çº¿å›¾ - ä½¿ç”¨matplotlibè€Œä¸æ˜¯pandas
            bp = axes[i].boxplot(groups, labels=labels, patch_artist=True)
            
            # è‡ªå®šä¹‰ç®±çº¿å›¾æ ·å¼
            for patch in bp['boxes']:
                patch.set_facecolor(colors[i])
                patch.set_alpha(0.7)
                patch.set_edgecolor(SCIENTIFIC_COLORS['dark'])
                patch.set_linewidth(1.2)
            
            for median in bp['medians']:
                median.set_color('white')
                median.set_linewidth(2)
            
            for whisker in bp['whiskers']:
                whisker.set_color(SCIENTIFIC_COLORS['dark'])
                whisker.set_linewidth(1.2)
            
            for cap in bp['caps']:
                cap.set_color(SCIENTIFIC_COLORS['dark'])
                cap.set_linewidth(1.2)
            
            for flier in bp['fliers']:
                flier.set_markerfacecolor(colors[i])
                flier.set_markeredgecolor('white')
                flier.set_markersize(4)
                flier.set_alpha(0.6)
            
            axes[i].set_title(f'{subplot_labels[i]} {col}', fontweight='bold', pad=15)
            axes[i].set_xlabel(col, fontweight='bold')
            axes[i].set_ylabel('Rental Count', fontweight='bold')
            axes[i].grid(True, alpha=0.3)
            
            # æ—‹è½¬xè½´æ ‡ç­¾ä»¥é¿å…é‡å 
            axes[i].tick_params(axis='x', rotation=45)
            
        plt.suptitle('Categorical Variables Impact on Bike Rental', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, f'categorical_analysis_{self.timestamp}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # éè¿è¥æ—¥å½±å“
        functioning_impact = self.df.groupby('Functioning Day')['Rented Bike Count'].mean()
        non_functioning_days = (self.df['Functioning Day'] == 'No').sum()
        non_functioning_ratio = non_functioning_days / len(self.df) * 100
        
        print(f"\nğŸš« éè¿è¥æ—¥å½±å“:")
        print(f"  éè¿è¥æ—¥æ•°é‡: {non_functioning_days} ({non_functioning_ratio:.2f}%)")
        
        self.insights.append(f"éè¿è¥æ—¥å æ¯”{non_functioning_ratio:.1f}%")
        
        return category_stats
    
    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        print("\nğŸ”— ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
        print("="*60)
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = self.df[numeric_cols].corr()
        
        # ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
        target_corr = correlation_matrix['Rented Bike Count'].abs().sort_values(ascending=False)
        
        print("ğŸ“Š ä¸ç§Ÿå€Ÿé‡ç›¸å…³æ€§æœ€å¼ºçš„å‰10ä¸ªç‰¹å¾:")
        for i, (feature, corr) in enumerate(target_corr.head(11).items(), 1):  # 11ä¸ªå› ä¸ºåŒ…å«ç›®æ ‡å˜é‡è‡ªå·±
            if feature != 'Rented Bike Count':
                print(f"  {i-1:2d}. {feature}: {corr:.4f}")
        
        # åˆ›å»ºç›¸å…³æ€§çƒ­å›¾ - ç§‘å­¦è®ºæ–‡æ ·å¼
        plt.figure(figsize=(16, 12))
        
        # åˆ›å»ºä¸Šä¸‰è§’é®ç½©
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        
        # ä½¿ç”¨ç§‘å­¦é…è‰²æ–¹æ¡ˆ
        sns.heatmap(correlation_matrix, 
                   mask=mask,
                   annot=True, 
                   cmap='RdBu_r',  # ç»å…¸çš„çº¢è“å‘æ•£é…è‰²
                   center=0,
                   square=True, 
                   linewidths=0.5,
                   linecolor='white',
                   cbar_kws={
                       "shrink": .8, 
                       "label": "Correlation Coefficient",
                       "orientation": "vertical"
                   },
                   fmt='.2f',
                   annot_kws={
                       'size': 8, 
                       'fontweight': 'bold',
                       'color': 'black'
                   })
        
        plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Features', fontsize=12, fontweight='bold')
        plt.ylabel('Features', fontsize=12, fontweight='bold')
        
        # è®¾ç½®åæ ‡è½´æ ‡ç­¾æ ·å¼
        plt.xticks(rotation=45, ha='right', fontsize=10)
        plt.yticks(rotation=0, fontsize=10)
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, f'correlation_matrix_{self.timestamp}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # å¤šé‡å…±çº¿æ€§æ£€æŸ¥
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = abs(correlation_matrix.iloc[i, j])
                if corr_val > 0.8:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                    high_corr_pairs.append((
                        correlation_matrix.columns[i], 
                        correlation_matrix.columns[j], 
                        corr_val
                    ))
        
        print(f"\nâš ï¸ é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r|>0.8):")
        if high_corr_pairs:
            for feat1, feat2, corr in high_corr_pairs:
                print(f"  {feat1} - {feat2}: {corr:.4f}")
        else:
            print("  æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")
        
        strongest_predictor = target_corr.drop('Rented Bike Count').index[0]
        self.insights.append(f"æœ€å¼ºé¢„æµ‹å› å­ï¼š{strongest_predictor} (r={target_corr[strongest_predictor]:.3f})")
        
        return correlation_matrix, target_corr
    
    def advanced_analysis(self):
        """é«˜çº§åˆ†æ"""
        print("\nğŸ§® é«˜çº§ç»Ÿè®¡åˆ†æ")
        print("="*60)
        
        # åŒå³°æ£€æµ‹
        hourly_avg = self.df.groupby('Hour')['Rented Bike Count'].mean()
        
        # æ‰¾åˆ°å³°å€¼
        peaks = []
        for i in range(1, len(hourly_avg)-1):
            if hourly_avg.iloc[i] > hourly_avg.iloc[i-1] and hourly_avg.iloc[i] > hourly_avg.iloc[i+1]:
                if hourly_avg.iloc[i] > hourly_avg.mean():  # åªè€ƒè™‘é«˜äºå¹³å‡å€¼çš„å³°
                    peaks.append((hourly_avg.index[i], hourly_avg.iloc[i]))
        
        print(f"ğŸ”ï¸ éœ€æ±‚å³°å€¼æ£€æµ‹:")
        for hour, demand in peaks:
            print(f"  {hour}æ—¶: {demand:.0f}è¾†")
        
        # å¤©æ°”é˜ˆå€¼åˆ†æ
        print(f"\nğŸŒ¡ï¸ æœ€ä¼˜å¤©æ°”æ¡ä»¶åˆ†æ:")
        optimal_temp = self.df.loc[self.df['Rented Bike Count'].idxmax(), 'Temperature(Â°C)']
        temp_range = self.df[(self.df['Temperature(Â°C)'] >= optimal_temp-5) & 
                           (self.df['Temperature(Â°C)'] <= optimal_temp+5)]
        optimal_temp_demand = temp_range['Rented Bike Count'].mean()
        
        print(f"  æœ€ä¼˜æ¸©åº¦èŒƒå›´: {optimal_temp-5:.1f}Â°C - {optimal_temp+5:.1f}Â°C")
        print(f"  è¯¥æ¸©åº¦èŒƒå›´å¹³å‡éœ€æ±‚: {optimal_temp_demand:.0f}è¾†")
        
        # éœ€æ±‚åˆ†çº§
        percentiles = [25, 50, 75, 90, 95]
        demand_thresholds = self.df['Rented Bike Count'].quantile([p/100 for p in percentiles])
        
        print(f"\nğŸ“Š éœ€æ±‚åˆ†çº§é˜ˆå€¼:")
        demand_levels = ['ä½éœ€æ±‚', 'ä¸­éœ€æ±‚', 'é«˜éœ€æ±‚', 'æé«˜éœ€æ±‚', 'å³°å€¼éœ€æ±‚']
        for i, (p, threshold) in enumerate(zip(percentiles, demand_thresholds)):
            if i < len(demand_levels):
                print(f"  {demand_levels[i]} (>{p}åˆ†ä½): >{threshold:.0f}è¾†")
        
        self.insights.append(f"è¯†åˆ«å‡º{len(peaks)}ä¸ªéœ€æ±‚å³°å€¼")
        
        return peaks, demand_thresholds
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆç»¼åˆæ•°æ®æ¢ç´¢æŠ¥å‘Š")
        print("="*60)
        
        # æ±‡æ€»æ‰€æœ‰æ´å¯Ÿ - ç¡®ä¿æ‰€æœ‰æ•°æ®ç±»å‹å¯JSONåºåˆ—åŒ–
        report = {
            'dataset_info': {
                'file_name': self.data_file,
                'shape': [int(x) for x in self.df.shape],  # è½¬æ¢ä¸ºintåˆ—è¡¨
                'memory_usage_mb': float(self.df.memory_usage(deep=True).sum() / 1024 / 1024),
                'analysis_timestamp': self.timestamp
            },
            'data_quality': {
                'missing_values': int(self.df.isnull().sum().sum()),
                'duplicate_rows': int(self.df.duplicated().sum()),
                'data_types': {str(k): int(v) for k, v in self.df.dtypes.value_counts().items()}  # è½¬æ¢é”®å’Œå€¼
            },
            'target_variable_stats': {
                'mean': float(self.df['Rented Bike Count'].mean()),
                'std': float(self.df['Rented Bike Count'].std()),
                'min': float(self.df['Rented Bike Count'].min()),
                'max': float(self.df['Rented Bike Count'].max()),
                'skewness': float(self.df['Rented Bike Count'].skew()),
                'kurtosis': float(self.df['Rented Bike Count'].kurtosis())
            },
            'temporal_patterns': {
                'date_range': {
                    'start': self.df['Date'].min().strftime('%Y-%m-%d'),
                    'end': self.df['Date'].max().strftime('%Y-%m-%d'),
                    'total_days': int((self.df['Date'].max() - self.df['Date'].min()).days + 1)
                },
                'peak_hour': int(self.df.groupby('Hour')['Rented Bike Count'].mean().idxmax()),
                'peak_season': str(self.df.groupby('Seasons')['Rented Bike Count'].mean().idxmax())
            },
            'key_insights': self.insights,
            'recommendations': [
                "è€ƒè™‘æ—¶é—´ç‰¹å¾å·¥ç¨‹ï¼ˆå°æ—¶ã€å­£èŠ‚ã€å·¥ä½œæ—¥ï¼‰",
                "æ¸©åº¦æ˜¯æœ€é‡è¦çš„é¢„æµ‹å› å­ï¼Œéœ€è¦è¯¦ç»†å»ºæ¨¡",
                "å¤„ç†éè¿è¥æ—¥æ•°æ®ï¼ˆæ’é™¤æˆ–ç‰¹æ®Šå¤„ç†ï¼‰",
                "è€ƒè™‘åŒå³°æ¨¡å¼çš„ç‰¹æ®Šå»ºæ¨¡æ–¹æ³•",
                "å¼‚å¸¸å€¼éœ€è¦é€‚å½“å¤„ç†ï¼ˆwinsorizationï¼‰"
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.eda_dir, f'comprehensive_eda_results_{self.timestamp}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.df.to_csv(os.path.join(self.eda_dir, f'explored_data_{self.timestamp}.csv'), 
                      index=False, encoding='utf-8')
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  ğŸ“Š åˆ†æç»“æœ: {report_file}")
        print(f"  ğŸ’¾ æ•°æ®æ–‡ä»¶: {os.path.join(self.eda_dir, f'explored_data_{self.timestamp}.csv')}")
        
        # æ‰“å°æ ¸å¿ƒæ´å¯Ÿæ‘˜è¦
        print(f"\nğŸ’¡ æ ¸å¿ƒæ´å¯Ÿæ‘˜è¦:")
        for i, insight in enumerate(self.insights, 1):
            print(f"  {i}. {insight}")
        
        return report
    
    def run_complete_exploration(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ¢ç´¢åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®æ¢ç´¢åˆ†æ")
        print("="*80)
        
        try:
            # 1. åŸºæœ¬ä¿¡æ¯
            basic_stats = self.load_and_basic_info()
            
            # 2. æ•°æ®è´¨é‡æ£€æŸ¥
            missing_info, duplicates = self.data_quality_check()
            
            # 3. ç›®æ ‡å˜é‡åˆ†æ
            target_stats, outliers = self.target_analysis()
            
            # 4. æ—¶é—´åºåˆ—åˆ†æ
            temporal_patterns = self.temporal_analysis()
            
            # 5. å¤©æ°”åˆ†æ
            weather_analysis = self.weather_analysis()
            
            # 6. åˆ†ç±»å˜é‡åˆ†æ
            categorical_analysis = self.categorical_analysis()
            
            # 7. ç›¸å…³æ€§åˆ†æ
            correlation_results = self.correlation_analysis()
            
            # 8. é«˜çº§åˆ†æ
            advanced_results = self.advanced_analysis()
            
            # 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            final_report = self.generate_comprehensive_report()
            
            print("\nğŸ‰ æ•°æ®æ¢ç´¢åˆ†æå®Œæˆï¼")
            print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° '{self.output_dir}' ç›®å½•")
            
            return final_report
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš´â€â™‚ï¸ é¦–å°”è‡ªè¡Œè½¦æ•°æ®æ¢ç´¢åˆ†æ")
    print("="*80)
    
    # åˆ›å»ºæ¢ç´¢å™¨
    explorer = BikeDataExplorer('SeoulBikeData.csv')
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = explorer.run_complete_exploration()
    
    if results:
        print("\nâœ¨ åˆ†ææ€»ç»“:")
        print("  - æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
        print("  - æ—¶é—´æ¨¡å¼è¯†åˆ«å®Œæˆ")
        print("  - å¤©æ°”å½±å“åˆ†æå®Œæˆ")
        print("  - ç‰¹å¾ç›¸å…³æ€§åˆ†æå®Œæˆ")
        print("  - é«˜çº§ç»Ÿè®¡åˆ†æå®Œæˆ")
        print("  - å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
        print("  - ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜")
        
        print(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
        print(f"  ğŸ“Š EDAç»“æœ: outputs/eda/")
        print(f"  ğŸ“ˆ å›¾è¡¨: outputs/figures/")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œä»£ç ")

if __name__ == "__main__":
    main() 