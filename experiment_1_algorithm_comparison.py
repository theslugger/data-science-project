#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒ1ï¼šæœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒ (æˆå‘˜A)
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹

ç›®æ ‡ï¼šæ¯”è¾ƒä¸åŒæœºå™¨å­¦ä¹ ç®—æ³•åœ¨è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½
ç®—æ³•ï¼šçº¿æ€§å›å½’ã€éšæœºæ£®æ—ã€XGBoostã€æ”¯æŒå‘é‡å›å½’ã€ç¥ç»ç½‘ç»œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
import xgboost as xgb
import time
import warnings
warnings.filterwarnings('ignore')

from data_preprocessing import BikeDataPreprocessor

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AlgorithmComparison:
    """ç®—æ³•æ¯”è¾ƒå®éªŒç±»"""
    
    def __init__(self):
        self.results = {}
        self.models = {}
        self.preprocessor = BikeDataPreprocessor()
        
    def calculate_metrics(self, y_true, y_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        return {
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'MAPE': mape
        }
    
    def define_algorithms(self):
        """å®šä¹‰è¦æ¯”è¾ƒçš„ç®—æ³•"""
        algorithms = {
            'Linear Regression': {
                'model': LinearRegression(),
                'params': {}
            },
            'Ridge Regression': {
                'model': Ridge(),
                'params': {
                    'alpha': [0.1, 1, 10, 100]
                }
            },
            'Lasso Regression': {
                'model': Lasso(),
                'params': {
                    'alpha': [0.1, 1, 10, 100]
                }
            },
            'Random Forest': {
                'model': RandomForestRegressor(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [10, 20, None],
                    'min_samples_split': [2, 5, 10]
                }
            },
            'XGBoost': {
                'model': xgb.XGBRegressor(random_state=42),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [3, 6, 9],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'subsample': [0.8, 1.0]
                }
            },
            'SVR': {
                'model': SVR(),
                'params': {
                    'C': [0.1, 1, 10],
                    'gamma': ['scale', 'auto'],
                    'kernel': ['rbf', 'linear']
                }
            },
            'Neural Network': {
                'model': MLPRegressor(random_state=42, max_iter=1000),
                'params': {
                    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
                    'alpha': [0.0001, 0.001, 0.01],
                    'learning_rate': ['constant', 'adaptive']
                }
            }
        }
        
        return algorithms
    
    def train_and_evaluate_model(self, name, model_config, X_train, X_val, X_test, 
                                y_train, y_val, y_test, use_cv=True):
        """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\n=== è®­ç»ƒ {name} ===")
        
        start_time = time.time()
        
        if use_cv and model_config['params']:
            # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
            tscv = TimeSeriesSplit(n_splits=3)
            
            # ç®€åŒ–å‚æ•°ç½‘æ ¼ä»¥åŠ å¿«é€Ÿåº¦
            if name in ['XGBoost', 'Random Forest', 'Neural Network']:
                # å¯¹äºè®¡ç®—å¯†é›†çš„æ¨¡å‹ï¼Œå‡å°‘å‚æ•°ç»„åˆ
                simplified_params = {}
                for key, values in model_config['params'].items():
                    simplified_params[key] = values[:2] if len(values) > 2 else values
                param_grid = simplified_params
            else:
                param_grid = model_config['params']
            
            print(f"å‚æ•°ç½‘æ ¼æœç´¢: {param_grid}")
            
            grid_search = GridSearchCV(
                model_config['model'],
                param_grid,
                cv=tscv,
                scoring='neg_mean_squared_error',
                n_jobs=-1,
                verbose=0
            )
            
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            
            print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
            print(f"æœ€ä½³CVåˆ†æ•°: {-grid_search.best_score_:.2f}")
            
        else:
            # ç›´æ¥ä½¿ç”¨é»˜è®¤å‚æ•°
            best_model = model_config['model']
            best_model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_train_pred = best_model.predict(X_train)
        y_val_pred = best_model.predict(X_val)
        y_test_pred = best_model.predict(X_test)
        
        training_time = time.time() - start_time
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        train_metrics = self.calculate_metrics(y_train, y_train_pred)
        val_metrics = self.calculate_metrics(y_val, y_val_pred)
        test_metrics = self.calculate_metrics(y_test, y_test_pred)
        
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"éªŒè¯é›† RMSE: {val_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RMSE: {test_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_metrics['RÂ²']:.4f}")
        
        # å­˜å‚¨ç»“æœ
        self.results[name] = {
            'model': best_model,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'training_time': training_time,
            'predictions': {
                'y_train_pred': y_train_pred,
                'y_val_pred': y_val_pred,
                'y_test_pred': y_test_pred
            }
        }
        
        return best_model
    
    def run_experiment(self, use_lag_features=False):
        """è¿è¡Œç®—æ³•æ¯”è¾ƒå®éªŒ"""
        print("ğŸš€ å¼€å§‹å®éªŒ1ï¼šæœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒ")
        print("="*50)
        
        # æ•°æ®é¢„å¤„ç†
        print("1. æ•°æ®é¢„å¤„ç†...")
        df = self.preprocessor.load_data()
        df_processed = self.preprocessor.prepare_features(df, use_lag_features=use_lag_features)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_val, X_test, y_train, y_val, y_test = \
            self.preprocessor.split_data_temporal(df_processed)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled = \
            self.preprocessor.scale_features(X_train, X_val, X_test)
        
        print(f"\næ•°æ®æ¦‚è§ˆ:")
        print(f"  ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]}")
        print(f"  è®­ç»ƒæ ·æœ¬: {X_train_scaled.shape[0]}")
        print(f"  éªŒè¯æ ·æœ¬: {X_val_scaled.shape[0]}")
        print(f"  æµ‹è¯•æ ·æœ¬: {X_test_scaled.shape[0]}")
        
        # å®šä¹‰ç®—æ³•
        algorithms = self.define_algorithms()
        
        print(f"\n2. å¼€å§‹è®­ç»ƒ {len(algorithms)} ä¸ªç®—æ³•...")
        
        # è®­ç»ƒæ‰€æœ‰ç®—æ³•
        for i, (name, config) in enumerate(algorithms.items(), 1):
            print(f"\nè¿›åº¦: {i}/{len(algorithms)}")
            try:
                self.train_and_evaluate_model(
                    name, config,
                    X_train_scaled, X_val_scaled, X_test_scaled,
                    y_train, y_val, y_test,
                    use_cv=True
                )
            except Exception as e:
                print(f"âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
                continue
        
        print("\nâœ… æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆ!")
        
        # ç”Ÿæˆç»“æœæŠ¥å‘Š
        self.generate_comparison_report(y_test)
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        return self.results
    
    def generate_comparison_report(self, y_test):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆç®—æ³•æ¯”è¾ƒæŠ¥å‘Š...")
        
        # åˆ›å»ºç»“æœDataFrame
        comparison_data = []
        for name, result in self.results.items():
            row = {
                'Algorithm': name,
                'Train_RMSE': result['train_metrics']['RMSE'],
                'Val_RMSE': result['val_metrics']['RMSE'],
                'Test_RMSE': result['test_metrics']['RMSE'],
                'Test_MAE': result['test_metrics']['MAE'],
                'Test_RÂ²': result['test_metrics']['RÂ²'],
                'Test_MAPE': result['test_metrics']['MAPE'],
                'Training_Time': result['training_time']
            }
            comparison_data.append(row)
        
        results_df = pd.DataFrame(comparison_data)
        results_df = results_df.sort_values('Test_RMSE')
        
        print("\nğŸ“ˆ ç®—æ³•æ€§èƒ½æ¯”è¾ƒ (æŒ‰æµ‹è¯•é›†RMSEæ’åº):")
        print("="*80)
        print(results_df.round(4).to_string(index=False))
        
        # ä¿å­˜ç»“æœè¡¨æ ¼
        results_df.to_csv('experiment_1_results.csv', index=False)
        
        # åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(results_df, y_test)
        
        # æ€§èƒ½åˆ†æ
        print(f"\nğŸ† æœ€ä½³ç®—æ³•:")
        best_algorithm = results_df.iloc[0]
        print(f"  ç®—æ³•: {best_algorithm['Algorithm']}")
        print(f"  æµ‹è¯•é›†RMSE: {best_algorithm['Test_RMSE']:.2f}")
        print(f"  æµ‹è¯•é›†RÂ²: {best_algorithm['Test_RÂ²']:.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {best_algorithm['Training_Time']:.2f}ç§’")
        
        # è¾¾åˆ°ç›®æ ‡æ£€æŸ¥
        target_rmse = 200
        target_r2 = 0.75
        
        print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        for _, row in results_df.iterrows():
            rmse_ok = "âœ…" if row['Test_RMSE'] < target_rmse else "âŒ"
            r2_ok = "âœ…" if row['Test_RÂ²'] > target_r2 else "âŒ"
            print(f"  {row['Algorithm']}: RMSE<200 {rmse_ok}, RÂ²>0.75 {r2_ok}")
    
    def create_visualizations(self, results_df, y_test):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        
        # è®¾ç½®å›¾å½¢æ ·å¼
        plt.style.use('default')
        fig = plt.figure(figsize=(20, 15))
        
        # 1. ç®—æ³•æ€§èƒ½æ¯”è¾ƒæ¡å½¢å›¾
        plt.subplot(2, 3, 1)
        algorithms = results_df['Algorithm']
        rmse_values = results_df['Test_RMSE']
        
        bars = plt.bar(range(len(algorithms)), rmse_values, color='skyblue', alpha=0.7)
        plt.xlabel('ç®—æ³•')
        plt.ylabel('RMSE')
        plt.title('æµ‹è¯•é›†RMSEæ¯”è¾ƒ')
        plt.xticks(range(len(algorithms)), algorithms, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(rmse_values):
            plt.text(i, v + 5, f'{v:.1f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 2. RÂ²åˆ†æ•°æ¯”è¾ƒ
        plt.subplot(2, 3, 2)
        r2_values = results_df['Test_RÂ²']
        bars = plt.bar(range(len(algorithms)), r2_values, color='lightgreen', alpha=0.7)
        plt.xlabel('ç®—æ³•')
        plt.ylabel('RÂ² Score')
        plt.title('æµ‹è¯•é›†RÂ²æ¯”è¾ƒ')
        plt.xticks(range(len(algorithms)), algorithms, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(r2_values):
            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 3. è®­ç»ƒæ—¶é—´æ¯”è¾ƒ
        plt.subplot(2, 3, 3)
        time_values = results_df['Training_Time']
        bars = plt.bar(range(len(algorithms)), time_values, color='orange', alpha=0.7)
        plt.xlabel('ç®—æ³•')
        plt.ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')
        plt.title('è®­ç»ƒæ—¶é—´æ¯”è¾ƒ')
        plt.xticks(range(len(algorithms)), algorithms, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(time_values):
            plt.text(i, v + max(time_values)*0.02, f'{v:.1f}', ha='center', va='bottom')
        
        plt.grid(axis='y', alpha=0.3)
        
        # 4. RMSE vs RÂ²æ•£ç‚¹å›¾
        plt.subplot(2, 3, 4)
        plt.scatter(rmse_values, r2_values, s=100, alpha=0.7, c='purple')
        
        for i, alg in enumerate(algorithms):
            plt.annotate(alg, (rmse_values.iloc[i], r2_values.iloc[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.xlabel('RMSE')
        plt.ylabel('RÂ² Score')
        plt.title('RMSE vs RÂ² å…³ç³»å›¾')
        plt.grid(True, alpha=0.3)
        
        # 5. é¢„æµ‹æ•ˆæœå¯¹æ¯”ï¼ˆæœ€ä½³ç®—æ³•ï¼‰
        plt.subplot(2, 3, 5)
        best_alg_name = results_df.iloc[0]['Algorithm']
        best_predictions = self.results[best_alg_name]['predictions']['y_test_pred']
        
        # éšæœºé€‰æ‹©ä¸€äº›ç‚¹è¿›è¡Œå¯è§†åŒ–
        sample_indices = np.random.choice(len(y_test), size=min(200, len(y_test)), replace=False)
        sample_indices = sorted(sample_indices)
        
        plt.scatter(y_test.iloc[sample_indices], best_predictions[sample_indices], 
                   alpha=0.6, s=20)
        
        # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
        min_val = min(y_test.min(), best_predictions.min())
        max_val = max(y_test.max(), best_predictions.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')
        
        plt.xlabel('çœŸå®å€¼')
        plt.ylabel('é¢„æµ‹å€¼')
        plt.title(f'{best_alg_name} - é¢„æµ‹æ•ˆæœ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. è¯¯å·®åˆ†å¸ƒ
        plt.subplot(2, 3, 6)
        errors = y_test.values - best_predictions
        plt.hist(errors, bins=30, alpha=0.7, color='red', edgecolor='black')
        plt.xlabel('é¢„æµ‹è¯¯å·®')
        plt.ylabel('é¢‘æ¬¡')
        plt.title(f'{best_alg_name} - è¯¯å·®åˆ†å¸ƒ')
        plt.axvline(x=0, color='black', linestyle='--', alpha=0.8)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        plt.text(0.02, 0.98, f'å‡å€¼: {errors.mean():.2f}\næ ‡å‡†å·®: {errors.std():.2f}', 
                transform=plt.gca().transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig('experiment_1_algorithm_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“¸ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: experiment_1_algorithm_comparison.png")
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        # ä¿å­˜æ¨¡å‹æ€§èƒ½æ€»ç»“
        summary = {}
        for name, result in self.results.items():
            summary[name] = {
                'test_rmse': result['test_metrics']['RMSE'],
                'test_r2': result['test_metrics']['RÂ²'],
                'test_mae': result['test_metrics']['MAE'],
                'test_mape': result['test_metrics']['MAPE'],
                'training_time': result['training_time']
            }
        
        # ä¿å­˜ä¸ºJSON
        import json
        with open('experiment_1_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜:")
        print("  - experiment_1_results.csv")
        print("  - experiment_1_summary.json")
        print("  - experiment_1_algorithm_comparison.png")

def main():
    """ä¸»å‡½æ•°"""
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = AlgorithmComparison()
    
    # è¿è¡Œå®éªŒ
    results = experiment.run_experiment(use_lag_features=False)
    
    print("\nğŸ‰ å®éªŒ1å®Œæˆï¼")
    print(f"æ€»å…±æµ‹è¯•äº† {len(results)} ä¸ªç®—æ³•")
    
    # è¾“å‡ºæœ€ç»ˆæ€»ç»“
    print("\nğŸ“‹ å®éªŒæ€»ç»“:")
    print("- å·²å®Œæˆæœºå™¨å­¦ä¹ ç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("- è¯†åˆ«æœ€ä½³ç®—æ³•ç”¨äºè‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹")
    print("- ç”Ÿæˆè¯¦ç»†æ€§èƒ½æŠ¥å‘Šå’Œå¯è§†åŒ–")
    print("- ä¸ºåç»­å®éªŒæä¾›åŸºçº¿æ¨¡å‹")

if __name__ == "__main__":
    main() 