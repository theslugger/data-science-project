#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹
åŸºäºæ•°æ®æ´å¯Ÿçš„æ™ºèƒ½ç‰¹å¾å·¥ç¨‹
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor
import warnings

from config import config
from utils import (Logger, DataValidator, DataLoader, ResultSaver, 
                  print_section_header, get_timestamp)

warnings.filterwarnings('ignore')

class EnhancedDataPreprocessor:
    """å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†ç±»"""
    
    def __init__(self):
        self.logger = Logger(self.__class__.__name__)
        self.data_loader = DataLoader(self.logger)
        self.validator = DataValidator()
        self.result_saver = ResultSaver(self.logger)
        
        # é¢„å¤„ç†ç»„ä»¶
        self.scaler = None
        self.feature_selector = None
        self.label_encoders = {}
        
        # å­˜å‚¨ç‰¹å¾ä¿¡æ¯
        self.feature_names = []
        self.feature_metadata = {}
        self.preprocessing_results = {}
        
    def load_and_validate_data(self, file_path=None):
        """åŠ è½½å¹¶éªŒè¯æ•°æ®"""
        print_section_header("æ•°æ®åŠ è½½ä¸åˆæ­¥éªŒè¯", level=1)
        
        # åŠ è½½æ•°æ®
        self.df = self.data_loader.load_data(file_path)
        
        # éªŒè¯å¿…éœ€åˆ—
        required_cols = [
            config.DATA_CONFIG['target_column'],
            config.DATA_CONFIG['date_column']
        ] + config.FEATURE_CONFIG['categorical_features']
        
        self.validator.validate_dataframe(self.df, required_cols)
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        missing_info = self.validator.check_missing_values(self.df)
        if missing_info is not None:
            self.logger.warning(f"å‘ç°ç¼ºå¤±å€¼:\n{missing_info}")
        else:
            self.logger.info("âœ… æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±å€¼")
        
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")
        return self.df
    
    def handle_datetime_features(self):
        """å¤„ç†æ—¥æœŸæ—¶é—´ç‰¹å¾"""
        print_section_header("æ—¥æœŸæ—¶é—´ç‰¹å¾å¤„ç†", level=2)
        
        date_col = config.DATA_CONFIG['date_column']
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        try:
            self.df[date_col] = pd.to_datetime(self.df[date_col], format='mixed', dayfirst=True)
        except:
            self.df[date_col] = pd.to_datetime(self.df[date_col])
        
        # æå–åŸºç¡€æ—¶é—´ç‰¹å¾
        self.df['Year'] = self.df[date_col].dt.year
        self.df['Month'] = self.df[date_col].dt.month
        self.df['Day'] = self.df[date_col].dt.day
        self.df['Weekday'] = self.df[date_col].dt.weekday
        self.df['DayOfYear'] = self.df[date_col].dt.dayofyear
        self.df['Quarter'] = self.df[date_col].dt.quarter
        
        # å‘¨æœ«æ ‡è¯†
        self.df['IsWeekend'] = (self.df['Weekday'] >= 5).astype(int)
        self.df['IsWeekday'] = (self.df['Weekday'] < 5).astype(int)
        
        # ç‰¹æ®Šæ—¥æœŸæ ‡è¯†
        self.df['IsMonday'] = (self.df['Weekday'] == 0).astype(int)
        self.df['IsFriday'] = (self.df['Weekday'] == 4).astype(int)
        
        # å‘¨æœŸæ€§ç¼–ç ï¼ˆä¸‰è§’å‡½æ•°ï¼‰
        if config.FEATURE_CONFIG['time_features']['use_hour_encoding']:
            self.df['Hour_Sin'] = np.sin(2 * np.pi * self.df['Hour'] / 24)
            self.df['Hour_Cos'] = np.cos(2 * np.pi * self.df['Hour'] / 24)
        
        if config.FEATURE_CONFIG['time_features']['use_day_encoding']:
            self.df['DayOfYear_Sin'] = np.sin(2 * np.pi * self.df['DayOfYear'] / 365)
            self.df['DayOfYear_Cos'] = np.cos(2 * np.pi * self.df['DayOfYear'] / 365)
        
        if config.FEATURE_CONFIG['time_features']['use_month_encoding']:
            self.df['Month_Sin'] = np.sin(2 * np.pi * self.df['Month'] / 12)
            self.df['Month_Cos'] = np.cos(2 * np.pi * self.df['Month'] / 12)
        
        time_features = [
            'Year', 'Month', 'Day', 'Weekday', 'DayOfYear', 'Quarter',
            'IsWeekend', 'IsWeekday', 'IsMonday', 'IsFriday'
        ]
        
        if config.FEATURE_CONFIG['time_features']['use_hour_encoding']:
            time_features.extend(['Hour_Sin', 'Hour_Cos'])
        
        if config.FEATURE_CONFIG['time_features']['use_day_encoding']:
            time_features.extend(['DayOfYear_Sin', 'DayOfYear_Cos'])
        
        if config.FEATURE_CONFIG['time_features']['use_month_encoding']:
            time_features.extend(['Month_Sin', 'Month_Cos'])
        
        self.logger.info(f"åˆ›å»ºäº† {len(time_features)} ä¸ªæ—¶é—´ç‰¹å¾")
        return time_features
    
    def create_advanced_time_features(self):
        """åˆ›å»ºé«˜çº§æ—¶é—´ç‰¹å¾ï¼ˆåŸºäºæ•°æ®æ´å¯Ÿï¼‰"""
        print_section_header("é«˜çº§æ—¶é—´ç‰¹å¾åˆ›å»º", level=2)
        
        # åŸºäºåŒå³°æ¨¡å¼çš„æ—¶é—´æ®µç‰¹å¾
        self.df['Hour_Deep_Night'] = (self.df['Hour'].isin([0, 1, 2, 3, 4, 5])).astype(int)
        self.df['Hour_Early_Morning'] = (self.df['Hour'].isin([6, 7])).astype(int)
        self.df['Hour_Morning_Peak'] = (self.df['Hour'].isin([8, 9])).astype(int)
        self.df['Hour_Morning_Decline'] = (self.df['Hour'].isin([10, 11, 12])).astype(int)
        self.df['Hour_Afternoon'] = (self.df['Hour'].isin([13, 14, 15, 16])).astype(int)
        self.df['Hour_Evening_Peak'] = (self.df['Hour'].isin([17, 18, 19])).astype(int)
        self.df['Hour_Evening_Decline'] = (self.df['Hour'].isin([20, 21, 22, 23])).astype(int)
        
        # å³°å€¼æ—¶é—´æ ‡è¯†
        self.df['Is_Peak_Hour'] = (self.df['Hour'].isin([8, 17, 18, 19])).astype(int)
        self.df['Is_Low_Hour'] = (self.df['Hour'].isin([3, 4, 5])).astype(int)
        
        # é€šå‹¤æ—¶é—´æ ‡è¯†
        self.df['Is_Rush_Hour'] = ((self.df['Hour'].between(7, 9)) | 
                                 (self.df['Hour'].between(17, 19))).astype(int)
        
        advanced_time_features = [
            'Hour_Deep_Night', 'Hour_Early_Morning', 'Hour_Morning_Peak',
            'Hour_Morning_Decline', 'Hour_Afternoon', 'Hour_Evening_Peak',
            'Hour_Evening_Decline', 'Is_Peak_Hour', 'Is_Low_Hour', 'Is_Rush_Hour'
        ]
        
        self.logger.info(f"åˆ›å»ºäº† {len(advanced_time_features)} ä¸ªé«˜çº§æ—¶é—´ç‰¹å¾")
        return advanced_time_features
    
    def create_weather_features(self):
        """åˆ›å»ºå¤©æ°”ç‰¹å¾"""
        print_section_header("å¤©æ°”ç‰¹å¾å·¥ç¨‹", level=2)
        
        weather_features = []
        
        # æ¸©åº¦åˆ†æ®µç‰¹å¾
        if 'Temperature(Â°C)' in self.df.columns:
            temp_col = 'Temperature(Â°C)'
            temp_ranges = config.FEATURE_CONFIG['weather_thresholds']['temp_ranges']
            
            self.df['Temp_Severe_Cold'] = (self.df[temp_col] < temp_ranges[1]).astype(int)
            self.df['Temp_Cold'] = ((self.df[temp_col] >= temp_ranges[1]) & 
                                  (self.df[temp_col] < temp_ranges[2])).astype(int)
            self.df['Temp_Cool'] = ((self.df[temp_col] >= temp_ranges[2]) & 
                                  (self.df[temp_col] < temp_ranges[3])).astype(int)
            self.df['Temp_Warm'] = ((self.df[temp_col] >= temp_ranges[3]) & 
                                  (self.df[temp_col] < temp_ranges[4])).astype(int)
            self.df['Temp_Hot'] = (self.df[temp_col] >= temp_ranges[4]).astype(int)
            
            weather_features.extend(['Temp_Severe_Cold', 'Temp_Cold', 'Temp_Cool', 'Temp_Warm', 'Temp_Hot'])
        
        # æ¹¿åº¦åˆ†æ®µç‰¹å¾
        if 'Humidity(%)' in self.df.columns:
            humidity_col = 'Humidity(%)'
            humidity_ranges = config.FEATURE_CONFIG['weather_thresholds']['humidity_ranges']
            
            self.df['Humidity_Low'] = (self.df[humidity_col] < humidity_ranges[1]).astype(int)
            self.df['Humidity_Medium'] = ((self.df[humidity_col] >= humidity_ranges[1]) & 
                                        (self.df[humidity_col] < humidity_ranges[2])).astype(int)
            self.df['Humidity_High'] = ((self.df[humidity_col] >= humidity_ranges[2]) & 
                                      (self.df[humidity_col] < humidity_ranges[3])).astype(int)
            self.df['Humidity_Very_High'] = (self.df[humidity_col] >= humidity_ranges[3]).astype(int)
            
            weather_features.extend(['Humidity_Low', 'Humidity_Medium', 'Humidity_High', 'Humidity_Very_High'])
        
        # é£é€Ÿåˆ†æ®µç‰¹å¾
        if 'Wind speed (m/s)' in self.df.columns:
            wind_col = 'Wind speed (m/s)'
            wind_ranges = config.FEATURE_CONFIG['weather_thresholds']['wind_ranges']
            
            self.df['Wind_Calm'] = (self.df[wind_col] < wind_ranges[1]).astype(int)
            self.df['Wind_Light'] = ((self.df[wind_col] >= wind_ranges[1]) & 
                                   (self.df[wind_col] < wind_ranges[2])).astype(int)
            self.df['Wind_Moderate'] = ((self.df[wind_col] >= wind_ranges[2]) & 
                                      (self.df[wind_col] < wind_ranges[3])).astype(int)
            self.df['Wind_Strong'] = (self.df[wind_col] >= wind_ranges[3]).astype(int)
            
            weather_features.extend(['Wind_Calm', 'Wind_Light', 'Wind_Moderate', 'Wind_Strong'])
        
        # é™æ°´ç‰¹å¾
        if 'Rainfall(mm)' in self.df.columns:
            self.df['Has_Rain'] = (self.df['Rainfall(mm)'] > 0).astype(int)
            self.df['Light_Rain'] = ((self.df['Rainfall(mm)'] > 0) & 
                                   (self.df['Rainfall(mm)'] <= 2.5)).astype(int)
            self.df['Moderate_Rain'] = ((self.df['Rainfall(mm)'] > 2.5) & 
                                      (self.df['Rainfall(mm)'] <= 10)).astype(int)
            self.df['Heavy_Rain'] = (self.df['Rainfall(mm)'] > 10).astype(int)
            
            weather_features.extend(['Has_Rain', 'Light_Rain', 'Moderate_Rain', 'Heavy_Rain'])
        
        if 'Snowfall (cm)' in self.df.columns:
            self.df['Has_Snow'] = (self.df['Snowfall (cm)'] > 0).astype(int)
            self.df['Light_Snow'] = ((self.df['Snowfall (cm)'] > 0) & 
                                   (self.df['Snowfall (cm)'] <= 2)).astype(int)
            self.df['Heavy_Snow'] = (self.df['Snowfall (cm)'] > 2).astype(int)
            
            weather_features.extend(['Has_Snow', 'Light_Snow', 'Heavy_Snow'])
        
        # é™æ°´æ€»é‡
        if 'Rainfall(mm)' in self.df.columns and 'Snowfall (cm)' in self.df.columns:
            self.df['Total_Precipitation'] = self.df['Rainfall(mm)'] + self.df['Snowfall (cm)']
            self.df['Has_Precipitation'] = ((self.df['Rainfall(mm)'] > 0) | 
                                          (self.df['Snowfall (cm)'] > 0)).astype(int)
            weather_features.extend(['Total_Precipitation', 'Has_Precipitation'])
        
        self.logger.info(f"åˆ›å»ºäº† {len(weather_features)} ä¸ªå¤©æ°”ç‰¹å¾")
        return weather_features
    
    def create_comfort_index_features(self):
        """åˆ›å»ºèˆ’é€‚åº¦æŒ‡æ•°ç‰¹å¾"""
        print_section_header("èˆ’é€‚åº¦æŒ‡æ•°ç‰¹å¾", level=2)
        
        comfort_features = []
        
        if 'Temperature(Â°C)' in self.df.columns and 'Humidity(%)' in self.df.columns:
            temp_col = 'Temperature(Â°C)'
            humidity_col = 'Humidity(%)'
            
            # èˆ’é€‚åº¦æŒ‡æ•°ï¼ˆåŸºäºæ¸©åº¦å’Œæ¹¿åº¦ï¼‰
            self.df['Comfort_Index'] = np.where(
                (self.df[temp_col].between(20, 30)) & (self.df[humidity_col].between(30, 70)),
                1.0,  # æœ€èˆ’é€‚
                np.where(
                    (self.df[temp_col].between(10, 35)) & (self.df[humidity_col].between(20, 80)),
                    0.7,  # è¾ƒèˆ’é€‚
                    np.where(
                        (self.df[temp_col].between(0, 40)) & (self.df[humidity_col].between(10, 90)),
                        0.4,  # ä¸€èˆ¬
                        0.1   # ä¸èˆ’é€‚
                    )
                )
            )
            
            # ä½“æ„Ÿæ¸©åº¦ï¼ˆHeat Indexç®€åŒ–ç‰ˆï¼‰
            self.df['Heat_Index'] = (self.df[temp_col] + 
                                   0.5 * (self.df[humidity_col] - 50) / 100 * self.df[temp_col])
            
            # å®Œç¾å¤©æ°”æ ‡è¯†
            perfect_weather_conditions = [
                self.df[temp_col].between(20, 28),
                self.df[humidity_col].between(40, 60),
                self.df.get('Rainfall(mm)', pd.Series([0]*len(self.df))) == 0,
                self.df.get('Snowfall (cm)', pd.Series([0]*len(self.df))) == 0
            ]
            
            if 'Wind speed (m/s)' in self.df.columns:
                perfect_weather_conditions.append(self.df['Wind speed (m/s)'] < 3)
            
            self.df['Perfect_Weather'] = pd.concat(perfect_weather_conditions, axis=1).all(axis=1).astype(int)
            
            # æç«¯å¤©æ°”æ ‡è¯†
            extreme_conditions = [
                self.df[temp_col] < -10,
                self.df[temp_col] > 35,
                self.df[humidity_col] > 90,
                self.df[humidity_col] < 20
            ]
            
            if 'Rainfall(mm)' in self.df.columns:
                extreme_conditions.append(self.df['Rainfall(mm)'] > 10)
            if 'Snowfall (cm)' in self.df.columns:
                extreme_conditions.append(self.df['Snowfall (cm)'] > 5)
            
            self.df['Extreme_Weather'] = pd.concat(extreme_conditions, axis=1).any(axis=1).astype(int)
            
            comfort_features = ['Comfort_Index', 'Heat_Index', 'Perfect_Weather', 'Extreme_Weather']
        
        self.logger.info(f"åˆ›å»ºäº† {len(comfort_features)} ä¸ªèˆ’é€‚åº¦ç‰¹å¾")
        return comfort_features
    
    def create_interaction_features(self):
        """åˆ›å»ºäº¤äº’ç‰¹å¾"""
        print_section_header("äº¤äº’ç‰¹å¾åˆ›å»º", level=2)
        
        if not config.FEATURE_CONFIG['interaction_features']:
            self.logger.info("è·³è¿‡äº¤äº’ç‰¹å¾åˆ›å»º")
            return []
        
        interaction_features = []
        
        # æ¸©åº¦Ã—æ—¶é—´äº¤äº’
        if 'Temperature(Â°C)' in self.df.columns:
            self.df['Temp_Hour'] = self.df['Temperature(Â°C)'] * self.df['Hour']
            self.df['Temp_Weekend'] = self.df['Temperature(Â°C)'] * self.df['IsWeekend']
            self.df['Temp_Peak'] = self.df['Temperature(Â°C)'] * self.df['Is_Peak_Hour']
            interaction_features.extend(['Temp_Hour', 'Temp_Weekend', 'Temp_Peak'])
        
        # å­£èŠ‚Ã—æ—¶é—´äº¤äº’
        if 'Seasons' in self.df.columns:
            for season in self.df['Seasons'].unique():
                season_col = f'Season_{season}'
                self.df[season_col] = (self.df['Seasons'] == season).astype(int)
                
                peak_interaction_col = f'{season}_Peak'
                weekend_interaction_col = f'{season}_Weekend'
                
                self.df[peak_interaction_col] = self.df[season_col] * self.df['Is_Peak_Hour']
                self.df[weekend_interaction_col] = self.df[season_col] * self.df['IsWeekend']
                
                interaction_features.extend([season_col, peak_interaction_col, weekend_interaction_col])
        
        # èˆ’é€‚åº¦Ã—æ—¶é—´äº¤äº’
        if 'Comfort_Index' in self.df.columns:
            self.df['Comfort_Peak'] = self.df['Comfort_Index'] * self.df['Is_Peak_Hour']
            self.df['Comfort_Weekend'] = self.df['Comfort_Index'] * self.df['IsWeekend']
            interaction_features.extend(['Comfort_Peak', 'Comfort_Weekend'])
        
        # å¤©æ°”ç»„åˆç‰¹å¾
        if 'Temperature(Â°C)' in self.df.columns and 'Humidity(%)' in self.df.columns:
            self.df['Temp_Humidity'] = self.df['Temperature(Â°C)'] * self.df['Humidity(%)'] / 100
            interaction_features.append('Temp_Humidity')
        
        self.logger.info(f"åˆ›å»ºäº† {len(interaction_features)} ä¸ªäº¤äº’ç‰¹å¾")
        return interaction_features
    
    def create_lag_features(self, target_col=None):
        """åˆ›å»ºæ»åç‰¹å¾ï¼ˆè°¨æ…ä½¿ç”¨ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰"""
        print_section_header("æ»åç‰¹å¾åˆ›å»º", level=2)
        
        if not config.FEATURE_CONFIG['lag_features']:
            self.logger.info("è·³è¿‡æ»åç‰¹å¾åˆ›å»ºï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰")
            return []
        
        target_col = target_col or config.DATA_CONFIG['target_column']
        
        # æŒ‰æ—¶é—´æ’åº
        self.df = self.df.sort_values([config.DATA_CONFIG['date_column'], 'Hour']).reset_index(drop=True)
        
        lag_features = []
        
        # 1å°æ—¶å‰çš„éœ€æ±‚
        self.df['Demand_Lag_1h'] = self.df[target_col].shift(1)
        
        # 24å°æ—¶å‰çš„éœ€æ±‚ï¼ˆæ˜¨å¤©åŒæ—¶é—´ï¼‰
        self.df['Demand_Lag_24h'] = self.df[target_col].shift(24)
        
        # 7å¤©å‰çš„éœ€æ±‚ï¼ˆä¸Šå‘¨åŒæ—¶é—´ï¼‰
        self.df['Demand_Lag_168h'] = self.df[target_col].shift(168)
        
        # ç§»åŠ¨å¹³å‡ç‰¹å¾
        self.df['Demand_MA_3h'] = self.df[target_col].rolling(window=3, min_periods=1).mean().shift(1)
        self.df['Demand_MA_24h'] = self.df[target_col].rolling(window=24, min_periods=1).mean().shift(1)
        
        lag_features = ['Demand_Lag_1h', 'Demand_Lag_24h', 'Demand_Lag_168h', 'Demand_MA_3h', 'Demand_MA_24h']
        
        # å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨å‡å€¼ï¼‰
        for feature in lag_features:
            self.df[feature].fillna(self.df[target_col].mean(), inplace=True)
        
        self.logger.warning(f"âš ï¸  åˆ›å»ºäº† {len(lag_features)} ä¸ªæ»åç‰¹å¾ï¼Œè¯·æ³¨æ„æ•°æ®æ³„éœ²é£é™©")
        return lag_features
    
    def encode_categorical_features(self):
        """ç¼–ç åˆ†ç±»ç‰¹å¾"""
        print_section_header("åˆ†ç±»ç‰¹å¾ç¼–ç ", level=2)
        
        categorical_cols = config.FEATURE_CONFIG['categorical_features']
        encoded_features = []
        
        for col in categorical_cols:
            if col in self.df.columns:
                # ä½¿ç”¨ç‹¬çƒ­ç¼–ç 
                dummies = pd.get_dummies(self.df[col], prefix=col, drop_first=True)
                self.df = pd.concat([self.df, dummies], axis=1)
                
                # è®°å½•ç¼–ç åçš„ç‰¹å¾å
                encoded_features.extend(dummies.columns.tolist())
                
                # åˆ é™¤åŸå§‹åˆ—
                self.df.drop(col, axis=1, inplace=True)
        
        self.logger.info(f"ç¼–ç äº† {len(categorical_cols)} ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œç”Ÿæˆ {len(encoded_features)} ä¸ªæ–°ç‰¹å¾")
        return encoded_features
    
    def handle_outliers(self, target_col=None):
        """å¤„ç†å¼‚å¸¸å€¼"""
        print_section_header("å¼‚å¸¸å€¼å¤„ç†", level=2)
        
        target_col = target_col or config.DATA_CONFIG['target_column']
        method = config.FEATURE_CONFIG['outlier_method']
        factor = config.FEATURE_CONFIG['outlier_factor']
        
        if method == 'iqr':
            Q1 = self.df[target_col].quantile(0.25)
            Q3 = self.df[target_col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - factor * IQR
            upper_bound = Q3 + factor * IQR
            
            outliers_mask = (self.df[target_col] < lower_bound) | (self.df[target_col] > upper_bound)
            outliers_count = outliers_mask.sum()
            
            self.logger.info(f"æ£€æµ‹åˆ° {outliers_count} ä¸ªå¼‚å¸¸å€¼ ({outliers_count/len(self.df)*100:.2f}%)")
            
            # ä½¿ç”¨Winsorizationè€Œä¸æ˜¯åˆ é™¤
            self.df.loc[self.df[target_col] < lower_bound, target_col] = lower_bound
            self.df.loc[self.df[target_col] > upper_bound, target_col] = upper_bound
            
            self.logger.info("ä½¿ç”¨Winsorizationå¤„ç†å¼‚å¸¸å€¼")
            
        return outliers_count
    
    def handle_non_functioning_days(self):
        """å¤„ç†éè¿è¥æ—¥æ•°æ®"""
        print_section_header("éè¿è¥æ—¥æ•°æ®å¤„ç†", level=2)
        
        if 'Functioning Day' not in self.df.columns:
            self.logger.info("æœªå‘ç°è¿è¥çŠ¶æ€åˆ—ï¼Œè·³è¿‡å¤„ç†")
            return len(self.df)
        
        # ç»Ÿè®¡éè¿è¥æ—¥
        non_functioning_mask = self.df['Functioning Day'] == 'No'
        non_functioning_count = non_functioning_mask.sum()
        
        if non_functioning_count > 0:
            self.logger.info(f"å‘ç° {non_functioning_count} æ¡éè¿è¥æ—¥è®°å½• ({non_functioning_count/len(self.df)*100:.2f}%)")
            
            # ç§»é™¤éè¿è¥æ—¥æ•°æ®
            self.df = self.df[~non_functioning_mask].reset_index(drop=True)
            self.logger.info(f"ç§»é™¤åå‰©ä½™ {len(self.df)} æ¡è®°å½•")
        else:
            self.logger.info("æœªå‘ç°éè¿è¥æ—¥è®°å½•")
        
        return len(self.df)
    
    def select_features(self, target_col=None, method='correlation', k=None):
        """ç‰¹å¾é€‰æ‹©"""
        print_section_header("ç‰¹å¾é€‰æ‹©", level=2)
        
        target_col = target_col or config.DATA_CONFIG['target_column']
        
        # è·å–æ‰€æœ‰æ•°å€¼ç‰¹å¾ï¼ˆæ’é™¤ç›®æ ‡å˜é‡å’Œæ—¥æœŸï¼‰
        exclude_cols = [target_col, config.DATA_CONFIG['date_column']]
        numeric_features = self.df.select_dtypes(include=[np.number]).columns.tolist()
        feature_candidates = [col for col in numeric_features if col not in exclude_cols]
        
        self.logger.info(f"å€™é€‰ç‰¹å¾æ•°é‡: {len(feature_candidates)}")
        
        if method == 'correlation':
            # åŸºäºç›¸å…³æ€§é€‰æ‹©
            correlations = self.df[feature_candidates + [target_col]].corr()[target_col].abs()
            correlations = correlations.drop(target_col).sort_values(ascending=False)
            
            # é€‰æ‹©å‰kä¸ªæˆ–ç›¸å…³æ€§>é˜ˆå€¼çš„ç‰¹å¾
            if k:
                selected_features = correlations.head(k).index.tolist()
            else:
                threshold = 0.1  # ç›¸å…³æ€§é˜ˆå€¼
                selected_features = correlations[correlations > threshold].index.tolist()
            
            self.logger.info(f"åŸºäºç›¸å…³æ€§é€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
            
        elif method == 'univariate':
            # å•å˜é‡ç»Ÿè®¡æ£€éªŒ
            X = self.df[feature_candidates].fillna(0)
            y = self.df[target_col]
            
            k = k or min(20, len(feature_candidates))  # é»˜è®¤é€‰æ‹©20ä¸ªç‰¹å¾
            selector = SelectKBest(score_func=f_regression, k=k)
            selector.fit(X, y)
            
            selected_features = [feature_candidates[i] for i in selector.get_support(indices=True)]
            self.logger.info(f"åŸºäºå•å˜é‡æ£€éªŒé€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
            
        elif method == 'rfe':
            # é€’å½’ç‰¹å¾æ¶ˆé™¤
            X = self.df[feature_candidates].fillna(0)
            y = self.df[target_col]
            
            estimator = RandomForestRegressor(n_estimators=50, random_state=42)
            k = k or min(15, len(feature_candidates))  # é»˜è®¤é€‰æ‹©15ä¸ªç‰¹å¾
            
            rfe = RFE(estimator=estimator, n_features_to_select=k, step=1)
            rfe.fit(X, y)
            
            selected_features = [feature_candidates[i] for i in rfe.get_support(indices=True)]
            self.logger.info(f"åŸºäºRFEé€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
            
        else:
            # ä¿ç•™æ‰€æœ‰ç‰¹å¾
            selected_features = feature_candidates
            self.logger.info(f"ä¿ç•™æ‰€æœ‰ {len(selected_features)} ä¸ªç‰¹å¾")
        
        self.feature_names = selected_features
        return selected_features
    
    def scale_features(self, X_train, X_val, X_test):
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        print_section_header("ç‰¹å¾æ ‡å‡†åŒ–", level=2)
        
        scaling_method = config.FEATURE_CONFIG['scaling_method']
        
        if scaling_method == 'standard':
            self.scaler = StandardScaler()
        elif scaling_method == 'robust':
            self.scaler = RobustScaler()
        else:
            self.logger.info("è·³è¿‡ç‰¹å¾æ ‡å‡†åŒ–")
            return X_train, X_val, X_test
        
        # åªå¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
        numeric_features = X_train.select_dtypes(include=[np.number]).columns
        
        X_train_scaled = X_train.copy()
        X_val_scaled = X_val.copy()
        X_test_scaled = X_test.copy()
        
        # æ‹Ÿåˆå’Œè½¬æ¢
        X_train_scaled[numeric_features] = self.scaler.fit_transform(X_train[numeric_features])
        X_val_scaled[numeric_features] = self.scaler.transform(X_val[numeric_features])
        X_test_scaled[numeric_features] = self.scaler.transform(X_test[numeric_features])
        
        self.logger.info(f"ä½¿ç”¨ {scaling_method} æ–¹æ³•æ ‡å‡†åŒ–äº† {len(numeric_features)} ä¸ªæ•°å€¼ç‰¹å¾")
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def split_data_temporal(self, target_col=None):
        """æ—¶é—´åºåˆ—å‹å¥½çš„æ•°æ®åˆ†å‰²"""
        print_section_header("æ—¶é—´åºåˆ—æ•°æ®åˆ†å‰²", level=2)
        
        target_col = target_col or config.DATA_CONFIG['target_column']
        
        # æŒ‰æ—¶é—´æ’åº
        date_col = config.DATA_CONFIG['date_column']
        self.df = self.df.sort_values([date_col, 'Hour']).reset_index(drop=True)
        
        # åˆ†å‰²æ¯”ä¾‹
        train_ratio = config.DATA_CONFIG['train_ratio']
        val_ratio = config.DATA_CONFIG['val_ratio']
        test_ratio = config.DATA_CONFIG['test_ratio']
        
        n = len(self.df)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        # åˆ†å‰²æ•°æ®
        train_df = self.df.iloc[:train_end].copy()
        val_df = self.df.iloc[train_end:val_end].copy()
        test_df = self.df.iloc[val_end:].copy()
        
        self.logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ:")
        self.logger.info(f"  è®­ç»ƒé›†: {len(train_df)} æ¡ ({len(train_df)/n*100:.1f}%)")
        self.logger.info(f"  éªŒè¯é›†: {len(val_df)} æ¡ ({len(val_df)/n*100:.1f}%)")
        self.logger.info(f"  æµ‹è¯•é›†: {len(test_df)} æ¡ ({len(test_df)/n*100:.1f}%)")
        
        # æå–ç‰¹å¾å’Œç›®æ ‡
        X_train = train_df[self.feature_names]
        y_train = train_df[target_col]
        
        X_val = val_df[self.feature_names]
        y_val = val_df[target_col]
        
        X_test = test_df[self.feature_names]
        y_test = test_df[target_col]
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def get_cross_validation_splitter(self):
        """è·å–æ—¶é—´åºåˆ—äº¤å‰éªŒè¯åˆ†å‰²å™¨"""
        cv_config = config.MODEL_CONFIG['cross_validation']
        
        if cv_config['method'] == 'TimeSeriesSplit':
            return TimeSeriesSplit(n_splits=cv_config['n_splits'])
        else:
            return TimeSeriesSplit(n_splits=5)  # é»˜è®¤
    
    def preprocess_pipeline(self, use_lag_features=False, feature_selection_method='correlation', 
                          feature_selection_k=None):
        """å®Œæ•´çš„é¢„å¤„ç†æµæ°´çº¿"""
        print_section_header("å®Œæ•´é¢„å¤„ç†æµæ°´çº¿", level=1)
        
        timestamp = get_timestamp()
        
        try:
            # 1. æ•°æ®åŠ è½½ä¸éªŒè¯
            self.load_and_validate_data()
            
            # 2. å¤„ç†éè¿è¥æ—¥
            remaining_count = self.handle_non_functioning_days()
            
            # 3. æ—¥æœŸæ—¶é—´ç‰¹å¾
            time_features = self.handle_datetime_features()
            
            # 4. é«˜çº§æ—¶é—´ç‰¹å¾
            advanced_time_features = self.create_advanced_time_features()
            
            # 5. å¤©æ°”ç‰¹å¾
            weather_features = self.create_weather_features()
            
            # 6. èˆ’é€‚åº¦ç‰¹å¾
            comfort_features = self.create_comfort_index_features()
            
            # 7. äº¤äº’ç‰¹å¾
            interaction_features = self.create_interaction_features()
            
            # 8. æ»åç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            lag_features = []
            if use_lag_features:
                lag_features = self.create_lag_features()
            
            # 9. åˆ†ç±»ç‰¹å¾ç¼–ç 
            encoded_features = self.encode_categorical_features()
            
            # 10. å¼‚å¸¸å€¼å¤„ç†
            outliers_count = self.handle_outliers()
            
            # 11. ç‰¹å¾é€‰æ‹©
            selected_features = self.select_features(method=feature_selection_method, k=feature_selection_k)
            
            # 12. æ•°æ®åˆ†å‰²
            X_train, X_val, X_test, y_train, y_val, y_test = self.split_data_temporal()
            
            # 13. ç‰¹å¾æ ‡å‡†åŒ–
            X_train_scaled, X_val_scaled, X_test_scaled = self.scale_features(X_train, X_val, X_test)
            
            # ä¿å­˜é¢„å¤„ç†ç»“æœ
            self.preprocessing_results = {
                'timestamp': timestamp,
                'original_shape': self.df.shape,
                'final_shape': (len(X_train_scaled) + len(X_val_scaled) + len(X_test_scaled), 
                               len(selected_features)),
                'feature_counts': {
                    'time_features': len(time_features),
                    'advanced_time_features': len(advanced_time_features),
                    'weather_features': len(weather_features),
                    'comfort_features': len(comfort_features),
                    'interaction_features': len(interaction_features),
                    'lag_features': len(lag_features),
                    'encoded_features': len(encoded_features),
                    'selected_features': len(selected_features)
                },
                'outliers_handled': outliers_count,
                'non_functioning_removed': self.df.shape[0] != remaining_count,
                'feature_names': selected_features,
                'preprocessing_config': {
                    'use_lag_features': use_lag_features,
                    'feature_selection_method': feature_selection_method,
                    'feature_selection_k': feature_selection_k
                }
            }
            
            # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
            self.save_preprocessed_data(X_train_scaled, X_val_scaled, X_test_scaled,
                                      y_train, y_val, y_test, timestamp)
            
            # ç”Ÿæˆé¢„å¤„ç†æŠ¥å‘Š
            self.generate_preprocessing_report()
            
            return (X_train_scaled, X_val_scaled, X_test_scaled, 
                   y_train, y_val, y_test, self.preprocessing_results)
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            raise
    
    def save_preprocessed_data(self, X_train, X_val, X_test, y_train, y_val, y_test, timestamp):
        """ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®"""
        print_section_header("ä¿å­˜é¢„å¤„ç†æ•°æ®", level=2)
        
        # ä¿å­˜ç‰¹å¾æ•°æ®
        datasets = {
            'X_train': X_train,
            'X_val': X_val,
            'X_test': X_test,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test
        }
        
        for name, data in datasets.items():
            if hasattr(data, 'values'):
                np.save(config.OUTPUT_DIR / f"{name}_{timestamp}.npy", data.values)
            else:
                np.save(config.OUTPUT_DIR / f"{name}_{timestamp}.npy", data)
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.result_saver.save_json(self.feature_names, f"feature_names_{timestamp}")
        
        # ä¿å­˜é¢„å¤„ç†å™¨çŠ¶æ€
        import pickle
        preprocessor_state = {
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'preprocessing_results': self.preprocessing_results
        }
        
        with open(config.OUTPUT_DIR / f"preprocessor_{timestamp}.pkl", 'wb') as f:
            pickle.dump(preprocessor_state, f)
        
        self.logger.info(f"âœ… é¢„å¤„ç†æ•°æ®å·²ä¿å­˜ï¼Œæ—¶é—´æˆ³: {timestamp}")
    
    def generate_preprocessing_report(self):
        """ç”Ÿæˆé¢„å¤„ç†æŠ¥å‘Š"""
        print_section_header("é¢„å¤„ç†æ€»ç»“æŠ¥å‘Š", level=1)
        
        results = self.preprocessing_results
        
        print("ğŸ¯ é¢„å¤„ç†å®Œæˆæ‘˜è¦:")
        print(f"1. åŸå§‹æ•°æ®: {results['original_shape'][0]} è¡Œ Ã— {results['original_shape'][1]} åˆ—")
        print(f"2. æœ€ç»ˆæ•°æ®: {results['final_shape'][0]} è¡Œ Ã— {results['final_shape'][1]} åˆ—")
        
        print(f"\nğŸ“Š ç‰¹å¾å·¥ç¨‹ç»Ÿè®¡:")
        feature_counts = results['feature_counts']
        for feature_type, count in feature_counts.items():
            print(f"  {feature_type}: {count} ä¸ª")
        
        print(f"\nğŸ”§ æ•°æ®å¤„ç†ç»Ÿè®¡:")
        print(f"  å¼‚å¸¸å€¼å¤„ç†: {results['outliers_handled']} ä¸ª")
        print(f"  éè¿è¥æ—¥ç§»é™¤: {'æ˜¯' if results['non_functioning_removed'] else 'å¦'}")
        
        print(f"\nâš™ï¸  é¢„å¤„ç†é…ç½®:")
        config_info = results['preprocessing_config']
        for key, value in config_info.items():
            print(f"  {key}: {value}")
        
        print(f"\nğŸ† Top 10 é€‰æ‹©ç‰¹å¾:")
        for i, feature in enumerate(results['feature_names'][:10], 1):
            print(f"  {i:2d}. {feature}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        self.result_saver.save_json(results, f"preprocessing_results_{results['timestamp']}", "preprocessing")
        
        print(f"\nâœ… é¢„å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {config.OUTPUT_DIR}")

def main():
    """ä¸»å‡½æ•°"""
    print_section_header("é¦–å°”è‡ªè¡Œè½¦å…±äº«æ•°æ® - å¢å¼ºç‰ˆé¢„å¤„ç†", level=1)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = EnhancedDataPreprocessor()
    
    try:
        # è¿è¡Œå®Œæ•´é¢„å¤„ç†æµæ°´çº¿
        (X_train, X_val, X_test, y_train, y_val, y_test, results) = preprocessor.preprocess_pipeline(
            use_lag_features=False,  # é»˜è®¤ä¸ä½¿ç”¨æ»åç‰¹å¾
            feature_selection_method='correlation',  # åŸºäºç›¸å…³æ€§é€‰æ‹©ç‰¹å¾
            feature_selection_k=None  # è‡ªåŠ¨ç¡®å®šç‰¹å¾æ•°é‡
        )
        
        print(f"\nâœ… é¢„å¤„ç†ç®¡é“æ‰§è¡ŒæˆåŠŸï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶:")
        print(f"  è®­ç»ƒé›†: {X_train.shape}")
        print(f"  éªŒè¯é›†: {X_val.shape}")
        print(f"  æµ‹è¯•é›†: {X_test.shape}")
        
        return results
        
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    results = main() 