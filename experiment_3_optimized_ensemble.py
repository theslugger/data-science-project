#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒ3ï¼šåŸºäº90ç‰¹å¾çš„ä¼˜åŒ–é›†æˆå­¦ä¹ 
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹

é’ˆå¯¹æ•°æ®é¢„å¤„ç†ç»“æœä¼˜åŒ–ï¼š
- 90ä¸ªå¢å¼ºç‰¹å¾
- 8465æ¡æœ‰æ•ˆè®°å½•ï¼ˆæ’é™¤295æ¡éè¿è¥æ—¥ï¼‰
- 152ä¸ªå¼‚å¸¸å€¼å·²å¤„ç†
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import (
    BaggingRegressor, VotingRegressor
)
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import time
import warnings
warnings.filterwarnings('ignore')

from data_preprocessing import BikeDataPreprocessor

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'PingFang SC', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class OptimizedEnsembleExperiment:
    """åŸºäº90ç‰¹å¾çš„ä¼˜åŒ–é›†æˆå­¦ä¹ å®éªŒç±»"""
    
    def __init__(self):
        self.results = {}
        self.models = {}
        self.preprocessor = BikeDataPreprocessor()
        
    def calculate_metrics(self, y_true, y_pred):
        """è®¡ç®—å…¨é¢çš„å›å½’æŒ‡æ ‡"""
        from sklearn.metrics import explained_variance_score, mean_squared_log_error
        
        # åŸºç¡€æŒ‡æ ‡
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # é¿å…é™¤é›¶é”™è¯¯å’Œè´Ÿå€¼
        y_true_safe = np.maximum(y_true, 1e-8)
        y_pred_safe = np.maximum(y_pred, 1e-8)
        
        # MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100
        
        # MSLE (Mean Squared Log Error)
        try:
            if np.all(y_true >= 0) and np.all(y_pred >= 0):
                msle = mean_squared_log_error(y_true_safe, y_pred_safe)
            else:
                msle = np.nan
        except:
            msle = np.nan
        
        # Explained Variance Score
        evs = explained_variance_score(y_true, y_pred)
        
        # SMAPE (Symmetric Mean Absolute Percentage Error)
        smape = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'MAPE': mape,
            'MSLE': msle,
            'EVS': evs,
            'SMAPE': smape
        }
    
    def create_optimized_bagging_models(self):
        """é’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„Baggingæ–¹æ³•"""
        print("ğŸ¯ åˆ›å»ºé’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„Baggingæ¨¡å‹...")
        
        bagging_models = {
            'Bagging Linear (90F)': BaggingRegressor(
                estimator=LinearRegression(),
                n_estimators=100,  # çº¿æ€§æ¨¡å‹é›†æˆæ•°é‡
                max_samples=0.8,  # æ ·æœ¬é‡‡æ ·
                max_features=0.7,  # 70%ç‰¹å¾é‡‡æ ·ï¼Œçº¦63ä¸ªç‰¹å¾
                bootstrap=True,
                bootstrap_features=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging Ridge (90F)': BaggingRegressor(
                estimator=Ridge(alpha=0.01, solver='auto', max_iter=5000),
                n_estimators=100,
                max_samples=0.8,
                max_features=0.7,  # 63ä¸ªç‰¹å¾
                bootstrap=True,
                bootstrap_features=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging KNN (90F)': BaggingRegressor(
                estimator=KNeighborsRegressor(
                    n_neighbors=15,  # é’ˆå¯¹5925ä¸ªè®­ç»ƒæ ·æœ¬
                    weights='distance',  # è·ç¦»åŠ æƒ
                    algorithm='auto',
                    leaf_size=30,
                    n_jobs=-1
                ),
                n_estimators=50,  # KNNé›†æˆæ•°é‡
                max_samples=0.8,
                max_features=0.6,  # 54ä¸ªç‰¹å¾ï¼Œé¿å…ç»´åº¦ç¾éš¾
                bootstrap=True,
                bootstrap_features=True,
                random_state=42,
                n_jobs=1  # KNNå·²ç»å¹¶è¡Œ
            ),
            
            'Bagging SVR (90F)': BaggingRegressor(
                estimator=SVR(
                    kernel='rbf',
                    C=10.0,
                    gamma='scale',
                    epsilon=0.01,
                    cache_size=300
                ),
                n_estimators=30,  # SVRè®¡ç®—é‡å¤§
                max_samples=0.7,
                max_features=0.5,  # 45ä¸ªç‰¹å¾ï¼Œé¿å…è¿‡æ‹Ÿåˆ
                bootstrap=True,
                bootstrap_features=True,
                random_state=42,
                n_jobs=-1
            ),
            
            'Bagging Neural Net (90F)': BaggingRegressor(
                estimator=MLPRegressor(
                    hidden_layer_sizes=(128, 64, 32),  # é’ˆå¯¹90ç‰¹å¾çš„ç½‘ç»œ
                    activation='relu',
                    solver='adam',
                    learning_rate='adaptive',
                    learning_rate_init=0.001,
                    alpha=0.001,
                    max_iter=3000,
                    early_stopping=True,
                    validation_fraction=0.15,
                    n_iter_no_change=50,
                    random_state=42
                ),
                n_estimators=20,  # ç¥ç»ç½‘ç»œé›†æˆæ•°é‡
                max_samples=0.85,
                max_features=0.8,  # 72ä¸ªç‰¹å¾
                random_state=42,
                n_jobs=1  # é¿å…åµŒå¥—å¹¶è¡Œ
            )
        }
        
        return bagging_models
    
    def create_base_models(self):
        """åˆ›å»ºå››ç§åŸºç¡€æ¨¡å‹"""
        print("ğŸš€ åˆ›å»ºå››ç§åŸºç¡€æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ã€KNNã€ç¥ç»ç½‘ç»œã€SVRï¼‰...")
        
        base_models = {
            'Linear Regression (90F)': LinearRegression(),
            
            'Ridge Regression (90F)': Ridge(
                alpha=0.01,  # é’ˆå¯¹90ç‰¹å¾çš„æ­£åˆ™åŒ–
                solver='auto',
                max_iter=5000
            ),
            
            'Lasso Regression (90F)': Lasso(
                alpha=0.001,  # è¾ƒå°çš„æ­£åˆ™åŒ–ä¿ç•™æ›´å¤šç‰¹å¾
                max_iter=4000,
                selection='random'
            ),
            
            'KNN Regression (90F)': KNeighborsRegressor(
                n_neighbors=20,  # é’ˆå¯¹5925ä¸ªè®­ç»ƒæ ·æœ¬
                weights='distance',  # è·ç¦»åŠ æƒ
                algorithm='auto',
                leaf_size=30,
                metric='minkowski',
                p=2,  # æ¬§å‡ é‡Œå¾—è·ç¦»
                n_jobs=-1
            ),
            
            'Neural Network Basic (90F)': MLPRegressor(
                hidden_layer_sizes=(128, 64),  # åŸºç¡€ç½‘ç»œ
                activation='relu',
                solver='adam',
                learning_rate='adaptive',
                learning_rate_init=0.001,
                alpha=0.001,
                max_iter=3000,
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=50,
                random_state=42
            ),
            
            'Neural Network Deep (90F)': MLPRegressor(
                hidden_layer_sizes=(180, 90, 45, 22),  # æ·±åº¦ç½‘ç»œ
                activation='relu',
                solver='adam',
                learning_rate='adaptive',
                learning_rate_init=0.0005,
                alpha=0.0001,
                max_iter=4000,
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=60,
                random_state=42
            ),
            
            'SVR Linear (90F)': SVR(
                kernel='linear',
                C=1.0,
                epsilon=0.01,
                max_iter=5000
            ),
            
            'SVR RBF (90F)': SVR(
                kernel='rbf',
                C=10.0,
                gamma='scale',
                epsilon=0.01,
                cache_size=500
            ),
            
            'SVR Poly (90F)': SVR(
                kernel='poly',
                degree=3,
                C=1.0,
                gamma='scale',
                epsilon=0.01,
                cache_size=300
            )
        }
        
        return base_models
    
    def create_optimized_voting_models(self):
        """é’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„Votingæ–¹æ³•"""
        print("ğŸ—³ï¸ åˆ›å»ºé’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„Votingæ¨¡å‹...")
        
        # åŸºç¡€æ¨¡å‹é›†åˆï¼ˆå››ç§ç±»å‹ï¼‰
        base_linear = LinearRegression()
        
        base_ridge = Ridge(alpha=0.01, solver='auto', max_iter=5000)
        
        base_knn = KNeighborsRegressor(
            n_neighbors=20, weights='distance', 
            algorithm='auto', n_jobs=-1
        )
        
        base_nn = MLPRegressor(
            hidden_layer_sizes=(180, 90, 45),  # 2x, 1x, 0.5xç‰¹å¾æ•°
            activation='relu', solver='adam',
            learning_rate='adaptive', learning_rate_init=0.001,
            alpha=0.001, max_iter=4000,
            early_stopping=True, validation_fraction=0.15,
            n_iter_no_change=60, random_state=42
        )
        
        base_svr = SVR(
            kernel='rbf', C=10.0, gamma='scale',
            epsilon=0.01, cache_size=500
        )
        
        voting_models = {
            'Voting Linear Models (90F)': VotingRegressor(
                estimators=[
                    ('linear', base_linear),
                    ('ridge', base_ridge)
                ],
                weights=[1.0, 1.2]  # Ridgeæƒé‡ç¨é«˜
            ),
            
            'Voting All Four (90F)': VotingRegressor(
                estimators=[
                    ('linear', base_linear),
                    ('knn', base_knn),
                    ('nn', base_nn),
                    ('svr', base_svr)
                ],
                weights=[1.0, 0.8, 1.3, 1.1]  # NNæƒé‡æœ€é«˜
            ),
            
            'Voting Linear+NN (90F)': VotingRegressor(
                estimators=[
                    ('ridge', base_ridge),
                    ('nn', base_nn)
                ],
                weights=[0.8, 1.2]  # NNæƒé‡æ›´é«˜
            ),
            
            'Voting KNN+SVR (90F)': VotingRegressor(
                estimators=[
                    ('knn', base_knn),
                    ('svr', base_svr)
                ],
                weights=[1.0, 1.0]  # ç­‰æƒé‡
            )
        }
        
        return voting_models
    
    def create_optimized_stacking_models(self, X_train, y_train, X_val, y_val):
        """é’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„Stackingæ–¹æ³•"""
        print("ğŸ—ï¸ åˆ›å»ºé’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„Stackingæ¨¡å‹...")
        
        # ç¬¬ä¸€å±‚ï¼šå››ç§åŸºç¡€æ¨¡å‹çš„å¤šä¸ªå˜ä½“
        base_models = {
            'linear': LinearRegression(),
            
            'ridge': Ridge(alpha=0.01, solver='auto', max_iter=5000),
            
            'lasso': Lasso(alpha=0.001, max_iter=4000, selection='random'),
            
            'knn1': KNeighborsRegressor(
                n_neighbors=15, weights='distance', 
                algorithm='auto', n_jobs=-1
            ),
            
            'knn2': KNeighborsRegressor(
                n_neighbors=25, weights='uniform',
                algorithm='auto', n_jobs=-1
            ),
            
            'svr_linear': SVR(
                kernel='linear', C=1.0, epsilon=0.01, max_iter=5000
            ),
            
            'svr_rbf': SVR(
                kernel='rbf', C=10.0, gamma='scale',
                epsilon=0.01, cache_size=500
            ),
            
            'nn1': MLPRegressor(
                hidden_layer_sizes=(128, 64),  # åŸºç¡€ç½‘ç»œ
                activation='relu', solver='adam',
                learning_rate='adaptive', learning_rate_init=0.001,
                alpha=0.001, max_iter=3000,
                early_stopping=True, validation_fraction=0.15,
                n_iter_no_change=40, random_state=42
            ),
            
            'nn2': MLPRegressor(
                hidden_layer_sizes=(180, 90, 45),  # æ·±åº¦ç½‘ç»œ
                activation='relu', solver='adam',
                learning_rate='adaptive', learning_rate_init=0.0005,
                alpha=0.0001, max_iter=4000,
                early_stopping=True, validation_fraction=0.15,
                n_iter_no_change=50, random_state=43
            )
        }
        
        # è®­ç»ƒåŸºç¡€æ¨¡å‹å¹¶è·å–é¢„æµ‹
        print("  è®­ç»ƒ9ä¸ªåŸºç¡€æ¨¡å‹...")
        base_predictions_train = np.zeros((X_train.shape[0], len(base_models)))
        base_predictions_val = np.zeros((X_val.shape[0], len(base_models)))
        
        for i, (name, model) in enumerate(base_models.items()):
            print(f"    è®­ç»ƒåŸºç¡€æ¨¡å‹ {i+1}/9: {name}")
            model.fit(X_train, y_train)
            base_predictions_train[:, i] = model.predict(X_train)
            base_predictions_val[:, i] = model.predict(X_val)
        
        # ç¬¬äºŒå±‚ï¼šå…ƒå­¦ä¹ å™¨ï¼ˆé’ˆå¯¹9ä¸ªåŸºå­¦ä¹ å™¨è¾“å‡ºä¼˜åŒ–ï¼‰
        print("  åˆ›å»ºå…ƒå­¦ä¹ å™¨...")
        meta_models = {
            'Stacking Linear (90F)': LinearRegression(),
            
            'Stacking Ridge (90F)': Ridge(alpha=0.1, solver='auto'),
            
            'Stacking Lasso (90F)': Lasso(alpha=0.02, max_iter=2000),
            
            'Stacking KNN (90F)': KNeighborsRegressor(
                n_neighbors=5, weights='distance', algorithm='auto'
            ),
            
            'Stacking SVR (90F)': SVR(
                kernel='rbf', C=1.0, gamma='scale', epsilon=0.01
            ),
            
            'Stacking NN (90F)': MLPRegressor(
                hidden_layer_sizes=(18, 9),  # é’ˆå¯¹9ä¸ªè¾“å…¥çš„ç½‘ç»œ
                activation='relu', solver='adam',
                learning_rate='constant', learning_rate_init=0.01,
                alpha=0.01, max_iter=2000,
                early_stopping=True, validation_fraction=0.15,
                n_iter_no_change=30, random_state=42
            )
        }
        
        stacking_results = {}
        
        for name, meta_model in meta_models.items():
            print(f"    è®­ç»ƒå…ƒæ¨¡å‹: {name}")
            meta_model.fit(base_predictions_train, y_train)
            
            # åˆ›å»ºç»„åˆæ¨¡å‹ç±»
            class StackingModel:
                def __init__(self, base_models, meta_model):
                    self.base_models = base_models
                    self.meta_model = meta_model
                
                def predict(self, X):
                    base_preds = np.zeros((X.shape[0], len(self.base_models)))
                    for i, model in enumerate(self.base_models.values()):
                        base_preds[:, i] = model.predict(X)
                    return self.meta_model.predict(base_preds)
                
                def fit(self, X, y):
                    pass  # å·²ç»è®­ç»ƒå¥½äº†
            
            stacking_model = StackingModel(base_models, meta_model)
            stacking_results[name] = stacking_model
        
        return stacking_results
    
    def evaluate_model(self, model, X_train, X_val, X_test, y_train, y_val, y_test, name):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\n=== è¯„ä¼° {name} ===")
        
        start_time = time.time()
        
        # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰è®­ç»ƒï¼Œåˆ™è®­ç»ƒå®ƒ
        if hasattr(model, 'fit') and not hasattr(model, 'base_models'):
            model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # é¢„æµ‹
        try:
            y_train_pred = model.predict(X_train)
            y_val_pred = model.predict(X_val)
            y_test_pred = model.predict(X_test)
        except Exception as e:
            print(f"âŒ {name} é¢„æµ‹å¤±è´¥: {str(e)}")
            return None
        
        # è®¡ç®—æŒ‡æ ‡
        train_metrics = self.calculate_metrics(y_train, y_train_pred)
        val_metrics = self.calculate_metrics(y_val, y_val_pred)
        test_metrics = self.calculate_metrics(y_test, y_test_pred)
        
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"éªŒè¯é›† RMSE: {val_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RMSE: {test_metrics['RMSE']:.2f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_metrics['RÂ²']:.4f}")
        print(f"æµ‹è¯•é›† MAPE: {test_metrics['MAPE']:.2f}%")
        
        return {
            'model': model,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'training_time': training_time,
            'predictions': {
                'y_train_pred': y_train_pred,
                'y_val_pred': y_val_pred,
                'y_test_pred': y_test_pred
            }
        }
    
    def run_optimized_experiment(self):
        """è¿è¡Œé’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„é›†æˆå­¦ä¹ å®éªŒ"""
        print("ğŸš€ å¼€å§‹é’ˆå¯¹90ç‰¹å¾çš„ä¼˜åŒ–é›†æˆå­¦ä¹ å®éªŒ")
        print("="*60)
        
        # 1. æ•°æ®é¢„å¤„ç†ï¼ˆä½¿ç”¨æ»åç‰¹å¾ï¼‰
        print("1. åŸºäº90ç‰¹å¾çš„æ•°æ®é¢„å¤„ç†...")
        df = self.preprocessor.load_data()
        df_processed = self.preprocessor.prepare_features(
            df, use_lag_features=True, exclude_non_operating=True
        )
        
        print(f"å¤„ç†åæ•°æ®: {len(df_processed)} æ¡è®°å½•")
        print(f"ç‰¹å¾æ•°é‡: {len(self.preprocessor.feature_names)}")
        
        # 2. æ•°æ®åˆ†å‰²
        X_train, X_val, X_test, y_train, y_val, y_test = \
            self.preprocessor.split_data_temporal(df_processed)
        
        # 3. ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled = \
            self.preprocessor.scale_features(X_train, X_val, X_test)
        
        print(f"\næ•°æ®æ¦‚è§ˆ:")
        print(f"  ç‰¹å¾æ•°é‡: {X_train_scaled.shape[1]}")
        print(f"  è®­ç»ƒæ ·æœ¬: {X_train_scaled.shape[0]}")
        print(f"  éªŒè¯æ ·æœ¬: {X_val_scaled.shape[0]}")
        print(f"  æµ‹è¯•æ ·æœ¬: {X_test_scaled.shape[0]}")
        
        # 4. åˆ›å»ºæ‰€æœ‰ä¼˜åŒ–æ¨¡å‹
        print("\n2. åˆ›å»ºé’ˆå¯¹90ç‰¹å¾ä¼˜åŒ–çš„é›†æˆæ¨¡å‹...")
        all_models = {}
        
        # åŸºç¡€æ¨¡å‹
        base_models = self.create_base_models()
        all_models.update(base_models)
        
        # Baggingæ–¹æ³•
        bagging_models = self.create_optimized_bagging_models()
        all_models.update(bagging_models)
        
        # Votingæ–¹æ³•
        voting_models = self.create_optimized_voting_models()
        all_models.update(voting_models)
        
        # Stackingæ–¹æ³•
        stacking_models = self.create_optimized_stacking_models(
            X_train_scaled, y_train, X_val_scaled, y_val
        )
        all_models.update(stacking_models)
        
        # 5. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        print(f"\n3. è¯„ä¼° {len(all_models)} ä¸ªä¼˜åŒ–é›†æˆæ¨¡å‹...")
        
        for i, (name, model) in enumerate(all_models.items(), 1):
            print(f"\næ¨¡å‹è¿›åº¦: {i}/{len(all_models)}")
            try:
                result = self.evaluate_model(
                    model, X_train_scaled, X_val_scaled, X_test_scaled,
                    y_train, y_val, y_test, name
                )
                if result:
                    self.results[name] = result
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {name} è¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        print(f"\nâœ… æˆåŠŸè¯„ä¼°äº† {len(self.results)} ä¸ªæ¨¡å‹")
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        if self.results:
            self.generate_optimized_report()
            self.save_optimized_results()
        
        return self.results
    
    def generate_optimized_report(self):
        """ç”Ÿæˆä¼˜åŒ–å®éªŒæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆ90ç‰¹å¾ä¼˜åŒ–é›†æˆå­¦ä¹ æŠ¥å‘Š...")
        
        # åˆ›å»ºç»“æœDataFrame
        comparison_data = []
        for name, result in self.results.items():
            # ç¡®å®šæ¨¡å‹ç±»å‹
            if 'Bagging' in name:
                method_type = 'Bagging'
            elif 'Voting' in name:
                method_type = 'Voting'
            elif 'Stacking' in name:
                method_type = 'Stacking'
            elif any(base_type in name for base_type in ['Linear', 'Ridge', 'Lasso', 'KNN', 'Neural', 'SVR']):
                method_type = 'Base Model'
            else:
                method_type = 'Other'
            
            row = {
                'Model': name,
                'Type': method_type,
                'Train_RMSE': result['train_metrics']['RMSE'],
                'Val_RMSE': result['val_metrics']['RMSE'],
                'Test_RMSE': result['test_metrics']['RMSE'],
                'Test_MAE': result['test_metrics']['MAE'],
                'Test_RÂ²': result['test_metrics']['RÂ²'],
                'Test_MAPE': result['test_metrics']['MAPE'],
                'Test_SMAPE': result['test_metrics']['SMAPE'],
                'Training_Time': result['training_time']
            }
            comparison_data.append(row)
        
        results_df = pd.DataFrame(comparison_data)
        results_df = results_df.sort_values('Test_RMSE')
        
        print("\nğŸ“ˆ 90ç‰¹å¾ä¼˜åŒ–é›†æˆå­¦ä¹ ç»“æœ (æŒ‰æµ‹è¯•é›†RMSEæ’åº):")
        print("="*120)
        print(results_df.round(4).to_string(index=False))
        
        # ä¿å­˜ç»“æœè¡¨æ ¼
        results_df.to_csv('optimized_90f_ensemble_results.csv', index=False)
        
        # æœ€ä½³æ¨¡å‹åˆ†æ
        best_model = results_df.iloc[0]
        print(f"\nğŸ† æœ€ä½³90ç‰¹å¾ä¼˜åŒ–æ¨¡å‹:")
        print(f"  æ¨¡å‹: {best_model['Model']}")
        print(f"  ç±»å‹: {best_model['Type']}")
        print(f"  æµ‹è¯•é›†RMSE: {best_model['Test_RMSE']:.2f}")
        print(f"  æµ‹è¯•é›†RÂ²: {best_model['Test_RÂ²']:.4f}")
        print(f"  æµ‹è¯•é›†MAPE: {best_model['Test_MAPE']:.2f}%")
        print(f"  è®­ç»ƒæ—¶é—´: {best_model['Training_Time']:.2f}ç§’")
        
        # æŒ‰ç±»å‹åˆ†æ
        print(f"\nğŸ“Š æŒ‰é›†æˆç±»å‹åˆ†æ (90ç‰¹å¾ä¼˜åŒ–):")
        for method_type in ['Base Model', 'Bagging', 'Voting', 'Stacking']:
            type_results = results_df[results_df['Type'] == method_type]
            if not type_results.empty:
                best_in_type = type_results.iloc[0]
                avg_rmse = type_results['Test_RMSE'].mean()
                print(f"  {method_type}:")
                print(f"    æœ€ä½³: {best_in_type['Model']} (RMSE: {best_in_type['Test_RMSE']:.2f})")
                print(f"    å¹³å‡RMSE: {avg_rmse:.2f}")
        
        # ç›®æ ‡è¾¾æˆæƒ…å†µ
        target_rmse = 200
        target_r2 = 0.75
        
        print(f"\nğŸ¯ 90ç‰¹å¾æ¨¡å‹ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        successful_models = 0
        for _, row in results_df.iterrows():
            rmse_ok = row['Test_RMSE'] < target_rmse
            r2_ok = row['Test_RÂ²'] > target_r2
            if rmse_ok and r2_ok:
                successful_models += 1
                print(f"  âœ… {row['Model']}: RMSE={row['Test_RMSE']:.2f}, RÂ²={row['Test_RÂ²']:.3f}")
            elif rmse_ok:
                print(f"  ğŸŸ¡ {row['Model']}: RMSEè¾¾æ ‡({row['Test_RMSE']:.2f}), RÂ²={row['Test_RÂ²']:.3f}")
        
        print(f"\nğŸ“ˆ å®Œå…¨è¾¾æ ‡æ¨¡å‹: {successful_models}/{len(results_df)} ({successful_models/len(results_df)*100:.1f}%)")
        
        return results_df
    
    def save_optimized_results(self):
        """ä¿å­˜ä¼˜åŒ–å®éªŒç»“æœ"""
        # ä¿å­˜90ç‰¹å¾ä¼˜åŒ–æ€»ç»“
        optimized_summary = {}
        for name, result in self.results.items():
            optimized_summary[name] = {
                'test_rmse': result['test_metrics']['RMSE'],
                'test_r2': result['test_metrics']['RÂ²'],
                'test_mae': result['test_metrics']['MAE'],
                'test_mape': result['test_metrics']['MAPE'],
                'test_smape': result['test_metrics']['SMAPE'],
                'training_time': result['training_time']
            }
        
        # ä¿å­˜ä¸ºJSON
        import json
        with open('optimized_90f_ensemble_summary.json', 'w', encoding='utf-8') as f:
            json.dump(optimized_summary, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ 90ç‰¹å¾ä¼˜åŒ–å®éªŒç»“æœå·²ä¿å­˜:")
        print("  - optimized_90f_ensemble_results.csv")
        print("  - optimized_90f_ensemble_summary.json")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ åŸºäº90ç‰¹å¾çš„ä¼˜åŒ–é›†æˆå­¦ä¹ å®éªŒ")
    print("=" * 80)
    print("é’ˆå¯¹æ•°æ®é¢„å¤„ç†ç»“æœçš„ä¼˜åŒ–:")
    print("âœ… 90ä¸ªå¢å¼ºç‰¹å¾ï¼ˆåŒ…å«æ»åç‰¹å¾ï¼‰")
    print("âœ… 8465æ¡æœ‰æ•ˆè®°å½•ï¼ˆæ’é™¤295æ¡éè¿è¥æ—¥ï¼‰")
    print("âœ… 152ä¸ªå¼‚å¸¸å€¼å·²å¤„ç†ï¼ˆwinsorizationï¼‰")
    print("âœ… é’ˆå¯¹ç‰¹å¾æ•°é‡ä¼˜åŒ–çš„æ¨¡å‹å‚æ•°")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = OptimizedEnsembleExperiment()
    
    # è¿è¡Œä¼˜åŒ–å®éªŒ
    results = experiment.run_optimized_experiment()
    
    if results:
        print(f"\nğŸ‰ å®éªŒå®Œæˆï¼æˆåŠŸæµ‹è¯•äº† {len(results)} ä¸ªä¼˜åŒ–é›†æˆæ¨¡å‹")
        
        # è¾“å‡ºå…³é”®æ´å¯Ÿ
        print("\nğŸ’¡ 90ç‰¹å¾ä¼˜åŒ–å…³é”®æ´å¯Ÿ:")
        print("- åŸºäºå››ç§æ ¸å¿ƒç®—æ³•ï¼šçº¿æ€§å›å½’ã€KNNã€ç¥ç»ç½‘ç»œã€SVR")
        print("- ç¥ç»ç½‘ç»œå±‚çº§è®¾è®¡è€ƒè™‘ç‰¹å¾ç»´åº¦ (180-90-45)")
        print("- KNNä½¿ç”¨è·ç¦»åŠ æƒé¿å…ç»´åº¦ç¾éš¾")
        print("- SVRé‡‡ç”¨å¤šç§æ ¸å‡½æ•°ï¼ˆçº¿æ€§ã€RBFã€å¤šé¡¹å¼ï¼‰")
        print("- Stackingå…ƒå­¦ä¹ å™¨é’ˆå¯¹9ä¸ªåŸºå­¦ä¹ å™¨è¾“å‡ºä¼˜åŒ–")
        print("- è€ƒè™‘äº†5925ä¸ªè®­ç»ƒæ ·æœ¬çš„æ¨¡å‹å¤æ‚åº¦å¹³è¡¡")
        
    else:
        print("\nâŒ å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œä»£ç ")
    
    print("\n=" * 80)

if __name__ == "__main__":
    main() 