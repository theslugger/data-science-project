#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹ - ä¸»æµç¨‹æ§åˆ¶è„šæœ¬
CDS503 Group Project - å®Œæ•´æ•°æ®åˆ†ææµæ°´çº¿
"""

import sys
import argparse
from pathlib import Path
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from config import config
from utils import Logger, print_section_header, get_timestamp

warnings.filterwarnings('ignore')

class BikeDataPipeline:
    """è‡ªè¡Œè½¦æ•°æ®åˆ†æä¸»æµæ°´çº¿"""
    
    def __init__(self, verbose=True):
        self.logger = Logger(self.__class__.__name__)
        self.verbose = verbose
        self.pipeline_results = {}
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        config.create_directories()
        
    def run_data_exploration(self, save_results=True):
        """è¿è¡Œæ•°æ®æ¢ç´¢åˆ†æ"""
        print_section_header("ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®æ¢ç´¢åˆ†æ (EDA)", level=1)
        
        try:
            from enhanced_data_exploration import EnhancedDataExplorer
            
            explorer = EnhancedDataExplorer()
            
            # æ‰§è¡Œå®Œæ•´çš„EDAæµç¨‹
            self.logger.info("å¼€å§‹æ•°æ®æ¢ç´¢åˆ†æ...")
            
            # æ•°æ®åŠ è½½ä¸éªŒè¯
            df = explorer.load_and_validate_data()
            
            # ç›®æ ‡å˜é‡åˆ†æ
            target_analysis = explorer.analyze_target_variable()
            
            # æ•°å€¼ç‰¹å¾åˆ†æ
            numeric_analysis, correlations = explorer.analyze_numerical_features()
            
            # åˆ†ç±»ç‰¹å¾åˆ†æ
            categorical_analysis = explorer.analyze_categorical_features()
            
            # æ—¶é—´æ¨¡å¼åˆ†æ
            time_analysis = explorer.analyze_time_patterns()
            
            # å¤©æ°”å½±å“åˆ†æ
            weather_analysis = explorer.analyze_weather_impact()
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            eda_results = explorer.generate_comprehensive_report()
            
            self.pipeline_results['eda'] = eda_results
            self.logger.info("âœ… æ•°æ®æ¢ç´¢åˆ†æå®Œæˆ")
            
            return eda_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æ¢ç´¢åˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def run_deep_analysis(self, save_results=True):
        """è¿è¡Œæ·±åº¦æ•°æ®åˆ†æ"""
        print_section_header("ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦æ•°æ®æ´å¯Ÿåˆ†æ", level=1)
        
        try:
            from enhanced_data_analysis import EnhancedDataAnalyzer
            
            analyzer = EnhancedDataAnalyzer()
            
            # æ‰§è¡Œæ·±åº¦åˆ†ææµç¨‹
            self.logger.info("å¼€å§‹æ·±åº¦æ•°æ®åˆ†æ...")
            
            # æ•°æ®åŠ è½½ä¸å‡†å¤‡
            df = analyzer.load_and_prepare_data()
            
            # æ·±åº¦ç›®æ ‡å˜é‡åˆ†æ
            target_analysis = analyzer.deep_target_analysis()
            
            # é«˜çº§æ—¶é—´æ¨¡å¼åˆ†æ
            time_analysis = analyzer.advanced_time_pattern_analysis()
            
            # å¤©æ°”å½±å“æ·±åº¦æŒ–æ˜
            weather_analysis = analyzer.weather_impact_deep_dive()
            
            # éœ€æ±‚æ¨¡å¼åˆ†å‰²åˆ†æ
            segmentation_analysis = analyzer.demand_pattern_segmentation()
            
            # é¢„æµ‹æ€§æ´å¯Ÿåˆ†æ
            predictive_analysis = analyzer.predictive_insights_analysis()
            
            # ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š
            deep_analysis_results = analyzer.generate_comprehensive_insights_report()
            
            self.pipeline_results['deep_analysis'] = deep_analysis_results
            self.logger.info("âœ… æ·±åº¦æ•°æ®åˆ†æå®Œæˆ")
            
            return deep_analysis_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ·±åº¦æ•°æ®åˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def run_data_preprocessing(self, use_lag_features=False, feature_selection_method='correlation'):
        """è¿è¡Œæ•°æ®é¢„å¤„ç†"""
        print_section_header("ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½æ•°æ®é¢„å¤„ç†", level=1)
        
        try:
            from enhanced_data_preprocessing import EnhancedDataPreprocessor
            
            preprocessor = EnhancedDataPreprocessor()
            
            # æ‰§è¡Œé¢„å¤„ç†æµç¨‹
            self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
            
            # è¿è¡Œå®Œæ•´é¢„å¤„ç†æµæ°´çº¿
            (X_train, X_val, X_test, y_train, y_val, y_test, preprocessing_results) = \
                preprocessor.preprocess_pipeline(
                    use_lag_features=use_lag_features,
                    feature_selection_method=feature_selection_method,
                    feature_selection_k=None
                )
            
            self.pipeline_results['preprocessing'] = {
                'results': preprocessing_results,
                'data_shapes': {
                    'X_train': X_train.shape,
                    'X_val': X_val.shape,
                    'X_test': X_test.shape,
                    'y_train': y_train.shape,
                    'y_val': y_val.shape,
                    'y_test': y_test.shape
                },
                'feature_names': preprocessing_results['feature_names']
            }
            
            self.logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
            return (X_train, X_val, X_test, y_train, y_val, y_test, preprocessing_results)
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def run_complete_pipeline(self, include_deep_analysis=True, 
                            use_lag_features=False, 
                            feature_selection_method='correlation'):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åˆ†ææµæ°´çº¿"""
        print_section_header("é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹ - å®Œæ•´æ•°æ®åˆ†ææµæ°´çº¿", level=1)
        
        timestamp = get_timestamp()
        pipeline_start_time = timestamp
        
        try:
            # é˜¶æ®µ1ï¼šæ•°æ®æ¢ç´¢åˆ†æ
            self.logger.info("ğŸš€ å¯åŠ¨å®Œæ•´æ•°æ®åˆ†ææµæ°´çº¿...")
            eda_results = self.run_data_exploration()
            
            # é˜¶æ®µ2ï¼šæ·±åº¦æ•°æ®åˆ†æï¼ˆå¯é€‰ï¼‰
            if include_deep_analysis:
                deep_analysis_results = self.run_deep_analysis()
            else:
                self.logger.info("â­ï¸  è·³è¿‡æ·±åº¦æ•°æ®åˆ†æ")
                deep_analysis_results = None
            
            # é˜¶æ®µ3ï¼šæ•°æ®é¢„å¤„ç†
            preprocessing_data = self.run_data_preprocessing(
                use_lag_features=use_lag_features,
                feature_selection_method=feature_selection_method
            )
            
            # æ•´åˆæ‰€æœ‰ç»“æœ
            complete_results = {
                'pipeline_info': {
                    'timestamp': pipeline_start_time,
                    'stages_completed': ['eda', 'deep_analysis', 'preprocessing'] if include_deep_analysis else ['eda', 'preprocessing'],
                    'configuration': {
                        'include_deep_analysis': include_deep_analysis,
                        'use_lag_features': use_lag_features,
                        'feature_selection_method': feature_selection_method
                    }
                },
                'eda_results': eda_results,
                'deep_analysis_results': deep_analysis_results,
                'preprocessing_results': self.pipeline_results['preprocessing']
            }
            
            # ç”Ÿæˆæµæ°´çº¿æ€»ç»“æŠ¥å‘Š
            self.generate_pipeline_summary(complete_results)
            
            # ä¿å­˜å®Œæ•´ç»“æœ
            from utils import ResultSaver
            result_saver = ResultSaver(self.logger)
            result_saver.save_json(complete_results, f"complete_pipeline_results_{pipeline_start_time}", "pipeline")
            
            self.logger.info("ğŸ‰ å®Œæ•´æ•°æ®åˆ†ææµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
            
            return complete_results, preprocessing_data
            
        except Exception as e:
            self.logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def generate_pipeline_summary(self, results):
        """ç”Ÿæˆæµæ°´çº¿æ€»ç»“æŠ¥å‘Š"""
        print_section_header("æ•°æ®åˆ†ææµæ°´çº¿æ€»ç»“æŠ¥å‘Š", level=1)
        
        pipeline_info = results['pipeline_info']
        eda_results = results['eda_results']
        preprocessing_results = results['preprocessing_results']['results']
        
        print("ğŸ¯ æµæ°´çº¿æ‰§è¡Œæ‘˜è¦:")
        print(f"  æ‰§è¡Œæ—¶é—´æˆ³: {pipeline_info['timestamp']}")
        print(f"  å®Œæˆé˜¶æ®µ: {', '.join(pipeline_info['stages_completed'])}")
        
        print(f"\nğŸ“Š æ•°æ®æ¢ç´¢å…³é”®å‘ç°:")
        if 'basic_info' in eda_results:
            basic_info = eda_results['basic_info']
            print(f"  åŸå§‹æ•°æ®è§„æ¨¡: {basic_info['shape']}")
            print(f"  æ•°æ®å®Œæ•´åº¦: {'100%' if not basic_info['has_missing_values'] else 'å­˜åœ¨ç¼ºå¤±å€¼'}")
        
        if 'target_analysis' in eda_results:
            target_info = eda_results['target_analysis']
            zero_pct = target_info['basic_stats']['é›¶å€¼æ¯”ä¾‹(%)']
            outlier_pct = target_info['outliers']['percentage']
            print(f"  ç›®æ ‡å˜é‡é›¶å€¼: {zero_pct:.1f}%")
            print(f"  ç›®æ ‡å˜é‡å¼‚å¸¸å€¼: {outlier_pct:.1f}%")
        
        if 'numerical_analysis' in eda_results:
            num_info = eda_results['numerical_analysis']
            strong_corr_count = len(num_info['strong_correlations'])
            print(f"  å¼ºç›¸å…³ç‰¹å¾æ•°é‡: {strong_corr_count}")
        
        print(f"\nğŸ”§ é¢„å¤„ç†æ‰§è¡Œæ‘˜è¦:")
        feature_counts = preprocessing_results['feature_counts']
        print(f"  æœ€ç»ˆç‰¹å¾æ•°é‡: {feature_counts['selected_features']}")
        print(f"  æ—¶é—´ç‰¹å¾: {feature_counts['time_features'] + feature_counts['advanced_time_features']}")
        print(f"  å¤©æ°”ç‰¹å¾: {feature_counts['weather_features'] + feature_counts['comfort_features']}")
        print(f"  äº¤äº’ç‰¹å¾: {feature_counts['interaction_features']}")
        print(f"  æ»åç‰¹å¾: {feature_counts['lag_features']}")
        
        data_shapes = results['preprocessing_results']['data_shapes']
        print(f"\nğŸ“ˆ æœ€ç»ˆæ•°æ®é›†:")
        print(f"  è®­ç»ƒé›†: {data_shapes['X_train']}")
        print(f"  éªŒè¯é›†: {data_shapes['X_val']}")
        print(f"  æµ‹è¯•é›†: {data_shapes['X_test']}")
        
        print(f"\nğŸ’¡ å»ºæ¨¡å‡†å¤‡å»ºè®®:")
        if results.get('deep_analysis_results') and 'predictive_insights' in results['deep_analysis_results']:
            strategies = results['deep_analysis_results']['predictive_insights']['modeling_strategies']
            for i, strategy in enumerate(strategies[:5], 1):
                print(f"  {i}. {strategy}")
        else:
            print("  1. ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯")
            print("  2. è€ƒè™‘é›†æˆå­¦ä¹ æ–¹æ³•")
            print("  3. æ³¨æ„ç‰¹å¾å·¥ç¨‹çš„æœ‰æ•ˆæ€§")
            print("  4. ç›‘æ§æ¨¡å‹è¿‡æ‹Ÿåˆ")
            print("  5. é‡ç‚¹å…³æ³¨é«˜å³°æ—¶æ®µé¢„æµ‹")
        
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        print("  1. è¿è¡Œå®éªŒ1ï¼šæœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒ")
        print("  2. è¿è¡Œå®éªŒ2ï¼šç‰¹å¾é€‰æ‹©ä¼˜åŒ–")
        print("  3. è¿è¡Œå®éªŒ3ï¼šé›†æˆå­¦ä¹ æ–¹æ³•")
        print("  4. è¿è¡Œå®éªŒ4ï¼šè®­ç»ƒæ ·æœ¬å¤§å°åˆ†æ")
        
        print(f"\nâœ… æ•°æ®åˆ†ææµæ°´çº¿å®Œæˆï¼Œå¯ä»¥å¼€å§‹æ¨¡å‹è®­ç»ƒå®éªŒï¼")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹ - æ•°æ®åˆ†ææµæ°´çº¿')
    
    parser.add_argument('--stage', type=str, choices=['eda', 'deep', 'preprocess', 'all'], 
                       default='all', help='æ‰§è¡Œçš„é˜¶æ®µ')
    parser.add_argument('--no-deep-analysis', action='store_true', 
                       help='è·³è¿‡æ·±åº¦æ•°æ®åˆ†æ')
    parser.add_argument('--use-lag-features', action='store_true',
                       help='ä½¿ç”¨æ»åç‰¹å¾ï¼ˆæ³¨æ„æ•°æ®æ³„éœ²é£é™©ï¼‰')
    parser.add_argument('--feature-selection', type=str, 
                       choices=['correlation', 'univariate', 'rfe', 'none'],
                       default='correlation', help='ç‰¹å¾é€‰æ‹©æ–¹æ³•')
    parser.add_argument('--verbose', action='store_true', default=True,
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pipeline = BikeDataPipeline(verbose=args.verbose)
    
    try:
        if args.stage == 'eda':
            # åªè¿è¡Œæ•°æ®æ¢ç´¢
            results = pipeline.run_data_exploration()
            print("âœ… æ•°æ®æ¢ç´¢åˆ†æå®Œæˆ")
            
        elif args.stage == 'deep':
            # åªè¿è¡Œæ·±åº¦åˆ†æ
            results = pipeline.run_deep_analysis()
            print("âœ… æ·±åº¦æ•°æ®åˆ†æå®Œæˆ")
            
        elif args.stage == 'preprocess':
            # åªè¿è¡Œé¢„å¤„ç†
            results = pipeline.run_data_preprocessing(
                use_lag_features=args.use_lag_features,
                feature_selection_method=args.feature_selection
            )
            print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
        else:  # args.stage == 'all'
            # è¿è¡Œå®Œæ•´æµæ°´çº¿
            complete_results, preprocessing_data = pipeline.run_complete_pipeline(
                include_deep_analysis=not args.no_deep_analysis,
                use_lag_features=args.use_lag_features,
                feature_selection_method=args.feature_selection
            )
            print("âœ… å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 