#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®æ·±åº¦åˆ†æ
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹
åŸºäºæ•°æ®æ´å¯Ÿçš„æ·±åº¦æ¨¡å¼æŒ–æ˜
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from scipy import stats
import warnings

from config import config
from utils import (Logger, DataValidator, DataLoader, ResultSaver, 
                  VisualizationHelper, MetricsCalculator, print_section_header, get_timestamp)

warnings.filterwarnings('ignore')

class EnhancedDataAnalyzer:
    """å¢å¼ºç‰ˆæ•°æ®æ·±åº¦åˆ†æç±»"""
    
    def __init__(self):
        self.logger = Logger(self.__class__.__name__)
        self.data_loader = DataLoader(self.logger)
        self.validator = DataValidator()
        self.result_saver = ResultSaver(self.logger)
        self.viz_helper = VisualizationHelper(self.logger)
        self.metrics_calc = MetricsCalculator()
        
        self.df = None
        self.analysis_results = {}
        
    def load_and_prepare_data(self, file_path=None):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        print_section_header("æ•°æ®åŠ è½½ä¸é¢„å¤„ç†", level=1)
        
        # åŠ è½½æ•°æ®
        self.df = self.data_loader.load_data(file_path)
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼ï¼ˆå¤„ç†æ··åˆæ ¼å¼ï¼‰
        date_col = config.DATA_CONFIG['date_column']
        try:
            self.df[date_col] = pd.to_datetime(self.df[date_col], format='mixed', dayfirst=True)
        except:
            self.df[date_col] = pd.to_datetime(self.df[date_col])
        
        # æ·»åŠ åŸºç¡€æ—¶é—´ç‰¹å¾
        self.df['Year'] = self.df[date_col].dt.year
        self.df['Month'] = self.df[date_col].dt.month
        self.df['Day'] = self.df[date_col].dt.day
        self.df['Weekday'] = self.df[date_col].dt.weekday
        self.df['IsWeekend'] = (self.df['Weekday'] >= 5).astype(int)
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {self.df.shape}")
        return self.df
    
    def deep_target_analysis(self):
        """æ·±åº¦ç›®æ ‡å˜é‡åˆ†æ"""
        print_section_header("ç›®æ ‡å˜é‡æ·±åº¦æ´å¯Ÿåˆ†æ", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        target_values = self.df[target_col]
        
        analysis_results = {}
        
        # 1. é«˜çº§ç»Ÿè®¡åˆ†æ
        print_section_header("é«˜çº§ç»Ÿè®¡ç‰¹å¾", level=2)
        
        # ååº¦å’Œå³°åº¦åˆ†æ
        skewness = target_values.skew()
        kurtosis = target_values.kurtosis()
        
        print(f"ååº¦ (Skewness): {skewness:.4f}")
        if skewness > 1:
            print("  - å¼ºå³ååˆ†å¸ƒ")
        elif skewness > 0.5:
            print("  - ä¸­ç­‰å³ååˆ†å¸ƒ")
        elif skewness < -1:
            print("  - å¼ºå·¦ååˆ†å¸ƒ")
        elif skewness < -0.5:
            print("  - ä¸­ç­‰å·¦ååˆ†å¸ƒ")
        else:
            print("  - è¿‘ä¼¼å¯¹ç§°åˆ†å¸ƒ")
        
        print(f"å³°åº¦ (Kurtosis): {kurtosis:.4f}")
        if kurtosis > 3:
            print("  - å°–å³°åˆ†å¸ƒï¼ˆåšå°¾ï¼‰")
        elif kurtosis < 3:
            print("  - å¹³å³°åˆ†å¸ƒï¼ˆè–„å°¾ï¼‰")
        else:
            print("  - æ­£æ€å³°åº¦")
        
        # æ­£æ€æ€§æ£€éªŒ
        print_section_header("æ­£æ€æ€§æ£€éªŒ", level=3)
        try:
            # Shapiro-Wilkæ£€éªŒï¼ˆé€‚ç”¨äºå°æ ·æœ¬ï¼‰
            if len(target_values) <= 5000:
                shapiro_stat, shapiro_p = stats.shapiro(target_values.sample(5000) if len(target_values) > 5000 else target_values)
                print(f"Shapiro-Wilk æ£€éªŒ: ç»Ÿè®¡é‡={shapiro_stat:.4f}, på€¼={shapiro_p:.6f}")
            
            # Kolmogorov-Smirnovæ£€éªŒ
            ks_stat, ks_p = stats.kstest(target_values, 'norm')
            print(f"Kolmogorov-Smirnov æ£€éªŒ: ç»Ÿè®¡é‡={ks_stat:.4f}, på€¼={ks_p:.6f}")
            
            if ks_p < 0.05:
                print("  - æ‹’ç»æ­£æ€åˆ†å¸ƒå‡è®¾")
            else:
                print("  - æ— æ³•æ‹’ç»æ­£æ€åˆ†å¸ƒå‡è®¾")
                
        except Exception as e:
            self.logger.warning(f"æ­£æ€æ€§æ£€éªŒå¤±è´¥: {str(e)}")
        
        # 2. é›¶å€¼æ¨¡å¼æ·±åº¦åˆ†æ
        print_section_header("é›¶å€¼æ¨¡å¼æ·±åº¦åˆ†æ", level=2)
        
        zero_mask = target_values == 0
        zero_count = zero_mask.sum()
        zero_percentage = zero_count / len(target_values) * 100
        
        print(f"é›¶å€¼æ€»æ•°: {zero_count} ({zero_percentage:.2f}%)")
        
        if zero_count > 0:
            zero_data = self.df[zero_mask].copy()
            
            # é›¶å€¼çš„æ—¶é—´åˆ†å¸ƒ
            print("é›¶å€¼æ—¶é—´åˆ†å¸ƒæ¨¡å¼:")
            zero_hour_dist = zero_data['Hour'].value_counts().sort_index()
            for hour, count in zero_hour_dist.items():
                pct = count / zero_count * 100
                print(f"  {hour:2d}æ—¶: {count:3d}æ¬¡ ({pct:5.1f}%)")
            
            # é›¶å€¼çš„å­£èŠ‚åˆ†å¸ƒ
            if 'Seasons' in zero_data.columns:
                print(f"\né›¶å€¼å­£èŠ‚åˆ†å¸ƒ:")
                zero_season_dist = zero_data['Seasons'].value_counts()
                for season, count in zero_season_dist.items():
                    pct = count / zero_count * 100
                    print(f"  {season}: {count}æ¬¡ ({pct:.1f}%)")
            
            # é›¶å€¼çš„è¿è¥çŠ¶æ€
            if 'Functioning Day' in zero_data.columns:
                print(f"\né›¶å€¼è¿è¥çŠ¶æ€:")
                zero_func_dist = zero_data['Functioning Day'].value_counts()
                for status, count in zero_func_dist.items():
                    pct = count / zero_count * 100
                    print(f"  {status}: {count}æ¬¡ ({pct:.1f}%)")
            
            # é›¶å€¼å¤©æ°”æ¡ä»¶åˆ†æ
            print(f"\né›¶å€¼å¤©æ°”æ¡ä»¶:")
            weather_cols = ['Temperature(Â°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Rainfall(mm)', 'Snowfall (cm)']
            for col in weather_cols:
                if col in zero_data.columns:
                    mean_val = zero_data[col].mean()
                    overall_mean = self.df[col].mean()
                    print(f"  {col}: {mean_val:.2f} (å…¨ä½“å¹³å‡: {overall_mean:.2f})")
        
        # 3. åˆ†å¸ƒå½¢çŠ¶åˆ†æ
        print_section_header("åˆ†å¸ƒå½¢çŠ¶ç‰¹å¾åˆ†æ", level=2)
        
        # å¤šå³°æ£€æµ‹
        hist, bin_edges = np.histogram(target_values, bins=50)
        peaks = []
        for i in range(1, len(hist)-1):
            if hist[i] > hist[i-1] and hist[i] > hist[i+1]:
                peak_value = (bin_edges[i] + bin_edges[i+1]) / 2
                peaks.append(peak_value)
        
        print(f"æ£€æµ‹åˆ° {len(peaks)} ä¸ªå³°å€¼:")
        for i, peak in enumerate(peaks):
            print(f"  å³°{i+1}: {peak:.1f}")
        
        if len(peaks) > 1:
            print("  - å¤šå³°åˆ†å¸ƒï¼Œå¯èƒ½å­˜åœ¨ä¸åŒçš„ä½¿ç”¨æ¨¡å¼")
        else:
            print("  - å•å³°åˆ†å¸ƒ")
        
        # 4. æå€¼åˆ†æ
        print_section_header("æå€¼åˆ†æ", level=2)
        
        # æœ€é«˜éœ€æ±‚åˆ†æ
        max_demand = target_values.max()
        max_indices = target_values[target_values == max_demand].index
        
        print(f"æœ€é«˜éœ€æ±‚: {max_demand} è¾†")
        print(f"æœ€é«˜éœ€æ±‚å‡ºç°æ¬¡æ•°: {len(max_indices)}")
        
        if len(max_indices) > 0:
            max_conditions = self.df.loc[max_indices[0]]
            print(f"æœ€é«˜éœ€æ±‚æ¡ä»¶:")
            print(f"  æ—¥æœŸ: {max_conditions[config.DATA_CONFIG['date_column']]}")
            print(f"  å°æ—¶: {max_conditions['Hour']}")
            if 'Seasons' in max_conditions:
                print(f"  å­£èŠ‚: {max_conditions['Seasons']}")
            if 'Temperature(Â°C)' in max_conditions:
                print(f"  æ¸©åº¦: {max_conditions['Temperature(Â°C)']}Â°C")
        
        # ä¿å­˜æ·±åº¦åˆ†æç»“æœ
        analysis_results['advanced_stats'] = {
            'skewness': skewness,
            'kurtosis': kurtosis,
            'zero_analysis': {
                'count': zero_count,
                'percentage': zero_percentage
            },
            'peaks': peaks,
            'max_demand': {
                'value': max_demand,
                'occurrences': len(max_indices)
            }
        }
        
        self.analysis_results['deep_target_analysis'] = analysis_results
        return analysis_results
    
    def advanced_time_pattern_analysis(self):
        """é«˜çº§æ—¶é—´æ¨¡å¼åˆ†æ"""
        print_section_header("é«˜çº§æ—¶é—´æ¨¡å¼æ´å¯Ÿ", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        analysis_results = {}
        
        # 1. åŒå³°æ¨¡å¼è¯¦ç»†åˆ†æ
        print_section_header("åŒå³°æ¨¡å¼è¯¦ç»†åˆ†æ", level=2)
        
        hourly_avg = self.df.groupby('Hour')[target_col].mean()
        hourly_std = self.df.groupby('Hour')[target_col].std()
        
        # è¯†åˆ«å³°å€¼
        peaks = []
        valleys = []
        
        for hour in range(1, 23):
            prev_val = hourly_avg.iloc[hour-1]
            curr_val = hourly_avg.iloc[hour]
            next_val = hourly_avg.iloc[hour+1]
            
            # å³°å€¼æ£€æµ‹
            if curr_val > prev_val and curr_val > next_val:
                peaks.append((hour, curr_val))
            
            # è°·å€¼æ£€æµ‹
            if curr_val < prev_val and curr_val < next_val:
                valleys.append((hour, curr_val))
        
        print(f"å‘ç° {len(peaks)} ä¸ªå³°å€¼:")
        for hour, value in peaks:
            print(f"  {hour:2d}æ—¶: {value:.1f}è¾†")
        
        print(f"å‘ç° {len(valleys)} ä¸ªè°·å€¼:")
        for hour, value in valleys:
            print(f"  {hour:2d}æ—¶: {value:.1f}è¾†")
        
        # 2. å·¥ä½œæ—¥vså‘¨æœ«æ¨¡å¼å¯¹æ¯”
        print_section_header("å·¥ä½œæ—¥vså‘¨æœ«æ¨¡å¼å¯¹æ¯”", level=2)
        
        weekday_pattern = self.df[self.df['IsWeekend'] == 0].groupby('Hour')[target_col].mean()
        weekend_pattern = self.df[self.df['IsWeekend'] == 1].groupby('Hour')[target_col].mean()
        
        pattern_diff = weekday_pattern - weekend_pattern
        max_diff_hour = pattern_diff.abs().idxmax()
        max_diff_value = pattern_diff[max_diff_hour]
        
        print(f"æœ€å¤§å·®å¼‚æ—¶æ®µ: {max_diff_hour}æ—¶")
        print(f"å·®å¼‚å€¼: {max_diff_value:.1f}è¾†")
        print(f"å·¥ä½œæ—¥å¹³å‡: {weekday_pattern[max_diff_hour]:.1f}è¾†")
        print(f"å‘¨æœ«å¹³å‡: {weekend_pattern[max_diff_hour]:.1f}è¾†")
        
        # 3. å­£èŠ‚æ€§å˜åŒ–æ¨¡å¼
        print_section_header("å­£èŠ‚æ€§å˜åŒ–æ¨¡å¼", level=2)
        
        if 'Seasons' in self.df.columns:
            seasonal_stats = {}
            for season in self.df['Seasons'].unique():
                season_data = self.df[self.df['Seasons'] == season]
                seasonal_hourly = season_data.groupby('Hour')[target_col].mean()
                
                # è®¡ç®—å­£èŠ‚æ€§ç‰¹å¾
                daily_total = season_data.groupby(season_data[config.DATA_CONFIG['date_column']].dt.date)[target_col].sum().mean()
                peak_hour = seasonal_hourly.idxmax()
                peak_value = seasonal_hourly.max()
                
                seasonal_stats[season] = {
                    'daily_average': daily_total,
                    'peak_hour': peak_hour,
                    'peak_value': peak_value,
                    'hourly_pattern': seasonal_hourly.to_dict()
                }
                
                print(f"{season}:")
                print(f"  æ—¥å‡æ€»é‡: {daily_total:.1f}è¾†")
                print(f"  å³°å€¼æ—¶é—´: {peak_hour}æ—¶")
                print(f"  å³°å€¼éœ€æ±‚: {peak_value:.1f}è¾†")
        
        # 4. æœˆåº¦è¶‹åŠ¿åˆ†æ
        print_section_header("æœˆåº¦è¶‹åŠ¿åˆ†æ", level=2)
        
        monthly_avg = self.df.groupby('Month')[target_col].mean()
        monthly_growth = monthly_avg.pct_change() * 100
        
        print("æœˆåº¦å¹³å‡éœ€æ±‚å˜åŒ–ç‡:")
        for month, growth in monthly_growth.dropna().items():
            growth_trend = "å¢é•¿" if growth > 0 else "ä¸‹é™"
            print(f"  {month:2d}æœˆ: {growth:+.1f}% ({growth_trend})")
        
        # ä¿å­˜é«˜çº§æ—¶é—´åˆ†æç»“æœ
        analysis_results.update({
            'peaks': peaks,
            'valleys': valleys,
            'weekday_weekend_diff': {
                'max_diff_hour': max_diff_hour,
                'max_diff_value': max_diff_value
            },
            'seasonal_stats': seasonal_stats if 'Seasons' in self.df.columns else None,
            'monthly_growth': monthly_growth.to_dict()
        })
        
        self.analysis_results['advanced_time_analysis'] = analysis_results
        return analysis_results
    
    def weather_impact_deep_dive(self):
        """å¤©æ°”å½±å“æ·±åº¦æŒ–æ˜"""
        print_section_header("å¤©æ°”å½±å“æ·±åº¦æŒ–æ˜", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        analysis_results = {}
        
        # 1. æ¸©åº¦èˆ’é€‚åŒºé—´åˆ†æ
        print_section_header("æ¸©åº¦èˆ’é€‚åŒºé—´åˆ†æ", level=2)
        
        temp_col = 'Temperature(Â°C)'
        if temp_col in self.df.columns:
            # æŒ‰æ¸©åº¦åŒºé—´åˆ†æéœ€æ±‚
            temp_bins = np.arange(-20, 41, 5)  # 5åº¦ä¸€ä¸ªåŒºé—´
            self.df['TempBin'] = pd.cut(self.df[temp_col], bins=temp_bins)
            
            temp_demand = self.df.groupby('TempBin')[target_col].agg(['mean', 'count'])
            temp_demand = temp_demand[temp_demand['count'] >= 10]  # è‡³å°‘10ä¸ªæ ·æœ¬
            
            # æ‰¾åˆ°æœ€ä¼˜æ¸©åº¦åŒºé—´
            optimal_temp_bin = temp_demand['mean'].idxmax()
            optimal_demand = temp_demand['mean'].max()
            
            print(f"æœ€ä¼˜æ¸©åº¦åŒºé—´: {optimal_temp_bin}")
            print(f"è¯¥åŒºé—´å¹³å‡éœ€æ±‚: {optimal_demand:.1f}è¾†")
            
            # è®¡ç®—æ¸©åº¦æ•æ„Ÿæ€§
            temp_corr = self.df[temp_col].corr(self.df[target_col])
            print(f"æ¸©åº¦ç›¸å…³æ€§: {temp_corr:.4f}")
            
            analysis_results['temperature'] = {
                'optimal_range': str(optimal_temp_bin),
                'optimal_demand': optimal_demand,
                'correlation': temp_corr
            }
        
        # 2. å¤åˆå¤©æ°”æ¡ä»¶åˆ†æ
        print_section_header("å¤åˆå¤©æ°”æ¡ä»¶åˆ†æ", level=2)
        
        # å®šä¹‰ç†æƒ³å¤©æ°”æ¡ä»¶
        ideal_weather_mask = (
            (self.df['Temperature(Â°C)'].between(15, 25)) &
            (self.df['Humidity(%)'].between(40, 70)) &
            (self.df['Wind speed (m/s)'] < 4) &
            (self.df['Rainfall(mm)'] == 0) &
            (self.df['Snowfall (cm)'] == 0)
        )
        
        ideal_demand = self.df[ideal_weather_mask][target_col].mean()
        ideal_count = ideal_weather_mask.sum()
        ideal_percentage = ideal_count / len(self.df) * 100
        
        print(f"ç†æƒ³å¤©æ°”æ¡ä»¶:")
        print(f"  å‡ºç°æ¬¡æ•°: {ideal_count} ({ideal_percentage:.1f}%)")
        print(f"  å¹³å‡éœ€æ±‚: {ideal_demand:.1f}è¾†")
        print(f"  vs æ€»ä½“å¹³å‡: {self.df[target_col].mean():.1f}è¾†")
        
        # æ¶åŠ£å¤©æ°”æ¡ä»¶åˆ†æ
        extreme_weather_mask = (
            (self.df['Temperature(Â°C)'] < 0) |
            (self.df['Temperature(Â°C)'] > 35) |
            (self.df['Humidity(%)'] > 90) |
            (self.df['Wind speed (m/s)'] > 8) |
            (self.df['Rainfall(mm)'] > 10) |
            (self.df['Snowfall (cm)'] > 2)
        )
        
        extreme_demand = self.df[extreme_weather_mask][target_col].mean()
        extreme_count = extreme_weather_mask.sum()
        extreme_percentage = extreme_count / len(self.df) * 100
        
        print(f"\næ¶åŠ£å¤©æ°”æ¡ä»¶:")
        print(f"  å‡ºç°æ¬¡æ•°: {extreme_count} ({extreme_percentage:.1f}%)")
        print(f"  å¹³å‡éœ€æ±‚: {extreme_demand:.1f}è¾†")
        print(f"  vs æ€»ä½“å¹³å‡: {self.df[target_col].mean():.1f}è¾†")
        
        # 3. å¤©æ°”Ã—æ—¶é—´äº¤äº’æ•ˆåº”
        print_section_header("å¤©æ°”Ã—æ—¶é—´äº¤äº’æ•ˆåº”", level=2)
        
        # åˆ†æä¸åŒå¤©æ°”æ¡ä»¶ä¸‹çš„æ—¶é—´æ¨¡å¼
        weather_conditions = {
            'ç†æƒ³å¤©æ°”': ideal_weather_mask,
            'æ¶åŠ£å¤©æ°”': extreme_weather_mask,
            'æ™®é€šå¤©æ°”': ~(ideal_weather_mask | extreme_weather_mask)
        }
        
        for condition_name, condition_mask in weather_conditions.items():
            if condition_mask.sum() > 0:
                condition_hourly = self.df[condition_mask].groupby('Hour')[target_col].mean()
                peak_hour = condition_hourly.idxmax()
                peak_value = condition_hourly.max()
                
                print(f"{condition_name}:")
                print(f"  é«˜å³°æ—¶é—´: {peak_hour}æ—¶")
                print(f"  é«˜å³°éœ€æ±‚: {peak_value:.1f}è¾†")
        
        # ä¿å­˜å¤©æ°”æ·±åº¦åˆ†æç»“æœ
        analysis_results.update({
            'ideal_weather': {
                'count': ideal_count,
                'percentage': ideal_percentage,
                'average_demand': ideal_demand
            },
            'extreme_weather': {
                'count': extreme_count,
                'percentage': extreme_percentage,
                'average_demand': extreme_demand
            },
            'weather_time_interaction': {
                condition: {
                    'peak_hour': self.df[mask].groupby('Hour')[target_col].mean().idxmax() if mask.sum() > 0 else None,
                    'peak_demand': self.df[mask].groupby('Hour')[target_col].mean().max() if mask.sum() > 0 else None
                }
                for condition, mask in weather_conditions.items()
            }
        })
        
        self.analysis_results['weather_deep_analysis'] = analysis_results
        return analysis_results
    
    def demand_pattern_segmentation(self):
        """éœ€æ±‚æ¨¡å¼åˆ†å‰²åˆ†æ"""
        print_section_header("éœ€æ±‚æ¨¡å¼åˆ†å‰²åˆ†æ", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        analysis_results = {}
        
        # 1. éœ€æ±‚æ°´å¹³åˆ†å±‚
        print_section_header("éœ€æ±‚æ°´å¹³åˆ†å±‚", level=2)
        
        # å®šä¹‰éœ€æ±‚ç­‰çº§
        demand_quantiles = self.df[target_col].quantile([0.1, 0.3, 0.7, 0.9])
        
        def categorize_demand(demand):
            if demand <= demand_quantiles[0.1]:
                return 'æä½éœ€æ±‚'
            elif demand <= demand_quantiles[0.3]:
                return 'ä½éœ€æ±‚'
            elif demand <= demand_quantiles[0.7]:
                return 'ä¸­ç­‰éœ€æ±‚'
            elif demand <= demand_quantiles[0.9]:
                return 'é«˜éœ€æ±‚'
            else:
                return 'æé«˜éœ€æ±‚'
        
        self.df['DemandLevel'] = self.df[target_col].apply(categorize_demand)
        
        demand_distribution = self.df['DemandLevel'].value_counts()
        print("éœ€æ±‚åˆ†å±‚åˆ†å¸ƒ:")
        for level, count in demand_distribution.items():
            percentage = count / len(self.df) * 100
            print(f"  {level}: {count} ({percentage:.1f}%)")
        
        # 2. å„éœ€æ±‚å±‚çº§çš„ç‰¹å¾åˆ†æ
        print_section_header("å„éœ€æ±‚å±‚çº§ç‰¹å¾åˆ†æ", level=2)
        
        level_characteristics = {}
        
        for level in demand_distribution.index:
            level_data = self.df[self.df['DemandLevel'] == level]
            
            # æ—¶é—´ç‰¹å¾
            peak_hour = level_data['Hour'].mode().iloc[0] if len(level_data) > 0 else None
            peak_season = level_data['Seasons'].mode().iloc[0] if 'Seasons' in level_data.columns and len(level_data) > 0 else None
            weekend_ratio = level_data['IsWeekend'].mean() if len(level_data) > 0 else 0
            
            # å¤©æ°”ç‰¹å¾
            avg_temp = level_data['Temperature(Â°C)'].mean() if 'Temperature(Â°C)' in level_data.columns else None
            avg_humidity = level_data['Humidity(%)'].mean() if 'Humidity(%)' in level_data.columns else None
            
            level_characteristics[level] = {
                'count': len(level_data),
                'peak_hour': peak_hour,
                'peak_season': peak_season,
                'weekend_ratio': weekend_ratio,
                'avg_temp': avg_temp,
                'avg_humidity': avg_humidity
            }
            
            print(f"{level}:")
            print(f"  ä¸»è¦æ—¶é—´: {peak_hour}æ—¶")
            if peak_season:
                print(f"  ä¸»è¦å­£èŠ‚: {peak_season}")
            print(f"  å‘¨æœ«æ¯”ä¾‹: {weekend_ratio:.2f}")
            if avg_temp:
                print(f"  å¹³å‡æ¸©åº¦: {avg_temp:.1f}Â°C")
        
        # 3. å¼‚å¸¸é«˜éœ€æ±‚äº‹ä»¶åˆ†æ
        print_section_header("å¼‚å¸¸é«˜éœ€æ±‚äº‹ä»¶åˆ†æ", level=2)
        
        # å®šä¹‰å¼‚å¸¸é«˜éœ€æ±‚ï¼ˆ99åˆ†ä½æ•°ä»¥ä¸Šï¼‰
        extreme_threshold = self.df[target_col].quantile(0.99)
        extreme_events = self.df[self.df[target_col] >= extreme_threshold]
        
        print(f"å¼‚å¸¸é«˜éœ€æ±‚é˜ˆå€¼: {extreme_threshold:.0f}è¾†")
        print(f"å¼‚å¸¸äº‹ä»¶æ•°é‡: {len(extreme_events)}")
        
        if len(extreme_events) > 0:
            # åˆ†æå¼‚å¸¸äº‹ä»¶ç‰¹å¾
            extreme_hour_dist = extreme_events['Hour'].value_counts().head(3)
            extreme_season_dist = extreme_events['Seasons'].value_counts().head(3) if 'Seasons' in extreme_events.columns else None
            
            print(f"å¼‚å¸¸äº‹ä»¶é«˜å‘æ—¶æ®µ:")
            for hour, count in extreme_hour_dist.items():
                print(f"  {hour}æ—¶: {count}æ¬¡")
            
            if extreme_season_dist is not None:
                print(f"å¼‚å¸¸äº‹ä»¶é«˜å‘å­£èŠ‚:")
                for season, count in extreme_season_dist.items():
                    print(f"  {season}: {count}æ¬¡")
        
        # ä¿å­˜éœ€æ±‚åˆ†å‰²åˆ†æç»“æœ
        analysis_results.update({
            'demand_levels': demand_distribution.to_dict(),
            'level_characteristics': level_characteristics,
            'extreme_events': {
                'threshold': extreme_threshold,
                'count': len(extreme_events),
                'characteristics': {
                    'hour_distribution': extreme_hour_dist.to_dict() if len(extreme_events) > 0 else {},
                    'season_distribution': extreme_season_dist.to_dict() if len(extreme_events) > 0 and extreme_season_dist is not None else {}
                }
            }
        })
        
        self.analysis_results['demand_segmentation'] = analysis_results
        return analysis_results
    
    def predictive_insights_analysis(self):
        """é¢„æµ‹æ€§æ´å¯Ÿåˆ†æ"""
        print_section_header("é¢„æµ‹æ€§æ´å¯Ÿåˆ†æ", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        analysis_results = {}
        
        # 1. ç‰¹å¾é‡è¦æ€§åˆæ­¥è¯„ä¼°
        print_section_header("ç‰¹å¾é‡è¦æ€§åˆæ­¥è¯„ä¼°", level=2)
        
        numeric_features = self.df.select_dtypes(include=[np.number]).columns.tolist()
        numeric_features = [col for col in numeric_features if col != target_col]
        
        feature_importance = {}
        
        for feature in numeric_features:
            if feature in self.df.columns:
                # è®¡ç®—ç›¸å…³æ€§
                correlation = abs(self.df[feature].corr(self.df[target_col]))
                
                # è®¡ç®—äº’ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
                try:
                    from sklearn.feature_selection import mutual_info_regression
                    mi_score = mutual_info_regression(
                        self.df[[feature]].fillna(0), 
                        self.df[target_col], 
                        random_state=42
                    )[0]
                except:
                    mi_score = 0
                
                feature_importance[feature] = {
                    'correlation': correlation,
                    'mutual_info': mi_score,
                    'combined_score': correlation * 0.7 + mi_score * 0.3
                }
        
        # æ’åºç‰¹å¾é‡è¦æ€§
        sorted_features = sorted(feature_importance.items(), 
                               key=lambda x: x[1]['combined_score'], 
                               reverse=True)
        
        print("ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆç»¼åˆè¯„åˆ†ï¼‰:")
        for i, (feature, scores) in enumerate(sorted_features[:10], 1):
            print(f"{i:2d}. {feature:25s}: {scores['combined_score']:.4f} "
                  f"(ç›¸å…³æ€§={scores['correlation']:.3f}, äº’ä¿¡æ¯={scores['mutual_info']:.3f})")
        
        # 2. å¯é¢„æµ‹æ€§åˆ†æ
        print_section_header("å¯é¢„æµ‹æ€§åˆ†æ", level=2)
        
        # åŸºäºè‡ªç›¸å…³åˆ†æå¯é¢„æµ‹æ€§
        try:
            from statsmodels.tsa.stattools import acf
            
            # è®¡ç®—è‡ªç›¸å…³å‡½æ•°
            autocorr = acf(self.df[target_col].values, nlags=24, fft=True)
            
            # 1å°æ—¶æ»åç›¸å…³æ€§
            lag_1h_corr = autocorr[1]
            # 24å°æ—¶æ»åç›¸å…³æ€§ï¼ˆåŒæ—¶é—´æ˜¨å¤©ï¼‰
            lag_24h_corr = autocorr[24] if len(autocorr) > 24 else 0
            
            print(f"1å°æ—¶è‡ªç›¸å…³æ€§: {lag_1h_corr:.4f}")
            print(f"24å°æ—¶è‡ªç›¸å…³æ€§: {lag_24h_corr:.4f}")
            
            if lag_1h_corr > 0.3:
                print("  - çŸ­æœŸé¢„æµ‹æ€§è¾ƒå¥½")
            if lag_24h_corr > 0.3:
                print("  - æ—¥æ¨¡å¼é¢„æµ‹æ€§è¾ƒå¥½")
            
            analysis_results['predictability'] = {
                'lag_1h_correlation': lag_1h_corr,
                'lag_24h_correlation': lag_24h_corr
            }
            
        except ImportError:
            self.logger.warning("statsmodelsä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªç›¸å…³åˆ†æ")
        
        # 3. å»ºæ¨¡ç­–ç•¥å»ºè®®
        print_section_header("å»ºæ¨¡ç­–ç•¥å»ºè®®", level=2)
        
        strategies = []
        
        # åŸºäºæ•°æ®ç‰¹å¾ç»™å‡ºå»ºè®®
        if self.analysis_results.get('deep_target_analysis', {}).get('advanced_stats', {}).get('skewness', 0) > 1:
            strategies.append("è€ƒè™‘å¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢ä»¥å¤„ç†å³ååˆ†å¸ƒ")
        
        if len(sorted_features) > 0 and sorted_features[0][1]['combined_score'] > 0.5:
            strategies.append(f"é‡ç‚¹å…³æ³¨ç‰¹å¾ '{sorted_features[0][0]}'ï¼Œå®ƒä¸ç›®æ ‡å˜é‡å…³ç³»æœ€å¼º")
        
        if 'predictability' in analysis_results:
            if analysis_results['predictability']['lag_24h_correlation'] > 0.3:
                strategies.append("å¯ä»¥è€ƒè™‘ä½¿ç”¨æ»åç‰¹å¾ï¼Œä½†è¦æ³¨æ„æ•°æ®æ³„éœ²")
        
        zero_percentage = self.analysis_results.get('deep_target_analysis', {}).get('advanced_stats', {}).get('zero_analysis', {}).get('percentage', 0)
        if zero_percentage > 10:
            strategies.append("è€ƒè™‘ä½¿ç”¨é›¶è†¨èƒ€æ¨¡å‹å¤„ç†å¤§é‡é›¶å€¼")
        
        strategies.append("ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›")
        strategies.append("è€ƒè™‘é›†æˆæ–¹æ³•ç»“åˆä¸åŒç®—æ³•çš„ä¼˜åŠ¿")
        
        print("å»ºæ¨¡ç­–ç•¥å»ºè®®:")
        for i, strategy in enumerate(strategies, 1):
            print(f"{i}. {strategy}")
        
        # ä¿å­˜é¢„æµ‹æ€§æ´å¯Ÿç»“æœ
        analysis_results.update({
            'feature_importance': {k: v for k, v in sorted_features[:10]},
            'modeling_strategies': strategies
        })
        
        self.analysis_results['predictive_insights'] = analysis_results
        return analysis_results
    
    def generate_comprehensive_insights_report(self):
        """ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š"""
        print_section_header("ç»¼åˆæ´å¯ŸæŠ¥å‘Š", level=1)
        
        # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
        timestamp = get_timestamp()
        self.result_saver.save_json(self.analysis_results, 
                                   f"deep_analysis_results_{timestamp}", 
                                   "analysis")
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        print_section_header("æ‰§è¡Œæ‘˜è¦", level=1)
        
        print("ğŸ¯ å…³é”®æ´å¯Ÿ:")
        
        # 1. æ•°æ®è´¨é‡æ´å¯Ÿ
        zero_pct = self.analysis_results.get('deep_target_analysis', {}).get('advanced_stats', {}).get('zero_analysis', {}).get('percentage', 0)
        print(f"1. æ•°æ®è´¨é‡: é›¶å€¼å æ¯” {zero_pct:.1f}%ï¼Œéœ€è¦ç‰¹åˆ«å¤„ç†")
        
        # 2. æ—¶é—´æ¨¡å¼æ´å¯Ÿ
        peaks = self.analysis_results.get('advanced_time_analysis', {}).get('peaks', [])
        if peaks:
            peak_hours = [str(p[0]) for p in peaks]
            print(f"2. æ—¶é—´æ¨¡å¼: å­˜åœ¨æ˜æ˜¾åŒå³°æ¨¡å¼ï¼Œé«˜å³°æ—¶æ®µä¸º {', '.join(peak_hours)}æ—¶")
        
        # 3. å¤©æ°”å½±å“æ´å¯Ÿ
        ideal_pct = self.analysis_results.get('weather_deep_analysis', {}).get('ideal_weather', {}).get('percentage', 0)
        extreme_pct = self.analysis_results.get('weather_deep_analysis', {}).get('extreme_weather', {}).get('percentage', 0)
        print(f"3. å¤©æ°”å½±å“: ç†æƒ³å¤©æ°”å æ¯” {ideal_pct:.1f}%ï¼Œæ¶åŠ£å¤©æ°”å æ¯” {extreme_pct:.1f}%")
        
        # 4. éœ€æ±‚åˆ†å±‚æ´å¯Ÿ
        demand_levels = self.analysis_results.get('demand_segmentation', {}).get('demand_levels', {})
        if demand_levels:
            high_demand_pct = demand_levels.get('é«˜éœ€æ±‚', 0) + demand_levels.get('æé«˜éœ€æ±‚', 0)
            print(f"4. éœ€æ±‚åˆ†å±‚: é«˜éœ€æ±‚æ—¶æ®µå æ¯” {high_demand_pct:.1f}%")
        
        # 5. å¯é¢„æµ‹æ€§æ´å¯Ÿ
        feature_importance = self.analysis_results.get('predictive_insights', {}).get('feature_importance', {})
        if feature_importance:
            top_feature = list(feature_importance.keys())[0]
            top_score = list(feature_importance.values())[0]['combined_score']
            print(f"5. å¯é¢„æµ‹æ€§: æœ€é‡è¦ç‰¹å¾ä¸º '{top_feature}'ï¼Œç»¼åˆè¯„åˆ† {top_score:.3f}")
        
        print(f"\nğŸ“‹ å»ºæ¨¡å»ºè®®:")
        strategies = self.analysis_results.get('predictive_insights', {}).get('modeling_strategies', [])
        for i, strategy in enumerate(strategies[:5], 1):
            print(f"{i}. {strategy}")
        
        print(f"\nâœ… æ·±åº¦åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {config.OUTPUT_DIR}")
        
        return self.analysis_results

def main():
    """ä¸»å‡½æ•°"""
    print_section_header("é¦–å°”è‡ªè¡Œè½¦å…±äº«æ•°æ® - æ·±åº¦æ´å¯Ÿåˆ†æ", level=1)
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = EnhancedDataAnalyzer()
    
    try:
        # 1. æ•°æ®åŠ è½½ä¸å‡†å¤‡
        df = analyzer.load_and_prepare_data()
        
        # 2. æ·±åº¦ç›®æ ‡å˜é‡åˆ†æ
        target_analysis = analyzer.deep_target_analysis()
        
        # 3. é«˜çº§æ—¶é—´æ¨¡å¼åˆ†æ
        time_analysis = analyzer.advanced_time_pattern_analysis()
        
        # 4. å¤©æ°”å½±å“æ·±åº¦æŒ–æ˜
        weather_analysis = analyzer.weather_impact_deep_dive()
        
        # 5. éœ€æ±‚æ¨¡å¼åˆ†å‰²åˆ†æ
        segmentation_analysis = analyzer.demand_pattern_segmentation()
        
        # 6. é¢„æµ‹æ€§æ´å¯Ÿåˆ†æ
        predictive_analysis = analyzer.predictive_insights_analysis()
        
        # 7. ç”Ÿæˆç»¼åˆæ´å¯ŸæŠ¥å‘Š
        results = analyzer.generate_comprehensive_insights_report()
        
        return results
        
    except Exception as e:
        analyzer.logger.error(f"æ·±åº¦åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise

if __name__ == "__main__":
    results = main() 