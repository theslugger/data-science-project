#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®æ¢ç´¢æ€§åˆ†æ
CDS503 Group Project - é¦–å°”è‡ªè¡Œè½¦éœ€æ±‚é¢„æµ‹
æ•´åˆç»Ÿä¸€æµç¨‹å’Œå·¥å…·
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings

from config import config
from utils import (Logger, DataValidator, DataLoader, ResultSaver, 
                  VisualizationHelper, print_section_header, get_timestamp)

warnings.filterwarnings('ignore')

class EnhancedDataExplorer:
    """å¢å¼ºç‰ˆæ•°æ®æ¢ç´¢ç±»"""
    
    def __init__(self):
        self.logger = Logger(self.__class__.__name__)
        self.data_loader = DataLoader(self.logger)
        self.validator = DataValidator()
        self.result_saver = ResultSaver(self.logger)
        self.viz_helper = VisualizationHelper(self.logger)
        
        self.df = None
        self.exploration_results = {}
        
    def load_and_validate_data(self, file_path=None):
        """åŠ è½½å¹¶éªŒè¯æ•°æ®"""
        print_section_header("æ•°æ®åŠ è½½ä¸éªŒè¯", level=1)
        
        # åŠ è½½æ•°æ®
        self.df = self.data_loader.load_data(file_path)
        
        # åŸºç¡€éªŒè¯
        required_cols = [config.DATA_CONFIG['target_column'], 
                        config.DATA_CONFIG['date_column']]
        self.validator.validate_dataframe(self.df, required_cols)
        
        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        print_section_header("æ•°æ®åŸºæœ¬ä¿¡æ¯", level=2)
        print(f"æ•°æ®é›†å½¢çŠ¶: {self.df.shape}")
        print(f"ç‰¹å¾æ•°é‡: {self.df.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {self.df.shape[0]}")
        print(f"å†…å­˜ä½¿ç”¨é‡: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # åˆ—ä¿¡æ¯
        print(f"\nåˆ—ä¿¡æ¯:")
        for i, col in enumerate(self.df.columns, 1):
            print(f"{i:2d}. {col}")
        
        # æ•°æ®ç±»å‹æ£€æŸ¥
        type_info, type_mismatches = self.validator.check_data_types(self.df)
        print(f"\næ•°æ®ç±»å‹ä¿¡æ¯:")
        print(type_info.to_string(index=False))
        
        if type_mismatches:
            self.logger.warning(f"æ•°æ®ç±»å‹ä¸åŒ¹é…: {type_mismatches}")
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        missing_info = self.validator.check_missing_values(self.df)
        if missing_info is not None:
            print(f"\nâš ï¸  å‘ç°ç¼ºå¤±å€¼:")
            print(missing_info)
        else:
            print(f"\nâœ… æ— ç¼ºå¤±å€¼ï¼Œæ•°æ®å®Œæ•´åº¦100%")
        
        # ä¿å­˜åŸºç¡€ä¿¡æ¯
        self.exploration_results['basic_info'] = {
            'shape': self.df.shape,
            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024**2,
            'has_missing_values': missing_info is not None,
            'data_types': self.df.dtypes.to_dict()
        }
        
        return self.df
    
    def analyze_target_variable(self):
        """åˆ†æç›®æ ‡å˜é‡"""
        print_section_header("ç›®æ ‡å˜é‡æ·±åº¦åˆ†æ", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        target_values = self.df[target_col]
        
        # åŸºç¡€ç»Ÿè®¡
        print_section_header("åŸºç¡€ç»Ÿè®¡æè¿°", level=2)
        stats = {
            'æ•°æ®ç‚¹æ•°é‡': len(target_values),
            'å¹³å‡å€¼': target_values.mean(),
            'ä¸­ä½æ•°': target_values.median(),
            'æ ‡å‡†å·®': target_values.std(),
            'æ–¹å·®': target_values.var(),
            'æœ€å°å€¼': target_values.min(),
            'æœ€å¤§å€¼': target_values.max(),
            'ååº¦': target_values.skew(),
            'å³°åº¦': target_values.kurtosis(),
            'é›¶å€¼æ•°é‡': (target_values == 0).sum(),
            'é›¶å€¼æ¯”ä¾‹(%)': (target_values == 0).mean() * 100
        }
        
        for key, value in stats.items():
            print(f"{key}: {value:.2f}")
        
        # åˆ†ä½æ•°åˆ†æ
        print_section_header("åˆ†ä½æ•°åˆ†æ", level=3)
        quantiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
        for q in quantiles:
            print(f"Q{q*100:5.1f}: {target_values.quantile(q):8.2f}")
        
        # å¼‚å¸¸å€¼åˆ†æ
        print_section_header("å¼‚å¸¸å€¼åˆ†æ (IQRæ–¹æ³•)", level=3)
        Q1 = target_values.quantile(0.25)
        Q3 = target_values.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers_mask = (target_values < lower_bound) | (target_values > upper_bound)
        outliers_count = outliers_mask.sum()
        
        print(f"ä¸‹ç•Œ: {lower_bound:.2f}")
        print(f"ä¸Šç•Œ: {upper_bound:.2f}")
        print(f"å¼‚å¸¸å€¼æ•°é‡: {outliers_count} ({outliers_count/len(target_values)*100:.2f}%)")
        
        # ä¿å­˜ç›®æ ‡å˜é‡åˆ†æç»“æœ
        self.exploration_results['target_analysis'] = {
            'basic_stats': stats,
            'quantiles': {f'Q{q*100:.1f}': target_values.quantile(q) for q in quantiles},
            'outliers': {
                'count': outliers_count,
                'percentage': outliers_count/len(target_values)*100,
                'lower_bound': lower_bound,
                'upper_bound': upper_bound
            }
        }
        
        # å¯è§†åŒ–ç›®æ ‡å˜é‡åˆ†å¸ƒ
        self.viz_helper.plot_target_distribution(target_values, 
                                                save_name=f"target_distribution_{get_timestamp()}")
        
        return target_values
    
    def analyze_numerical_features(self):
        """åˆ†ææ•°å€¼ç‰¹å¾"""
        print_section_header("æ•°å€¼ç‰¹å¾åˆ†æ", level=1)
        
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        target_col = config.DATA_CONFIG['target_column']
        
        print(f"æ•°å€¼ç‰¹å¾æ•°é‡: {len(numeric_cols)}")
        print(f"æ•°å€¼ç‰¹å¾åˆ—è¡¨: {numeric_cols}")
        
        # åŸºç¡€ç»Ÿè®¡æè¿°
        print_section_header("æ•°å€¼ç‰¹å¾ç»Ÿè®¡æè¿°", level=2)
        desc_stats = self.df[numeric_cols].describe()
        print(desc_stats.round(2).to_string())
        
        # ç›¸å…³æ€§åˆ†æ
        print_section_header("ç‰¹å¾ç›¸å…³æ€§åˆ†æ", level=2)
        correlations = self.df[numeric_cols].corr()[target_col].abs().sort_values(ascending=False)
        
        print("ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§ (ç»å¯¹å€¼æ’åº):")
        for feature, corr in correlations.items():
            if feature != target_col:
                correlation_level = "å¼º" if corr > 0.5 else "ä¸­" if corr > 0.3 else "å¼±"
                print(f"  {feature:30s}: {corr:.4f} ({correlation_level})")
        
        # å¼ºç›¸å…³ç‰¹å¾è¯†åˆ«
        strong_corr_features = correlations[correlations > 0.3]
        strong_corr_features = strong_corr_features[strong_corr_features.index != target_col]
        
        if len(strong_corr_features) > 0:
            print(f"\nğŸ¯ å¼ºç›¸å…³ç‰¹å¾ (|r| > 0.3):")
            for feature, corr in strong_corr_features.items():
                print(f"  {feature}: {corr:.4f}")
        
        # å¼‚å¸¸å€¼æ£€æŸ¥
        print_section_header("æ•°å€¼ç‰¹å¾å¼‚å¸¸å€¼æ£€æŸ¥", level=2)
        outlier_summary = {}
        
        for col in numeric_cols:
            if col != 'Hour':  # è·³è¿‡å°æ—¶åˆ—ï¼ˆ0-23æ˜¯æ­£å¸¸èŒƒå›´ï¼‰
                Q1 = self.df[col].quantile(0.25)
                Q3 = self.df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]
                outlier_count = len(outliers)
                outlier_percentage = outlier_count / len(self.df) * 100
                
                if outlier_count > 0:
                    print(f"  {col:30s}: {outlier_count:4d} ä¸ªå¼‚å¸¸å€¼ ({outlier_percentage:.2f}%)")
                    outlier_summary[col] = {
                        'count': outlier_count,
                        'percentage': outlier_percentage,
                        'bounds': [lower_bound, upper_bound]
                    }
        
        if not outlier_summary:
            print("  âœ… æœªå‘ç°æ˜¾è‘—å¼‚å¸¸å€¼")
        
        # ä¿å­˜æ•°å€¼ç‰¹å¾åˆ†æç»“æœ
        self.exploration_results['numerical_analysis'] = {
            'feature_count': len(numeric_cols),
            'descriptive_stats': desc_stats.to_dict(),
            'correlations': correlations.to_dict(),
            'strong_correlations': strong_corr_features.to_dict(),
            'outliers': outlier_summary
        }
        
        # ç»˜åˆ¶ç›¸å…³æ€§çŸ©é˜µ
        self.viz_helper.plot_correlation_matrix(self.df, target_col, 
                                              save_name=f"correlation_matrix_{get_timestamp()}")
        
        return numeric_cols, correlations
    
    def analyze_categorical_features(self):
        """åˆ†æåˆ†ç±»ç‰¹å¾"""
        print_section_header("åˆ†ç±»ç‰¹å¾åˆ†æ", level=1)
        
        categorical_cols = config.FEATURE_CONFIG['categorical_features']
        target_col = config.DATA_CONFIG['target_column']
        
        categorical_analysis = {}
        
        for col in categorical_cols:
            if col in self.df.columns:
                print_section_header(f"{col} åˆ†å¸ƒåˆ†æ", level=2)
                
                # å€¼è®¡æ•°å’Œæ¯”ä¾‹
                value_counts = self.df[col].value_counts()
                percentages = (self.df[col].value_counts(normalize=True) * 100).round(2)
                
                print("ç±»åˆ«åˆ†å¸ƒ:")
                for val, count, pct in zip(value_counts.index, value_counts.values, percentages.values):
                    print(f"  {val:15s}: {count:5d} ({pct:5.2f}%)")
                
                # å„ç±»åˆ«çš„ç›®æ ‡å˜é‡ç»Ÿè®¡
                print(f"\nå„ç±»åˆ«çš„{target_col}ç»Ÿè®¡:")
                category_stats = self.df.groupby(col)[target_col].agg(['count', 'mean', 'std', 'min', 'max'])
                print(category_stats.round(2).to_string())
                
                categorical_analysis[col] = {
                    'value_counts': value_counts.to_dict(),
                    'percentages': percentages.to_dict(),
                    'target_stats': category_stats.to_dict()
                }
        
        # ä¿å­˜åˆ†ç±»ç‰¹å¾åˆ†æç»“æœ
        self.exploration_results['categorical_analysis'] = categorical_analysis
        
        return categorical_analysis
    
    def analyze_time_patterns(self):
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        print_section_header("æ—¶é—´æ¨¡å¼åˆ†æ", level=1)
        
        date_col = config.DATA_CONFIG['date_column']
        target_col = config.DATA_CONFIG['target_column']
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        try:
            self.df[date_col] = pd.to_datetime(self.df[date_col], format='mixed', dayfirst=True)
        except:
            self.df[date_col] = pd.to_datetime(self.df[date_col])
        
        # æå–æ—¶é—´ç‰¹å¾
        self.df['Year'] = self.df[date_col].dt.year
        self.df['Month'] = self.df[date_col].dt.month
        self.df['Day'] = self.df[date_col].dt.day
        self.df['Weekday'] = self.df[date_col].dt.weekday
        self.df['WeekdayName'] = self.df[date_col].dt.day_name()
        self.df['IsWeekend'] = (self.df['Weekday'] >= 5).astype(int)
        
        time_analysis = {}
        
        # å¹´åº¦åˆ†æ
        if self.df['Year'].nunique() > 1:
            print_section_header("å¹´åº¦åˆ†æ", level=2)
            yearly_stats = self.df.groupby('Year')[target_col].agg(['count', 'mean', 'std', 'min', 'max'])
            print(yearly_stats.round(2).to_string())
            time_analysis['yearly'] = yearly_stats.to_dict()
        
        # æœˆåº¦åˆ†æ
        print_section_header("æœˆåº¦åˆ†æ", level=2)
        monthly_stats = self.df.groupby('Month')[target_col].agg(['count', 'mean', 'std'])
        print(monthly_stats.round(2).to_string())
        time_analysis['monthly'] = monthly_stats.to_dict()
        
        # å°æ—¶åˆ†æ
        print_section_header("å°æ—¶åˆ†æ", level=2)
        hourly_stats = self.df.groupby('Hour')[target_col].agg(['count', 'mean', 'std'])
        print(hourly_stats.round(2).to_string())
        time_analysis['hourly'] = hourly_stats.to_dict()
        
        # å·¥ä½œæ—¥vså‘¨æœ«
        print_section_header("å·¥ä½œæ—¥ vs å‘¨æœ«åˆ†æ", level=2)
        weekend_stats = self.df.groupby('IsWeekend')[target_col].agg(['count', 'mean', 'std'])
        weekend_stats.index = ['å·¥ä½œæ—¥', 'å‘¨æœ«']
        print(weekend_stats.round(2).to_string())
        time_analysis['weekend'] = weekend_stats.to_dict()
        
        # ä¸€å‘¨ä¸­å„å¤©
        print_section_header("ä¸€å‘¨å„å¤©åˆ†æ", level=2)
        weekday_stats = self.df.groupby('WeekdayName')[target_col].agg(['count', 'mean', 'std'])
        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        weekday_stats = weekday_stats.reindex(weekday_order)
        print(weekday_stats.round(2).to_string())
        time_analysis['weekday'] = weekday_stats.to_dict()
        
        # é«˜å³°æ—¶æ®µè¯†åˆ«
        print_section_header("é«˜å³°æ—¶æ®µè¯†åˆ«", level=2)
        hourly_avg = self.df.groupby('Hour')[target_col].mean()
        peak_threshold = hourly_avg.quantile(0.8)
        peak_hours = hourly_avg[hourly_avg >= peak_threshold].index.tolist()
        
        print(f"é«˜å³°é˜ˆå€¼ï¼ˆ80åˆ†ä½æ•°ï¼‰: {peak_threshold:.2f}")
        print(f"é«˜å³°æ—¶æ®µ: {peak_hours}")
        
        time_analysis['peaks'] = {
            'threshold': peak_threshold,
            'peak_hours': peak_hours
        }
        
        # ä¿å­˜æ—¶é—´åˆ†æç»“æœ
        self.exploration_results['time_analysis'] = time_analysis
        
        # ç»˜åˆ¶æ—¶é—´åºåˆ—å›¾
        self.viz_helper.plot_time_series(self.df, date_col, target_col,
                                        save_name=f"time_series_{get_timestamp()}")
        
        return time_analysis
    
    def analyze_weather_impact(self):
        """åˆ†æå¤©æ°”å½±å“"""
        print_section_header("å¤©æ°”å› ç´ å½±å“åˆ†æ", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        weather_analysis = {}
        
        # æ¸©åº¦å½±å“
        print_section_header("æ¸©åº¦å½±å“åˆ†æ", level=2)
        temp_col = 'Temperature(Â°C)'
        if temp_col in self.df.columns:
            print(f"æ¸©åº¦èŒƒå›´: {self.df[temp_col].min():.1f}Â°C åˆ° {self.df[temp_col].max():.1f}Â°C")
            
            # æ¸©åº¦åˆ†æ®µåˆ†æ
            temp_ranges = config.FEATURE_CONFIG['weather_thresholds']['temp_ranges']
            temp_labels = ['ä¸¥å¯’(<0Â°C)', 'å¯’å†·(0-10Â°C)', 'å‡‰çˆ½(10-20Â°C)', 'æ¸©æš–(20-30Â°C)', 'ç‚çƒ­(>30Â°C)']
            
            self.df['TempRange'] = pd.cut(self.df[temp_col], bins=temp_ranges, labels=temp_labels, include_lowest=True)
            temp_impact = self.df.groupby('TempRange')[target_col].agg(['count', 'mean', 'std'])
            print(temp_impact.round(2).to_string())
            weather_analysis['temperature'] = temp_impact.to_dict()
        
        # æ¹¿åº¦å½±å“
        print_section_header("æ¹¿åº¦å½±å“åˆ†æ", level=2)
        humidity_col = 'Humidity(%)'
        if humidity_col in self.df.columns:
            humidity_ranges = config.FEATURE_CONFIG['weather_thresholds']['humidity_ranges']
            humidity_labels = ['ä½æ¹¿åº¦(<30%)', 'ä¸­ç­‰æ¹¿åº¦(30-50%)', 'é«˜æ¹¿åº¦(50-70%)', 'æé«˜æ¹¿åº¦(>70%)']
            
            self.df['HumidityRange'] = pd.cut(self.df[humidity_col], bins=humidity_ranges, 
                                            labels=humidity_labels, include_lowest=True)
            humidity_impact = self.df.groupby('HumidityRange')[target_col].agg(['count', 'mean', 'std'])
            print(humidity_impact.round(2).to_string())
            weather_analysis['humidity'] = humidity_impact.to_dict()
        
        # é£é€Ÿå½±å“
        print_section_header("é£é€Ÿå½±å“åˆ†æ", level=2)
        wind_col = 'Wind speed (m/s)'
        if wind_col in self.df.columns:
            print(f"é£é€ŸèŒƒå›´: {self.df[wind_col].min():.1f} åˆ° {self.df[wind_col].max():.1f} m/s")
            
            wind_ranges = config.FEATURE_CONFIG['weather_thresholds']['wind_ranges']
            wind_labels = ['å¾®é£(<2m/s)', 'è½»é£(2-4m/s)', 'å’Œé£(4-6m/s)', 'å¼ºé£(>6m/s)']
            
            self.df['WindRange'] = pd.cut(self.df[wind_col], bins=wind_ranges, 
                                        labels=wind_labels, include_lowest=True)
            wind_impact = self.df.groupby('WindRange')[target_col].agg(['count', 'mean', 'std'])
            print(wind_impact.round(2).to_string())
            weather_analysis['wind'] = wind_impact.to_dict()
        
        # é™é›¨å½±å“
        print_section_header("é™é›¨å½±å“åˆ†æ", level=2)
        rain_col = 'Rainfall(mm)'
        if rain_col in self.df.columns:
            rain_impact = self.df.groupby(self.df[rain_col] > 0)[target_col].agg(['count', 'mean', 'std'])
            rain_impact.index = ['æ— é›¨', 'æœ‰é›¨']
            print(rain_impact.round(2).to_string())
            
            rain_days = (self.df[rain_col] > 0).sum()
            rain_percentage = rain_days / len(self.df) * 100
            print(f"é™é›¨å¤©æ•°: {rain_days} ({rain_percentage:.1f}%)")
            weather_analysis['rainfall'] = {
                'impact': rain_impact.to_dict(),
                'rainy_days': rain_days,
                'rain_percentage': rain_percentage
            }
        
        # é™é›ªå½±å“
        print_section_header("é™é›ªå½±å“åˆ†æ", level=2)
        snow_col = 'Snowfall (cm)'
        if snow_col in self.df.columns:
            snow_impact = self.df.groupby(self.df[snow_col] > 0)[target_col].agg(['count', 'mean', 'std'])
            snow_impact.index = ['æ— é›ª', 'æœ‰é›ª']
            print(snow_impact.round(2).to_string())
            
            snow_days = (self.df[snow_col] > 0).sum()
            snow_percentage = snow_days / len(self.df) * 100
            print(f"é™é›ªå¤©æ•°: {snow_days} ({snow_percentage:.1f}%)")
            weather_analysis['snowfall'] = {
                'impact': snow_impact.to_dict(),
                'snowy_days': snow_days,
                'snow_percentage': snow_percentage
            }
        
        # ä¿å­˜å¤©æ°”åˆ†æç»“æœ
        self.exploration_results['weather_analysis'] = weather_analysis
        
        return weather_analysis
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print_section_header("ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆ", level=1)
        
        # ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ
        self.result_saver.save_json(self.exploration_results, 
                                   f"comprehensive_eda_results_{get_timestamp()}", 
                                   "eda")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print_section_header("æ•°æ®æ¢ç´¢æ€»ç»“", level=1)
        
        target_col = config.DATA_CONFIG['target_column']
        
        print("ğŸ¯ å…³é”®å‘ç°:")
        print(f"1. æ•°æ®è´¨é‡: {self.df.shape[0]} æ¡è®°å½•ï¼Œ{self.df.shape[1]} ä¸ªç‰¹å¾")
        
        if 'target_analysis' in self.exploration_results:
            zero_pct = self.exploration_results['target_analysis']['basic_stats']['é›¶å€¼æ¯”ä¾‹(%)']
            outlier_pct = self.exploration_results['target_analysis']['outliers']['percentage']
            print(f"2. ç›®æ ‡å˜é‡: é›¶å€¼å æ¯” {zero_pct:.1f}%ï¼Œå¼‚å¸¸å€¼å æ¯” {outlier_pct:.1f}%")
        
        if 'numerical_analysis' in self.exploration_results:
            strong_corr = self.exploration_results['numerical_analysis']['strong_correlations']
            print(f"3. å¼ºç›¸å…³ç‰¹å¾: {len(strong_corr)} ä¸ªç‰¹å¾ä¸ç›®æ ‡å˜é‡å¼ºç›¸å…³")
        
        if 'time_analysis' in self.exploration_results:
            peak_hours = self.exploration_results['time_analysis']['peaks']['peak_hours']
            print(f"4. æ—¶é—´æ¨¡å¼: é«˜å³°æ—¶æ®µä¸º {peak_hours}")
        
        if 'weather_analysis' in self.exploration_results:
            print("5. å¤©æ°”å½±å“: æ¸©åº¦ã€æ¹¿åº¦ã€é™é›¨é™é›ªå¯¹éœ€æ±‚æœ‰æ˜¾è‘—å½±å“")
        
        print(f"\nğŸ“ å»ºè®®çš„åç»­æ­¥éª¤:")
        print("1. ç‰¹å¾å·¥ç¨‹: åŸºäºæ—¶é—´æ¨¡å¼åˆ›å»ºé€šå‹¤æ—¶æ®µç‰¹å¾")
        print("2. å¤©æ°”ç‰¹å¾: åˆ›å»ºæ¸©åº¦èˆ’é€‚åº¦å’Œæç«¯å¤©æ°”æ ‡è¯†")
        print("3. äº¤äº’ç‰¹å¾: è€ƒè™‘å¤©æ°”Ã—æ—¶é—´çš„äº¤äº’æ•ˆåº”")
        print("4. å¼‚å¸¸å€¼å¤„ç†: ä½¿ç”¨ç¨³å¥çš„æ–¹æ³•å¤„ç†æç«¯å€¼")
        print("5. æ•°æ®åˆ†å‰²: é‡‡ç”¨æ—¶é—´åºåˆ—å‹å¥½çš„åˆ†å‰²æ–¹æ³•")
        
        # ä¿å­˜åŸºç¡€æ•°æ®æ¡†ï¼ˆæ·»åŠ äº†æ—¶é—´ç‰¹å¾ï¼‰
        processed_df_path = self.result_saver.save_dataframe(
            self.df, f"explored_data_{get_timestamp()}", "eda"
        )
        
        print(f"\nâœ… æ¢ç´¢æ€§åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {config.OUTPUT_DIR}")
        
        return self.exploration_results

def main():
    """ä¸»å‡½æ•°"""
    print_section_header("é¦–å°”è‡ªè¡Œè½¦å…±äº«æ•°æ® - å¢å¼ºç‰ˆæ¢ç´¢æ€§åˆ†æ", level=1)
    
    # åˆ›å»ºæ¢ç´¢å™¨å®ä¾‹
    explorer = EnhancedDataExplorer()
    
    try:
        # 1. æ•°æ®åŠ è½½ä¸éªŒè¯
        df = explorer.load_and_validate_data()
        
        # 2. ç›®æ ‡å˜é‡åˆ†æ
        target_values = explorer.analyze_target_variable()
        
        # 3. æ•°å€¼ç‰¹å¾åˆ†æ
        numeric_cols, correlations = explorer.analyze_numerical_features()
        
        # 4. åˆ†ç±»ç‰¹å¾åˆ†æ
        categorical_analysis = explorer.analyze_categorical_features()
        
        # 5. æ—¶é—´æ¨¡å¼åˆ†æ
        time_analysis = explorer.analyze_time_patterns()
        
        # 6. å¤©æ°”å½±å“åˆ†æ
        weather_analysis = explorer.analyze_weather_impact()
        
        # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        results = explorer.generate_comprehensive_report()
        
        return results
        
    except Exception as e:
        explorer.logger.error(f"æ¢ç´¢æ€§åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise

if __name__ == "__main__":
    results = main() 